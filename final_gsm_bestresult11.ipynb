{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/final_gsm_bestresult11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "0b7e76d5-51dc-4bcc-c1d9-430daee3105e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_05.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "63a1589e-44a8-4e92-8004-aa7efc1cde24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t0\n",
            "Rejected: \t36\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t0\n",
            "Rejected: \t36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=72,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F42F05C2040),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F42F05C2040, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "02c82097-d9da-472f-b301-34b08a079f18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 13) dtype=float32 (created by layer 'input_2')>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='relu',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='relu'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='relu',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='relu'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.002), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=75, batch_size=64)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(np.sqrt(mean_squared_error(y,y_test)))\n",
        "  best_weights= model.get_weights()\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuddmGCf_1dR",
        "outputId": "26ad393e-2104-46eb-eb09-2f99decc88fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 81ms/step - loss: 0.0096\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0046\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0039\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0038\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0037\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0036\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0035\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0032\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0033\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0033\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0032\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0031\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0031\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0031\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0030\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0032\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0030\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0030\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0030\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0030\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0029\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0030\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0029\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0029\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0028\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0028\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0028\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0029\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0028\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0028\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0028\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0028\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0028\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0027\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 9s 117ms/step - loss: 0.0028\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 7s 97ms/step - loss: 0.0027\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0027\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0027\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0028\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0027\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0026\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0025\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0026\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0025\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "50/50 [==============================] - 3s 18ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 82ms/step - loss: 0.0089\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0047\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0040\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0037\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0037\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0033\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0034\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0034\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0033\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0034\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0031\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0031\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0031\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0030\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0030\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0031\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 7s 99ms/step - loss: 0.0027\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0026\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0025\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0025\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "50/50 [==============================] - 2s 17ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 82ms/step - loss: 0.0100\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0045\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0039\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0037\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0037\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0035\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0034\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0034\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0033\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0034\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0031\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0032\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0030\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 7s 100ms/step - loss: 0.0029\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0029\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0026\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0026\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0026\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0026\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0025\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0025\n",
            "50/50 [==============================] - 3s 18ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 21s 83ms/step - loss: 0.0087\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0044\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0040\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0036\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0037\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0034\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0035\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0035\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0033\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0033\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0032\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0031\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0031\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0032\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0032\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0032\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0030\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0030\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0028\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 7s 100ms/step - loss: 0.0029\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0029\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0027\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0028\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0025\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0026\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0025\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0027\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0025\n",
            "50/50 [==============================] - 2s 17ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 19s 82ms/step - loss: 0.0094\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0042\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0038\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0034\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0035\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0034\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0034\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0033\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0034\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0032\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 81ms/step - loss: 0.0032\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0033\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 82ms/step - loss: 0.0031\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0031\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0031\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0031\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0029\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0028\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0030\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0029\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 86ms/step - loss: 0.0029\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 8s 102ms/step - loss: 0.0027\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0029\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0028\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0029\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0027\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0028\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0026\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0027\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0026\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0026\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.0027\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0029\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0026\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0026\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0027\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0026\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0026\n",
            "50/50 [==============================] - 3s 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a97e293-00de-4e3c-d454-afe60aa76cb7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06387223389745474, 0.07650237651674292, 0.06353997322519006, 0.06691960419176068, 0.06865700864386115]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ykd7kawkvX",
        "outputId": "a9f55b40-9081-4cf0-c0e0-1d61ca1810dd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06353997322519006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "b323394e-7f26-4263-a803-992fef824268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f42f0c61670>]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}