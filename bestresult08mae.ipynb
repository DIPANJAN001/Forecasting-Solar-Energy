{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult08mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "0ae0dfdf-2c28-4f1e-cac6-ede4101927ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.9/dist-packages (0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_08.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "27c45c7f-1275-4a83-d0c0-8bbf40675bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t15\n",
            "Rejected: \t26\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t15\n",
            "Rejected: \t26\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t15\n",
            "Rejected: \t26\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t15\n",
            "Rejected: \t26\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t10\n",
            "Rejected: \t31\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t10\n",
            "Rejected: \t31\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t10\n",
            "Rejected: \t31\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t10\n",
            "Rejected: \t31\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t9\n",
            "Rejected: \t32\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t9\n",
            "Rejected: \t32\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t9\n",
            "Rejected: \t32\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t7\n",
            "Rejected: \t33\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t7\n",
            "Rejected: \t33\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t7\n",
            "Rejected: \t33\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t10\n",
            "Tentative: \t6\n",
            "Rejected: \t33\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA54CDC0740),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7FA54CDC0740, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA54CDC0740),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FA54CDC0740, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA54CDC0740),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FA54CDC0740, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                      random_state=RandomState(MT19937) at 0x7FA54CDC0740)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                      random_state=RandomState(MT19937) at 0x7FA54CDC0740)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "1bbf8ea4-fff7-4833-8105-90ee1d7d889b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 12) dtype=float32 (created by layer 'input_2')>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "bf11cc85-6a66-4d0c-f07c-c0a7e4d28d89"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "37/37 [==============================] - 26s 158ms/step - loss: 0.0327\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 7s 195ms/step - loss: 0.0121\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 7s 193ms/step - loss: 0.0131\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 7s 203ms/step - loss: 0.0107\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0096\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0095\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0092\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 7s 188ms/step - loss: 0.0093\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0097\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 7s 188ms/step - loss: 0.0090\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0091\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 8s 226ms/step - loss: 0.0086\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0088\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0087\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0085\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 7s 189ms/step - loss: 0.0085\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0088\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 7s 189ms/step - loss: 0.0088\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0089\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 7s 189ms/step - loss: 0.0084\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0080\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 8s 229ms/step - loss: 0.0081\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0081\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 7s 192ms/step - loss: 0.0081\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0080\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 7s 194ms/step - loss: 0.0084\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0085\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0079\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0078\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0079\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0080\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 7s 204ms/step - loss: 0.0082\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0078\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0079\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0078\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0080\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0076\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0080\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0078\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0079\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0077\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 0.0076\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0079\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0077\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0083\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0079\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0079\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0077\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0074\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0078\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0085\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0076\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0078\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0075\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0077\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0078\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0076\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0078\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0077\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0076\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0075\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0073\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0076\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0074\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0076\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0075\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0074\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0074\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 7s 174ms/step - loss: 0.0074\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0073\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0075\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 7s 200ms/step - loss: 0.0080\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0074\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 7s 176ms/step - loss: 0.0073\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0074\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0071\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0073\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0073\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0076\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0082\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0074\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0070\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0072\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0073\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0072\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0072\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0072\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0074\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0071\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0071\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0073\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0075\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0077\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0078\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0075\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0074\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0073\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0072\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0073\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0072\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0071\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0071\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 7s 203ms/step - loss: 0.0071\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0074\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0071\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0072\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0073\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0076\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0072\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0071\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0072\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0072\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 0.0071\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0069\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0070\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0070\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0069\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0070\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0069\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0069\n",
            "50/50 [==============================] - 4s 24ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 27s 158ms/step - loss: 0.0306\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 7s 203ms/step - loss: 0.0128\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0127\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0104\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0104\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0103\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0096\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0093\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0090\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0093\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0086\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 8s 207ms/step - loss: 0.0097\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0094\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0085\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0084\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0087\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0086\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0084\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0083\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0083\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 7s 188ms/step - loss: 0.0085\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0085\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0087\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0082\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0082\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0079\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0080\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0080\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0079\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0082\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 7s 194ms/step - loss: 0.0082\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0080\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0086\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0080\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0081\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 7s 185ms/step - loss: 0.0078\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0077\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 7s 188ms/step - loss: 0.0082\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0081\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0078\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 7s 185ms/step - loss: 0.0076\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0080\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0079\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0076\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0075\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0076\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0079\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0077\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0075\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0075\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 7s 201ms/step - loss: 0.0077\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0076\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 7s 176ms/step - loss: 0.0076\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0074\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0074\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0079\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0075\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0073\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0078\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0077\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 8s 204ms/step - loss: 0.0075\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0073\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0074\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0076\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0072\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0076\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0075\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0079\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0074\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0074\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0074\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0074\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0076\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0075\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0072\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0074\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0075\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0072\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0074\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0073\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0071\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 0.0072\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0072\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0074\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0075\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0072\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0071\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0073\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0074\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0073\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0071\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 8s 202ms/step - loss: 0.0071\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0070\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 7s 173ms/step - loss: 0.0070\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 7s 176ms/step - loss: 0.0072\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0076\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0075\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0073\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0074\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0071\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0075\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0072\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0070\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0073\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0074\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0068\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0069\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0069\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0072\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0073\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0070\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 7s 189ms/step - loss: 0.0070\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0074\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0074\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0072\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0069\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0069\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0071\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0483\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0094\n",
            "50/50 [==============================] - 3s 23ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 25s 148ms/step - loss: 0.0301\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0128\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0116\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0106\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0101\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0096\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0093\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 7s 194ms/step - loss: 0.0095\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0094\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0089\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0089\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0089\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0086\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0089\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0087\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0091\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0084\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0083\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0086\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0084\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0082\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0086\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0082\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0081\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0080\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0087\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0082\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0077\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0078\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 7s 192ms/step - loss: 0.0078\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0079\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0079\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0078\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0078\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0077\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0076\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0078\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0078\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0076\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0084\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 7s 194ms/step - loss: 0.0079\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0076\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0075\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0081\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0080\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0077\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0076\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0075\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0077\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0076\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0076\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0076\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0077\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0073\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0074\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0076\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0077\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0075\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0077\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0078\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0075\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0076\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0075\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0101\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0098\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0094\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0089\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0089\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0083\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0082\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0078\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0079\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0080\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0078\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0077\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0085\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0081\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0078\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0078\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0077\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0077\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0074\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0090\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0078\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 7s 193ms/step - loss: 0.0077\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0076\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0075\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0075\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0077\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0074\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0074\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0075\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0075\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0073\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 7s 188ms/step - loss: 0.0075\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0074\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0078\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0076\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0073\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0075\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0073\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0073\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0075\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0074\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0073\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0071\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0072\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0075\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0075\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0074\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0073\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0071\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0072\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0070\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0073\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0076\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0070\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0073\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0075\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0072\n",
            "50/50 [==============================] - 3s 20ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 24s 133ms/step - loss: 0.0310\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0120\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0110\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0109\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0098\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0099\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0096\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0096\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0090\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0089\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0085\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0085\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0084\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0086\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0084\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0084\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0086\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0084\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0084\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0085\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0083\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0088\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0082\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0079\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0081\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0079\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0079\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0079\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0080\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0082\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0080\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0086\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0077\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0078\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0079\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0079\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0079\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0082\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0077\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0080\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0078\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0079\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0082\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0076\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0078\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0078\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 0.0078\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0076\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0075\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0079\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0076\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0074\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0094\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0079\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0076\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0080\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0075\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0078\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0079\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0074\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0076\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0079\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0074\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0075\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0074\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0077\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0081\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0074\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0075\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0075\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0076\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0075\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0075\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0073\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 7s 185ms/step - loss: 0.0076\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0073\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0073\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0074\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0076\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0071\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0074\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0073\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0072\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0070\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0074\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0076\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0072\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0071\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0071\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0074\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0074\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0075\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0074\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0071\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0072\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0071\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0072\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0072\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0071\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0070\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0071\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0070\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0074\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0070\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0071\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0072\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0069\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0072\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0070\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0071\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0076\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0075\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0073\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0071\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0071\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0071\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0073\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0071\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0070\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0075\n",
            "50/50 [==============================] - 3s 22ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 26s 163ms/step - loss: 0.0295\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0121\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0109\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0103\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0105\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0105\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0095\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0088\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0090\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0090\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0094\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0092\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0088\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0088\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0086\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0082\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0085\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0085\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0084\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0082\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0082\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0080\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0081\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0080\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0082\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0081\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0086\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0083\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0081\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0077\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0080\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0078\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0077\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0079\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0080\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0078\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0074\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0084\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0080\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0077\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0075\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0077\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0075\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0079\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0083\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0077\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0076\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0079\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0077\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0076\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0076\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0076\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0079\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0077\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0075\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0074\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0077\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0078\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0074\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0074\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 7s 185ms/step - loss: 0.0076\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0074\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0072\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0076\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0075\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0072\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0086\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0084\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0077\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0076\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0076\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0074\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0078\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0076\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0076\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0073\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0075\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0073\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 0.0077\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0074\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0076\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0075\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0073\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0071\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0074\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0073\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0070\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0073\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0073\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0071\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0071\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0072\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0070\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0070\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0072\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 7s 196ms/step - loss: 0.0073\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0075\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0072\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0074\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0072\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0070\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0071\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0073\n",
            "Epoch 104/120\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 0.0067"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-44-e17d81e7f30f>\", line 1, in <module>\n",
            "    GlobalMinimaSearch(mdl.get_weights())\n",
            "  File \"<ipython-input-43-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-43-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-43-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<ipython-input-43-437148bbfb2e>\", line 9, in GlobalMinimaSearch\n",
            "    model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
            "    return concrete_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 755, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 392, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 426, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 167, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88605922-f362-48d1-a085-75c573653b77"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.046576930920265006, 0.05337781330564282, 0.04509284457526624, 0.0600418400334855]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "ed340090-d1a2-4f62-a39e-8faf0d473b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04509284457526624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "8f1396d3-ec32-4402-e947-3c1740ac90c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa54eb0ec40>]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}