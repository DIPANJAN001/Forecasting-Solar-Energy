{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult04mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "29eb9cae-162a-48b3-d160-1a938f95933f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.9/dist-packages (0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_04.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "e2092054-ef35-4cb7-9786-8b29d8ba754c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t1\n",
            "Rejected: \t30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAB0274C540),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7FAB0274C540, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAB0274C540),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FAB0274C540, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAB0274C540),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FAB0274C540, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                      random_state=RandomState(MT19937) at 0x7FAB0274C540)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                      random_state=RandomState(MT19937) at 0x7FAB0274C540)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "5ddb74a9-0696-4cbb-bcd9-57798142a3ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 17) dtype=float32 (created by layer 'input_2')>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuddmGCf_1dR",
        "outputId": "43f1f117-f293-4aaa-a7c0-3177b05dbdad"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "36/36 [==============================] - 26s 135ms/step - loss: 0.0118\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0042\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0039\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0036\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0035\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0031\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0031\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0031\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0030\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0028\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 7s 192ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0027\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0025\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0027\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0026\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0026\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0025\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0025\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0026\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0024\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0026\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0025\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0024\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0026\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0024\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0025\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0024\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0022\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0023\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0022\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0023\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0024\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0023\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0023\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0022\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0022\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0025\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0022\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0023\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0021\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0021\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0020\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0021\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0021\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0022\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0022\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0021\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0021\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0021\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0022\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0021\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0021\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0020\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 7s 203ms/step - loss: 0.0020\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0021\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0020\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0021\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0020\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0021\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0021\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0019\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0022\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0021\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0019\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 7s 195ms/step - loss: 0.0020\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0019\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0021\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0019\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0019\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0021\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0019\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0020\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0020\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0018\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0018\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0018\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0019\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0019\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0019\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0018\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 7s 199ms/step - loss: 0.0018\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0019\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0020\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0019\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0019\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 7s 189ms/step - loss: 0.0018\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0018\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0019\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0018\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0018\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0019\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0017\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0017\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0018\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0018\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 124ms/step - loss: 0.0018\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0017\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0018\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0018\n",
            "48/48 [==============================] - 3s 22ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 26s 158ms/step - loss: 0.0150\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0042\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0036\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0034\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0034\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0033\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0033\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0029\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0028\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0029\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0028\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0029\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 7s 188ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0026\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0025\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0024\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0025\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0024\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0024\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0026\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0023\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0023\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0024\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0023\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0024\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0022\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0022\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0021\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0022\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0022\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 7s 191ms/step - loss: 0.0021\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0023\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0020\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0021\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0020\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0020\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0020\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0020\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0020\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0020\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0019\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0020\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0020\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0021\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0019\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0020\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0020\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0021\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0021\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0019\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0019\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0019\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0019\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0018\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0020\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 7s 200ms/step - loss: 0.0019\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0019\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0018\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0019\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0020\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0020\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0018\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0018\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0018\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0017\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0018\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0018\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0018\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0019\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0019\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0017\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0020\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0018\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0018\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0018\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0018\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0018\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0019\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0019\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0019\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0018\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0018\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0017\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0017\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0017\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0017\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0017\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0017\n",
            "48/48 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 24s 125ms/step - loss: 0.0151\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0044\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0037\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0034\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0032\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0032\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 7s 190ms/step - loss: 0.0030\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0029\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0029\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0030\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0026\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0026\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0026\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0025\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0026\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 7s 190ms/step - loss: 0.0025\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0024\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0026\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0026\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0025\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0024\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0024\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0023\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0023\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0023\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0023\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0022\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0022\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0023\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0023\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0022\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0022\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0022\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0022\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0021\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0021\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 7s 199ms/step - loss: 0.0021\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0022\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0021\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0021\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0022\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0022\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0021\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0020\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 7s 196ms/step - loss: 0.0021\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0020\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0020\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0022\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0020\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0021\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0020\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0020\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0020\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0020\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0019\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0021\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0020\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0020\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0020\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0021\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0020\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0020\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0020\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0019\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0020\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0019\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0020\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0020\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0021\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0020\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0020\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0020\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0019\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0018\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 122ms/step - loss: 0.0020\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0022\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0018\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0020\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0020\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0019\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0018\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0018\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0018\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0018\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0018\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0018\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0019\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0018\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 7s 190ms/step - loss: 0.0017\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0018\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0018\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0018\n",
            "48/48 [==============================] - 4s 33ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 28s 130ms/step - loss: 0.0121\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0042\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0043\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0036\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0033\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0033\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0032\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0031\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0029\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0028\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0028\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0029\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0027\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0026\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0027\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 124ms/step - loss: 0.0029\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0027\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0025\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0025\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0026\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0025\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0024\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0024\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0024\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0023\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 7s 195ms/step - loss: 0.0023\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0023\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0023\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0024\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0023\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0022\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0021\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0023\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0021\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0021\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0027\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0022\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0022\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0020\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0021\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0021\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0022\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0022\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0020\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0022\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0021\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0021\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0022\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0021\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0020\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0020\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0020\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0020\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0020\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0019\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0021\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0020\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0020\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0020\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0019\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0019\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0018\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0019\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0019\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0019\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0019\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0019\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0018\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0018\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0020\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0019\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0018\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0019\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0018\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0019\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 7s 206ms/step - loss: 0.0019\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0019\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0018\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0018\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0018\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0018\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0018\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0020\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0019\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0017\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 7s 180ms/step - loss: 0.0019\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0017\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0017\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0020\n",
            "48/48 [==============================] - 3s 19ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 23s 120ms/step - loss: 0.0127\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0041\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0038\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0034\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0034\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0034\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0030\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0030\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0029\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0028\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0029\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0027\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0026\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0027\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0025\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0029\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0025\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0024\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0024\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0023\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0025\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0025\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0023\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0023\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0023\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0023\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0024\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0024\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0023\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0024\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0024\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0023\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0023\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0023\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0022\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0023\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0021\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0023\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0022\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0022\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0021\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0022\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0022\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0022\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0021\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0022\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0022\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0020\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0021\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0019\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 7s 193ms/step - loss: 0.0020\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0019\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0020\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0019\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0021\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0022\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0021\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0020\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0021\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 7s 190ms/step - loss: 0.0019\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0019\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0019\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0020\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0019\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0019\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0019\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0019\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0019\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0018\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0020\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0019\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0019\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0021\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0020\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0019\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0018\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0020\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0018\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0018\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0018\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0019\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0019\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0019\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0018\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0018\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0019\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0017\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0019\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0019\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0017\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0017\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0018\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0018\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0017\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0017\n",
            "48/48 [==============================] - 3s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd44af68-39a9-4aaa-c985-3a619082eb85"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.022456710454244983, 0.02268198140001909, 0.023392638634004558, 0.025356251760718027, 0.0269356002768888]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "aaafef62-48f7-4007-b0a9-80528a6b5763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.022456710454244983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "7597c020-501d-45a8-ecb6-8fc4a3b5244a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faaf4a5ae80>]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}