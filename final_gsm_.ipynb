{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/final_gsm_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "5d2e8c4f-aae8-42fe-fdf2-9a000a1b97be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_04.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "a21c7d43-320e-49b8-b74e-4b2b276b696c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t9\n",
            "Tentative: \t14\n",
            "Rejected: \t26\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t8\n",
            "Rejected: \t29\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t5\n",
            "Rejected: \t30\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t2\n",
            "Rejected: \t30\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t1\n",
            "Rejected: \t30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F75B4F65840),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F75B4F65840, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.33, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "1fa4b413-9de5-4fb8-8a1d-bfb984e96ae1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 17) dtype=float32 (created by layer 'input_2')>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "class CustomAdam(optimizers.Adam):\n",
        "    def __init__(self, new_idea_param=0.1, *args, **kwargs):\n",
        "        self.new_idea_param = new_idea_param\n",
        "        super(CustomAdam, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        new_idea_t = self.new_idea_param * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        self.weights = [self.iterations] + ms + vs\n",
        "\n",
        "        for p, g, m, v in zip(params, grads, ms, vs):\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - new_idea_t * g\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            self.updates.append(K.update(p, p_t))\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "g8F6yCGl21Sz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "class GradientAdam(optimizers.Adam):\n",
        "    def __init__(self, gradient_param=0.1, *args, **kwargs):\n",
        "        self.gradient_param = gradient_param\n",
        "        super(GradientAdam, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        gradient_t = self.gradient_param * grads\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        self.weights = [self.iterations] + ms + vs\n",
        "\n",
        "        for p, g, m, v, g_t in zip(params, grads, ms, vs, gradient_t):\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) + g_t\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            self.updates.append(K.update(p, p_t))\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "J_gyZaJU8f4W"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class AdaptiveAdam(Adam):\n",
        "    def __init__(self, *args, factor=0.5, patience=5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.wait = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_weights = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        current_loss = loss()\n",
        "        if current_loss < self.best_loss:\n",
        "            self.best_loss = current_loss\n",
        "            self.best_weights = params\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.wait = 0\n",
        "                self.lr = self.lr * self.factor\n",
        "                params = self.best_weights\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(g, \"float32\") for g in grads]\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "PmHcGR7JJLo_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class MomentumAdam(Adam):\n",
        "    def __init__(self, *args, momentum=0.9, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.momentum = momentum\n",
        "        self.velocities = [tf.Variable(tf.zeros_like(p), trainable=False) for p in self.weights]\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = []\n",
        "        for p, g, v in zip(params, grads, self.velocities):\n",
        "            v_t = self.momentum * v - self.lr * g\n",
        "            p_t = p + v_t\n",
        "            self.updates.append(p_t)\n",
        "            self.updates.append(v_t)\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "Zqo9SvzjbTtq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='relu',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='relu'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='relu',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='relu'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>9:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.002), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=80, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(np.sqrt(mean_squared_error(y,y_test)))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "ee22be84-c5c5-4d20-d862-781517449655"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "32/32 [==============================] - 17s 108ms/step - loss: 0.0126\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0048\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0037\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0035\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0032\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0030\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0030\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 5s 154ms/step - loss: 0.0031\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0028\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0026\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0027\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0024\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0026\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0025\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0025\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0024\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.0024\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.0026\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 0.0023\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 5s 169ms/step - loss: 0.0023\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0025\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.0021\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0022\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0022\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0022\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0021\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.0021\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.0022\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0021\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.0020\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0020\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0020\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0021\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.0021\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0024\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0020\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0019\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0019\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0019\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0018\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0019\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 109ms/step - loss: 0.0020\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0018\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0020\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0019\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0019\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0020\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0018\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0017\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0017\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0019\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0017\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0019\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0018\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0019\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0018\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0017\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0017\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0016\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0016\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0016\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0018\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0018\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0016\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0016\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0018\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0016\n",
            "63/63 [==============================] - 3s 16ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 26s 113ms/step - loss: 0.0148\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0058\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0039\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0035\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0032\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0033\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0029\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0030\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.0028\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0026\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0029\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0028\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0025\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0026\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0024\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 4s 132ms/step - loss: 0.0026\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.0025\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0023\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0023\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0023\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0023\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0024\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0021\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0022\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0022\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0021\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 4s 109ms/step - loss: 0.0021\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0022\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0022\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0021\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0021\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0021\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 109ms/step - loss: 0.0020\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0019\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0020\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0020\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0024\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0020\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0019\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0019\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0020\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0019\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0020\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0019\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0020\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0020\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0019\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0018\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0020\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0019\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0019\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0018\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0017\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0018\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0019\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0018\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0019\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0019\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0019\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0018\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0019\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0017\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0016\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0017\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0018\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0017\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0018\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0017\n",
            "63/63 [==============================] - 3s 16ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 17s 107ms/step - loss: 0.0139\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0050\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0037\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0034\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0035\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0034\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0032\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0028\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0031\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 5s 174ms/step - loss: 0.0028\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0025\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0027\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0026\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0026\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0026\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0025\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0025\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.0024\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0022\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0022\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0023\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0022\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0021\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0020\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0021\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0021\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0020\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0019\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0019\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0020\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0020\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0020\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0019\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0020\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0019\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0019\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0018\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0019\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0018\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0017\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0019\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0018\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0017\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0016\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0017\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0016\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0016\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0017\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0016\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0016\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0016\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "63/63 [==============================] - 3s 17ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 19s 116ms/step - loss: 0.0133\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0042\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 5s 154ms/step - loss: 0.0036\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 5s 149ms/step - loss: 0.0034\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0033\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0032\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0029\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0028\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0027\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0027\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0028\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.0026\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0029\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0025\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.0024\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0023\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0022\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0024\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0023\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0025\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0023\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0024\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0022\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0021\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0025\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0021\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0019\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0022\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0019\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0021\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0018\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0019\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0019\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0019\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0017\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0017\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0017\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0016\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0016\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0016\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0015\n",
            "63/63 [==============================] - 4s 34ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 18s 115ms/step - loss: 0.0140\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0051\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0041\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0035\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0033\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0031\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0030\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0030\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0028\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0026\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0026\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0026\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0025\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0027\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0024\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0025\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0023\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0023\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0022\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0022\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0021\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0021\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0020\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0019\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0020\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0019\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.0019\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.0020\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0020\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0019\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0019\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0018\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0019\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0018\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 0.0018\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0017\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0017\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0017\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0016\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0018\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0017\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0017\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0016\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0016\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0016\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0016\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.0017\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0016\n",
            "63/63 [==============================] - 4s 16ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 19s 114ms/step - loss: 0.0150\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0044\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0038\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 4s 109ms/step - loss: 0.0035\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0035\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0029\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0029\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0029\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0027\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0026\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0027\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0025\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0025\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0029\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0024\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0023\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0024\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0024\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0025\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0022\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0022\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0022\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0024\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0023\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0023\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0023\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0022\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0020\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0022\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0021\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0020\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0019\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0018\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0018\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0020\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.0019\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0020\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0019\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0020\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0020\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.0018\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 0.0018\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.0017\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 0.0018\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.0017\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0017\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0018\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0016\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0017\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 4s 111ms/step - loss: 0.0017\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0016\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0017\n",
            "63/63 [==============================] - 3s 18ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 19s 116ms/step - loss: 0.0130\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0046\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0037\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0034\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0034\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0031\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0031\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0030\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0028\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.0026\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0026\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 4s 114ms/step - loss: 0.0025\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0025\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0025\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0024\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0025\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0023\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0023\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0023\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0023\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0023\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0023\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0025\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0023\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0023\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0022\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0020\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0022\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0020\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0021\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0021\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0022\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0020\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0020\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0019\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0020\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0019\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 6s 197ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0018\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0017\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0018\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 54/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 55/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 56/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 57/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0018\n",
            "Epoch 58/80\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0018\n",
            "Epoch 59/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 60/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0017\n",
            "Epoch 61/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0018\n",
            "Epoch 62/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0018\n",
            "Epoch 63/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0017\n",
            "Epoch 64/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 65/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 66/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 67/80\n",
            "32/32 [==============================] - 4s 123ms/step - loss: 0.0017\n",
            "Epoch 68/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0017\n",
            "Epoch 69/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 70/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 71/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0017\n",
            "Epoch 72/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 73/80\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.0016\n",
            "Epoch 74/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0017\n",
            "Epoch 75/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0016\n",
            "Epoch 76/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 77/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 78/80\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0016\n",
            "Epoch 79/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "Epoch 80/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0016\n",
            "63/63 [==============================] - 3s 20ms/step\n",
            "Epoch 1/80\n",
            "32/32 [==============================] - 20s 122ms/step - loss: 0.0137\n",
            "Epoch 2/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0057\n",
            "Epoch 3/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0037\n",
            "Epoch 4/80\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.0036\n",
            "Epoch 5/80\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.0033\n",
            "Epoch 6/80\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.0033\n",
            "Epoch 7/80\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.0029\n",
            "Epoch 8/80\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0030\n",
            "Epoch 9/80\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.0028\n",
            "Epoch 10/80\n",
            "32/32 [==============================] - 4s 138ms/step - loss: 0.0028\n",
            "Epoch 11/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0027\n",
            "Epoch 12/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0026\n",
            "Epoch 13/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0026\n",
            "Epoch 14/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0025\n",
            "Epoch 15/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0025\n",
            "Epoch 16/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0024\n",
            "Epoch 17/80\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.0024\n",
            "Epoch 18/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0023\n",
            "Epoch 19/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0023\n",
            "Epoch 20/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0022\n",
            "Epoch 21/80\n",
            "32/32 [==============================] - 4s 124ms/step - loss: 0.0023\n",
            "Epoch 22/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0023\n",
            "Epoch 23/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0021\n",
            "Epoch 24/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0023\n",
            "Epoch 25/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0022\n",
            "Epoch 26/80\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.0022\n",
            "Epoch 27/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0020\n",
            "Epoch 28/80\n",
            "32/32 [==============================] - 4s 134ms/step - loss: 0.0021\n",
            "Epoch 29/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0023\n",
            "Epoch 30/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0021\n",
            "Epoch 31/80\n",
            "32/32 [==============================] - 4s 133ms/step - loss: 0.0020\n",
            "Epoch 32/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0020\n",
            "Epoch 33/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0020\n",
            "Epoch 34/80\n",
            "32/32 [==============================] - 4s 135ms/step - loss: 0.0022\n",
            "Epoch 35/80\n",
            "32/32 [==============================] - 4s 132ms/step - loss: 0.0019\n",
            "Epoch 36/80\n",
            "32/32 [==============================] - 7s 216ms/step - loss: 0.0020\n",
            "Epoch 37/80\n",
            "32/32 [==============================] - 5s 146ms/step - loss: 0.0019\n",
            "Epoch 38/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0020\n",
            "Epoch 39/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0019\n",
            "Epoch 40/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0019\n",
            "Epoch 41/80\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.0018\n",
            "Epoch 42/80\n",
            "32/32 [==============================] - 4s 134ms/step - loss: 0.0018\n",
            "Epoch 43/80\n",
            "32/32 [==============================] - 4s 128ms/step - loss: 0.0021\n",
            "Epoch 44/80\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.0019\n",
            "Epoch 45/80\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.0020\n",
            "Epoch 46/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0019\n",
            "Epoch 47/80\n",
            "32/32 [==============================] - 4s 130ms/step - loss: 0.0019\n",
            "Epoch 48/80\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.0018\n",
            "Epoch 49/80\n",
            "32/32 [==============================] - 4s 129ms/step - loss: 0.0018\n",
            "Epoch 50/80\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.0018\n",
            "Epoch 51/80\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.0020\n",
            "Epoch 52/80\n",
            "32/32 [==============================] - 4s 131ms/step - loss: 0.0019\n",
            "Epoch 53/80\n",
            "32/32 [==============================] - 5s 154ms/step - loss: 0.0017\n",
            "Epoch 54/80\n",
            " 9/32 [=======>......................] - ETA: 3s - loss: 0.0018"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-e17d81e7f30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-ffa2192e0faf>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHalvAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfeaa9ad-4c32-4b41-8fd5-d89fcdce155a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05372226391675299, 0.05214374453135038, 0.054463217665213066, 0.05219497378114318, 0.054059696657286514, 0.05674707466450721, 0.053485580318512645]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ykd7kawkvX",
        "outputId": "05e12c27-f612-48b6-a50b-a689586811b4"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05214374453135038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "O622nEj3Krt7",
        "outputId": "f06a48df-c7ac-492d-c2e6-7fd0b35433c0"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "from scipy.interpolate import make_interp_spline\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# Dataset\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 7])\n",
        "y = np.array(loss)\n",
        " \n",
        "X_Y_Spline = make_interp_spline(x, y)\n",
        " \n",
        "# Returns evenly spaced numbers\n",
        "# over a specified interval.\n",
        "X_ = np.linspace(x.min(), x.max(), 100)\n",
        "Y_ = X_Y_Spline(X_)\n",
        " \n",
        "# Plotting the Graph\n",
        "plt.plot(X_, Y_)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RCWp29cXTl3Q",
        "outputId": "b54979a5-a765-4de1-f391-427154585fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}