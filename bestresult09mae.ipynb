{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult09mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "6b016cce-1242-45ae-e166-890215df637e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.9/dist-packages (0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_09.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "6814e7f2-4d7b-4fdd-d688-3a784351e458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t1\n",
            "Rejected: \t32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=83,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA56219E240),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7FA56219E240, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=83,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA56219E240),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FA56219E240, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=83,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA56219E240),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FA56219E240, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=83,\n",
              "                      random_state=RandomState(MT19937) at 0x7FA56219E240)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=83,\n",
              "                      random_state=RandomState(MT19937) at 0x7FA56219E240)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "841bcc03-1a67-4fe8-d37e-24184f4ecf34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 15) dtype=float32 (created by layer 'input_3')>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "a71f7600-59aa-4a83-f834-3f3712936930"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "36/36 [==============================] - 25s 132ms/step - loss: 0.0224\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0085\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0078\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0071\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0064\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0058\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0064\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0059\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0059\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0055\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0056\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0054\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0054\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0054\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0054\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0052\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0053\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0051\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0050\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0052\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0050\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0051\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0058\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0051\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0049\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0048\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0052\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0051\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0048\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0048\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0048\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0047\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0048\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0048\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0048\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0047\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0045\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0047\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0046\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0049\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0045\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0046\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0046\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 150ms/step - loss: 0.0046\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0048\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0050\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0046\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0046\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0045\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0047\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0046\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0045\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0046\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0044\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0045\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0045\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0045\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0046\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0045\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0047\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0045\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0045\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0044\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0043\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0047\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0051\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0046\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0045\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0047\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0044\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0043\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0042\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0046\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0045\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0046\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0046\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0042\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0042\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0041\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0043\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0044\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0044\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0044\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0042\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0042\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0041\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0041\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0041\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0043\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0042\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0042\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0042\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0042\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0042\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0042\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0041\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0040\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0041\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0041\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 151ms/step - loss: 0.0041\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0041\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0039\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0041\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0042\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0043\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0042\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0040\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0041\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0041\n",
            "48/48 [==============================] - 3s 20ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 24s 130ms/step - loss: 0.0188\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0083\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0068\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0063\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0062\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0061\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0063\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0057\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0058\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0056\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0053\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0056\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0055\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0054\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0051\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0050\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0051\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0053\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0052\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0051\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0052\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0052\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0050\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0049\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0050\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0051\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0051\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0053\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0053\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0046\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0046\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0046\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0048\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0047\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0048\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0046\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0046\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0054\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0049\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0049\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0046\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0048\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0047\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0045\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0047\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0044\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0048\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0045\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0048\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0046\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0044\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0044\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0045\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0047\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0044\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0045\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0086\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0064\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0055\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0051\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0051\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0049\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0050\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0052\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0047\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0048\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0046\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0047\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0047\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0046\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0046\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0045\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0045\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0047\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0045\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0046\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0044\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0044\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0043\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0042\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0043\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0044\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0043\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0044\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0043\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0043\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0043\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0042\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0041\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0043\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0043\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0042\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0041\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0040\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0041\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0041\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0041\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0041\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0040\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0041\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0042\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0044\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0042\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0040\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0039\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0040\n",
            "48/48 [==============================] - 4s 26ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 25s 129ms/step - loss: 0.0216\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0085\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0068\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0063\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0062\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0060\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0057\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0056\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0059\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0057\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0055\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0055\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0055\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0057\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0056\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0050\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0051\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0050\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0049\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0049\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0052\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0049\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0051\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0052\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0050\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0048\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0050\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0048\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0049\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0050\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0047\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0048\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0049\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0048\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0048\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0048\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0046\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0049\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0049\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0061\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0057\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0049\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0048\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0049\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0047\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0045\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0046\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0048\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0046\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0044\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0046\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0049\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0044\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0044\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0047\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0046\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0045\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0045\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0044\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0045\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0045\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0049\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0045\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0043\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0046\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0044\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0043\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0044\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0044\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0047\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0044\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0043\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0045\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0045\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0045\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0044\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0044\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0044\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0045\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0042\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0042\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0041\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0042\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0042\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0042\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0041\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0042\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 154ms/step - loss: 0.0041\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0041\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0042\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0040\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0040\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0040\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0043\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0041\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0040\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0040\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0042\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0039\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0040\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0040\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 7s 180ms/step - loss: 0.0041\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0039\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0040\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0040\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0039\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0039\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0040\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0040\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0037\n",
            "48/48 [==============================] - 4s 30ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 24s 155ms/step - loss: 0.0204\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0082\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0072\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0068\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0063\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0060\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0063\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0058\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0057\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0057\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0057\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0055\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0058\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0056\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0054\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0051\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0052\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0055\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0052\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0052\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0051\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0051\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0053\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0050\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0055\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0051\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0049\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0053\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0051\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0048\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0048\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0050\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0049\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0045\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0046\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0047\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0047\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0046\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0047\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0046\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0050\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0047\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0047\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0044\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0048\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0047\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0047\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0044\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0045\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0046\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0045\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0047\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0046\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0046\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0045\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0051\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0044\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0046\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0045\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0043\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0042\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0044\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0043\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0044\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0044\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0042\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0043\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0042\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0042\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0044\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0047\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0044\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0041\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0043\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0047\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0042\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0040\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0041\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0042\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0041\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0042\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0041\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0042\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0041\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0042\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0041\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0040\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0040\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0040\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0040\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0041\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0041\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0044\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0040\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0040\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0049\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0044\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0042\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0041\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0043\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 7s 193ms/step - loss: 0.0040\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0042\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0039\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0039\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0041\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0041\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0039\n",
            "48/48 [==============================] - 3s 22ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 26s 150ms/step - loss: 0.0215\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0086\n",
            "Epoch 3/120\n",
            "11/36 [========>.....................] - ETA: 3s - loss: 0.0073"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-68-e17d81e7f30f>\", line 1, in <module>\n",
            "    GlobalMinimaSearch(mdl.get_weights())\n",
            "  File \"<ipython-input-67-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-67-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-67-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<ipython-input-67-437148bbfb2e>\", line 9, in GlobalMinimaSearch\n",
            "    model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
            "    return concrete_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 755, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 392, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 426, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 167, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b26d6d5-7303-444b-a9fe-9900dbf9e61f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.03990450556191524, 0.04298258916447583, 0.039001188888227484, 0.04136985547022129]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "878830d2-8636-479c-8a0c-4e1db070a100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.039001188888227484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "cb016387-4cbc-4cd5-cf64-853942c55410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa537649430>]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}