{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult03mae1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "01c2abc0-7781-4101-8688-b9c3d94ec347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_03.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "c31c0b5a-66ed-4d77-da56-cc59c96effee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t5\n",
            "Rejected: \t33\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t4\n",
            "Rejected: \t33\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t3\n",
            "Rejected: \t33\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t2\n",
            "Rejected: \t33\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t1\n",
            "Rejected: \t33\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAB80D16C40),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7FAB80D16C40, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAB80D16C40),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FAB80D16C40, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAB80D16C40),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FAB80D16C40, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                      random_state=RandomState(MT19937) at 0x7FAB80D16C40)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=80,\n",
              "                      random_state=RandomState(MT19937) at 0x7FAB80D16C40)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "79cc72d0-248b-4066-a4f6-07c127fcad6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 14) dtype=float32 (created by layer 'input_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "1eaeee0c-923b-4532-a8e8-4f313d350306"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "37/37 [==============================] - 31s 162ms/step - loss: 0.0143\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0058\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0047\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0046\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0039\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0040\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0039\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 11s 289ms/step - loss: 0.0032\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 10s 282ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 11s 293ms/step - loss: 0.0034\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0029\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0032\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0030\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0029\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0028\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0030\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0027\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0029\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0026\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0026\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 7s 198ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0027\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0029\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0027\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0027\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0025\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0024\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0025\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0024\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0024\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0026\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0025\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0026\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0023\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0024\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0025\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0023\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0023\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0022\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0024\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0022\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0022\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0022\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0021\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0024\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0022\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0023\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0023\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 177ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0021\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0021\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0026\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0024\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0023\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0023\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0023\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0022\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0022\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0021\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0022\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0021\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0021\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0020\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "50/50 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 26s 167ms/step - loss: 0.0147\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0052\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0043\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0042\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0033\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0033\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0031\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0032\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0037\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0030\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0030\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0027\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0031\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0028\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0027\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0029\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0026\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0026\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0029\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0026\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0026\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0025\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0025\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0025\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0026\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0027\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0024\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0027\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0025\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0027\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0024\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0025\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0024\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0023\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0024\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0023\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0023\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0022\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0022\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0023\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0022\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0022\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0022\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0022\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0022\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0022\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0023\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0021\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0021\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0025\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0023\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0023\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0022\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0022\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0022\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0022\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0023\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0023\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0023\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0022\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0021\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "50/50 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 25s 121ms/step - loss: 0.0159\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0053\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0045\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0041\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0036\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0034\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0034\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0033\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0030\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0029\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0029\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0028\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0026\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0027\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0025\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0029\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0026\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0026\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0025\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0026\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0026\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0024\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0026\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0025\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0025\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0023\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0025\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0025\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0023\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0025\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0022\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0023\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0024\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0024\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0024\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0022\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0022\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0023\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0022\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0021\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0022\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0022\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0022\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0022\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0024\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0022\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0023\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0021\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0023\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0022\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0021\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0023\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0021\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0022\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0023\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0020\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0020\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0022\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0020\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0021\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0021\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0020\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0022\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0020\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0020\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "50/50 [==============================] - 3s 19ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 28s 164ms/step - loss: 0.0137\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0062\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0051\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0040\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0037\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0035\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0035\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0032\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0035\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0033\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0028\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0029\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0029\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0029\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0027\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0029\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0028\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0028\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0026\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0026\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0026\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0027\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0027\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0025\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0026\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0025\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0026\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0028\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0026\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0027\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0026\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0026\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0023\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0023\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0023\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0026\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0026\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0023\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0023\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0025\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0024\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 7s 174ms/step - loss: 0.0023\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0022\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0024\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0024\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0023\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0023\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0023\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0023\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0021\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0022\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0021\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0022\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0022\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0022\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0021\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0021\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0021\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0020\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0023\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0022\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0020\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0021\n",
            "50/50 [==============================] - 3s 19ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 25s 167ms/step - loss: 0.0165\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0055\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0043\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0037\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0035\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0033\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0035\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0034\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0028\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0028\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0026\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0026\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0026\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0026\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0025\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0025\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0026\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0028\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0027\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0025\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0027\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0025\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0025\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0025\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0024\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0027\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0023\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0026\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0024\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0023\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0023\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0022\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0023\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0024\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0025\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0022\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0023\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0022\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0023\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0024\n",
            "Epoch 75/120\n",
            "18/37 [=============>................] - ETA: 2s - loss: 0.0022"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-20-e17d81e7f30f>\", line 1, in <module>\n",
            "    GlobalMinimaSearch(mdl.get_weights())\n",
            "  File \"<ipython-input-19-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-19-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-19-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<ipython-input-19-437148bbfb2e>\", line 9, in GlobalMinimaSearch\n",
            "    model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
            "    return concrete_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 755, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 392, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 426, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 167, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09160022-7864-4240-ed86-96616fa11c6c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.027250221185025146, 0.0245619705848947, 0.026133879279604658, 0.02614004826347699]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "e9270074-0df0-45ca-cc47-c165e3b5ff44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0245619705848947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "aac7d647-cded-4502-ab15-bcb6c10b840a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fab04fa14f0>]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}