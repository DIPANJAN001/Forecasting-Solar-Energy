{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult01mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "e7e86fda-3842-46e2-ea34-8940abb02e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_01.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "305b1c10-ef0e-4156-e149-db369f3c6b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t11\n",
            "Rejected: \t27\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t11\n",
            "Rejected: \t27\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t11\n",
            "Rejected: \t27\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t11\n",
            "Rejected: \t27\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t9\n",
            "Rejected: \t27\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t8\n",
            "Rejected: \t28\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t7\n",
            "Rejected: \t29\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t6\n",
            "Rejected: \t29\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t5\n",
            "Rejected: \t29\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t4\n",
            "Rejected: \t30\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t3\n",
            "Rejected: \t30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F1919C30D40),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F1919C30D40, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F1919C30D40),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7F1919C30D40, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F1919C30D40),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7F1919C30D40, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                      random_state=RandomState(MT19937) at 0x7F1919C30D40)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=88,\n",
              "                      random_state=RandomState(MT19937) at 0x7F1919C30D40)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "46ec1a32-cfb7-4ca4-b860-63f9e4765857"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 16) dtype=float32 (created by layer 'input_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "0245a09b-6958-43d0-ecb0-693622678ed1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "37/37 [==============================] - 29s 131ms/step - loss: 0.0193\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0079\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0068\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0068\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0060\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0059\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0058\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0053\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0054\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0055\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0053\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0051\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0051\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0057\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0053\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0051\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0050\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0050\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0052\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0055\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0049\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0049\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0047\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0046\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0049\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0050\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0048\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0047\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0047\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0048\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 8s 208ms/step - loss: 0.0047\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0048\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0046\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0050\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0050\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0048\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0046\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0046\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0043\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0047\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0045\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0046\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0045\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0043\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0043\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0044\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0045\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0045\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0046\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0043\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0044\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0043\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0043\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0044\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0044\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0042\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0043\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0044\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0043\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0044\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0044\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0041\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0042\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0042\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0043\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0042\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0041\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0042\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0042\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0042\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0040\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0042\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0041\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0040\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0040\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0046\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0042\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0041\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0040\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0039\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0040\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0039\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0039\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0045\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0042\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0038\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0038\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0038\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0038\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0037\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0038\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0039\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0039\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0039\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0045\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0341\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0060\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0048\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0048\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0047\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0046\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0044\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0044\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0046\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0043\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0043\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 7s 204ms/step - loss: 0.0041\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0042\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0043\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "49/49 [==============================] - 3s 20ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 25s 136ms/step - loss: 0.0222\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0083\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0068\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0062\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0059\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0059\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0056\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0056\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0055\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0052\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0057\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0056\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0050\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0050\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0049\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0052\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0048\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0049\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0048\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0049\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0048\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0048\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0050\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0050\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0048\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0047\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0048\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0047\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0048\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0048\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0047\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0048\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0046\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0045\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0045\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0046\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0046\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0046\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0046\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0044\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0045\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 0.0045\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0044\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0043\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0046\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0044\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0044\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0042\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0043\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0046\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0043\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0043\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0045\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0044\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0046\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0045\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0045\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0042\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0041\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0042\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0043\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0041\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0043\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0042\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0041\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0041\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.0041\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0041\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0041\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0040\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0042\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0042\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0040\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0041\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0050\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0043\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0043\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0043\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0041\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0040\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0039\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0040\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0039\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0042\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0040\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0039\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0038\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0038\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0039\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0039\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0039\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0039\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0039\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0039\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0039\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0053\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0045\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0043\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0041\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0039\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0040\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0041\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0039\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0038\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0038\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0039\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0039\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0038\n",
            "49/49 [==============================] - 3s 21ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 26s 176ms/step - loss: 0.0188\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0084\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0070\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0065\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0059\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0059\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0061\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0056\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0056\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0053\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0054\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0055\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0056\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0052\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0053\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0052\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0053\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0048\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0050\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0050\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0047\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0048\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0050\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0049\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0048\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0047\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0051\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0046\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0047\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0048\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0046\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 7s 203ms/step - loss: 0.0047\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0048\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0046\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0046\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0047\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0045\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0044\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0045\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0045\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0046\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0048\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0046\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0045\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0049\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0045\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0043\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0043\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0045\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0045\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0044\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0049\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0046\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0048\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0045\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0044\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0043\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0042\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0041\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0042\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0042\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0044\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0042\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0042\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0044\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0046\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0040\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0041\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0044\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0041\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 7s 176ms/step - loss: 0.0041\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0040\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0041\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0042\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0041\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0041\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 0.0040\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0041\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0039\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0038\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0041\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0039\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0039\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0039\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0038\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0040\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0040\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0039\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0040\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0038\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0038\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0040\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0040\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0037\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0037\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0038\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0037\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0039\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0038\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0037\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0038\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0036\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0038\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 0.0036\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0036\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0037\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0040\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 8s 206ms/step - loss: 0.0047\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0044\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0043\n",
            "49/49 [==============================] - 3s 20ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 27s 136ms/step - loss: 0.0200\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0081\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0067\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0065\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0061\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0061\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0054\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0054\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0055\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0059\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0057\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 146ms/step - loss: 0.0054\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0056\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0051\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0051\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0053\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0050\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0049\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0049\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0049\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0048\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0047\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0056\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0048\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0047\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0047\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0047\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0052\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0047\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0047\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0045\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0046\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0047\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0046\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0047\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0048\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0049\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0046\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0048\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0046\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0044\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0045\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0046\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0044\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0046\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0044\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0047\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0044\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0046\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0043\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0043\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0046\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0042\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0045\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0042\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0042\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0044\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 0.0043\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0045\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0043\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0041\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0043\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0048\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0041\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0044\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0042\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0041\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0040\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0042\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0047\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0041\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0040\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0040\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0041\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0040\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0040\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0040\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0041\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0039\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0039\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0039\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0040\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0038\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0040\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0042\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0041\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0038\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0037\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0039\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0038\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0037\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0038\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0039\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0039\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0039\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0038\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0038\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0038\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0038\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0036\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0039\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0036\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0037\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0036\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0043\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0037\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0036\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0037\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.0038\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0037\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0036\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0037\n",
            "49/49 [==============================] - 3s 25ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 26s 174ms/step - loss: 0.0193\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0075\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0070\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0067\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0060\n",
            "Epoch 6/120\n",
            "23/37 [=================>............] - ETA: 2s - loss: 0.0062"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-23-e17d81e7f30f>\", line 1, in <module>\n",
            "    GlobalMinimaSearch(mdl.get_weights())\n",
            "  File \"<ipython-input-22-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-22-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-22-437148bbfb2e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<ipython-input-22-437148bbfb2e>\", line 9, in GlobalMinimaSearch\n",
            "    model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
            "    return concrete_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 748, in getmodule\n",
            "    if f == _filesbymodname.get(modname, None):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace9308d-ad72-4e0b-f1c5-687f2bb94422"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.03406055569276922, 0.03274255192713846, 0.03641437414036544, 0.034600053666302116]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "07aad8bc-9925-49fa-b96a-10f89cbad733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03274255192713846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "0b72f831-a356-4338-f404-441a14d1a091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f18f27d8a90>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}