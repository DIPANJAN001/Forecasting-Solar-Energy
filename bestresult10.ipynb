{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "dd5e75a3-36ef-4074-9e39-6fd4b3ca6194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_10.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "3a30a5c5-fca1-4faa-c58d-bf60be88abc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t9\n",
            "Rejected: \t23\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t9\n",
            "Rejected: \t23\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t9\n",
            "Rejected: \t23\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t18\n",
            "Tentative: \t8\n",
            "Rejected: \t23\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t18\n",
            "Tentative: \t8\n",
            "Rejected: \t23\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t6\n",
            "Rejected: \t23\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t6\n",
            "Rejected: \t23\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t6\n",
            "Rejected: \t23\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=101,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F601A9E0E40),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F601A9E0E40, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "e471d394-bfc8-441c-8228-c0c142f01293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 22) dtype=float32 (created by layer 'input_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "class CustomAdam(optimizers.Adam):\n",
        "    def __init__(self, new_idea_param=0.1, *args, **kwargs):\n",
        "        self.new_idea_param = new_idea_param\n",
        "        super(CustomAdam, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        new_idea_t = self.new_idea_param * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        self.weights = [self.iterations] + ms + vs\n",
        "\n",
        "        for p, g, m, v in zip(params, grads, ms, vs):\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - new_idea_t * g\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            self.updates.append(K.update(p, p_t))\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "g8F6yCGl21Sz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "class GradientAdam(optimizers.Adam):\n",
        "    def __init__(self, gradient_param=0.1, *args, **kwargs):\n",
        "        self.gradient_param = gradient_param\n",
        "        super(GradientAdam, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n",
        "\n",
        "        t = K.cast(self.iterations, K.floatx()) + 1\n",
        "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
        "\n",
        "        gradient_t = self.gradient_param * grads\n",
        "\n",
        "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
        "        self.weights = [self.iterations] + ms + vs\n",
        "\n",
        "        for p, g, m, v, g_t in zip(params, grads, ms, vs, gradient_t):\n",
        "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
        "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) + g_t\n",
        "\n",
        "            self.updates.append(K.update(m, m_t))\n",
        "            self.updates.append(K.update(v, v_t))\n",
        "            self.updates.append(K.update(p, p_t))\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "J_gyZaJU8f4W"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class AdaptiveAdam(Adam):\n",
        "    def __init__(self, *args, factor=0.5, patience=5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.factor = factor\n",
        "        self.patience = patience\n",
        "        self.wait = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_weights = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        current_loss = loss()\n",
        "        if current_loss < self.best_loss:\n",
        "            self.best_loss = current_loss\n",
        "            self.best_weights = params\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.wait = 0\n",
        "                self.lr = self.lr * self.factor\n",
        "                params = self.best_weights\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(g, \"float32\") for g in grads]\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "PmHcGR7JJLo_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class MomentumAdam(Adam):\n",
        "    def __init__(self, *args, momentum=0.9, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.momentum = momentum\n",
        "        self.velocities = [tf.Variable(tf.zeros_like(p), trainable=False) for p in self.weights]\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = []\n",
        "        for p, g, v in zip(params, grads, self.velocities):\n",
        "            v_t = self.momentum * v - self.lr * g\n",
        "            p_t = p + v_t\n",
        "            self.updates.append(p_t)\n",
        "            self.updates.append(v_t)\n",
        "        return self.updates\n"
      ],
      "metadata": {
        "id": "Zqo9SvzjbTtq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(np.sqrt(mean_squared_error(y,y_test)))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "3a338748-82c5-4dd4-f668-be5f3eb3f2da"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "36/36 [==============================] - 24s 158ms/step - loss: 0.0117\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0042\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0037\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0034\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0032\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0031\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0030\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0030\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0028\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0031\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0028\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0027\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0028\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0027\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0026\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0026\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0025\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0025\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0026\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0025\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0026\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0029\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0025\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0024\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0024\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0025\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0025\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0025\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0025\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0025\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0026\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0024\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0023\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0025\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 7s 189ms/step - loss: 0.0028\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0023\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0024\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0024\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0023\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0022\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0022\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0023\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 7s 199ms/step - loss: 0.0023\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0022\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0024\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0022\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0023\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0023\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 124ms/step - loss: 0.0022\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0022\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 7s 192ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0021\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0021\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0023\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0021\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0022\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0021\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0022\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0021\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0021\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0021\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "48/48 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 22s 123ms/step - loss: 0.0097\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0042\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0042\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0032\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0034\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0033\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0030\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0029\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0030\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0030\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0030\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0027\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0028\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0028\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0030\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0028\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0027\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0026\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0025\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0025\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0026\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0026\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 7s 193ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0025\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0026\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0026\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0026\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 7s 195ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0025\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0026\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0026\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0025\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0025\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0026\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0024\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0027\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0024\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0024\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0024\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0026\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 6s 151ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0022\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0023\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0022\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0023\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0023\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0024\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0023\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0023\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0021\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 7s 189ms/step - loss: 0.0023\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0023\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0022\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0022\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0023\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 151ms/step - loss: 0.0023\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0023\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0022\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0022\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0021\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0021\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0021\n",
            "48/48 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 22s 115ms/step - loss: 0.0135\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0044\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0035\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0036\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 7s 209ms/step - loss: 0.0036\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0030\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0032\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0032\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 4s 114ms/step - loss: 0.0029\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0029\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0027\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0027\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0027\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0027\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 151ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 4s 114ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0027\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0025\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0027\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0026\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0027\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0025\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0026\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0026\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0030\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 7s 191ms/step - loss: 0.0025\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0025\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0026\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0026\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0025\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 7s 192ms/step - loss: 0.0024\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0025\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 0.0025\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0025\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0023\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0023\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0024\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0024\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0026\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0024\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0022\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0024\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 4s 114ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0024\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 4s 113ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0022\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0022\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0021\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0024\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0023\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0023\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0022\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0021\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0021\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0022\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0020\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0023\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 123ms/step - loss: 0.0021\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0022\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0022\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0023\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0027\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 7s 188ms/step - loss: 0.0030\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0025\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0024\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0024\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0023\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0023\n",
            "48/48 [==============================] - 3s 21ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 21s 117ms/step - loss: 0.0110\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0041\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0038\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0033\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0032\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 4s 113ms/step - loss: 0.0032\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0036\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0032\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0029\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0029\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0027\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0027\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0027\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 4s 113ms/step - loss: 0.0028\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0027\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0028\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0028\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0029\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 4s 114ms/step - loss: 0.0028\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 4s 113ms/step - loss: 0.0026\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0026\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0025\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0027\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0026\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0026\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0025\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0026\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0025\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 4s 113ms/step - loss: 0.0024\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0025\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 4s 118ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0024\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0025\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0026\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 4s 125ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0023\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 5s 122ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0023\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0024\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0023\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0025\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0023\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0024\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.0023\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0022\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0022\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0024\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 4s 114ms/step - loss: 0.0023\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.0024\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0021\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0022\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.0023\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 4s 114ms/step - loss: 0.0022\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0023\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0021\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 143ms/step - loss: 0.0023\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0022\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0021\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 4s 116ms/step - loss: 0.0021\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0022\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 4s 115ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0020\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 4s 124ms/step - loss: 0.0020\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 4s 122ms/step - loss: 0.0022\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 125ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0020\n",
            "48/48 [==============================] - 4s 20ms/step\n",
            "Epoch 1/120\n",
            " 2/36 [>.............................] - ETA: 4s - loss: 0.0303  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-35-e17d81e7f30f>\", line 1, in <module>\n",
            "    GlobalMinimaSearch(mdl.get_weights())\n",
            "  File \"<ipython-input-34-3b28ff99804e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-34-3b28ff99804e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-34-3b28ff99804e>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"<ipython-input-34-3b28ff99804e>\", line 9, in GlobalMinimaSearch\n",
            "    model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 915, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 947, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 2453, in __call__\n",
            "    return graph_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1860, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 497, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 365, in _fixed_getinnerframes\n",
            "    aux = traceback.extract_tb(etb)\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 72, in extract_tb\n",
            "    return StackSummary.extract(walk_tb(tb), limit=limit)\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 362, in extract\n",
            "    linecache.checkcache(filename)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa32fc0-dcdb-4285-bffc-39d932510a2c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.067048762299756, 0.06937578704381764, 0.06620104825355451, 0.06768674469404547]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce5c10b-9ac9-444c-80a1-34ff7f58d245"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06620104825355451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "69e93128-6774-461c-c821-34095027761e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5ff9988970>]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}