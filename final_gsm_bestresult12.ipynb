{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/final_gsm_bestresult12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "fa8eb277-8436-4ef8-d073-fdfe6634ff95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_09.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "c03a73a5-4c2f-40c2-9f3e-ffa22ee3b92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t4\n",
            "Rejected: \t32\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t3\n",
            "Rejected: \t32\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t32\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t1\n",
            "Rejected: \t32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=83,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F8F0C7E4240),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F8F0C7E4240, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "b0f78688-e64e-45c8-dc05-217c2230c523"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 15) dtype=float32 (created by layer 'input_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='relu',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='relu'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='relu',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='relu'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.002), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=75, batch_size=64)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(np.sqrt(mean_squared_error(y,y_test)))\n",
        "  best_weights= model.get_weights()\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuddmGCf_1dR",
        "outputId": "0f99bf64-2b1b-48ca-d6ba-98d61c4e3d0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "72/72 [==============================] - 21s 83ms/step - loss: 0.0153\n",
            "Epoch 2/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0076\n",
            "Epoch 3/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0074\n",
            "Epoch 4/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0061\n",
            "Epoch 5/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0062\n",
            "Epoch 6/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0063\n",
            "Epoch 7/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0060\n",
            "Epoch 8/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0059\n",
            "Epoch 9/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0058\n",
            "Epoch 10/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0053\n",
            "Epoch 11/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0056\n",
            "Epoch 12/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0053\n",
            "Epoch 13/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0056\n",
            "Epoch 14/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0053\n",
            "Epoch 15/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0053\n",
            "Epoch 16/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0053\n",
            "Epoch 17/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0054\n",
            "Epoch 18/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0051\n",
            "Epoch 19/75\n",
            "72/72 [==============================] - 7s 94ms/step - loss: 0.0050\n",
            "Epoch 20/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0054\n",
            "Epoch 21/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0050\n",
            "Epoch 22/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0051\n",
            "Epoch 23/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0049\n",
            "Epoch 24/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0050\n",
            "Epoch 25/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0050\n",
            "Epoch 26/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0048\n",
            "Epoch 27/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0050\n",
            "Epoch 28/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0049\n",
            "Epoch 29/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 30/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0049\n",
            "Epoch 31/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0051\n",
            "Epoch 32/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0049\n",
            "Epoch 33/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0048\n",
            "Epoch 34/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0048\n",
            "Epoch 35/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0048\n",
            "Epoch 36/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0045\n",
            "Epoch 37/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0047\n",
            "Epoch 38/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0049\n",
            "Epoch 39/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 40/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0046\n",
            "Epoch 41/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 42/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 43/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 44/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0048\n",
            "Epoch 45/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0046\n",
            "Epoch 46/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0045\n",
            "Epoch 47/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 48/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 49/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0045\n",
            "Epoch 50/75\n",
            "72/72 [==============================] - 7s 96ms/step - loss: 0.0046\n",
            "Epoch 51/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 52/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0045\n",
            "Epoch 53/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 54/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 55/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0044\n",
            "Epoch 56/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 57/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 58/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 59/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 60/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0045\n",
            "Epoch 61/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0043\n",
            "Epoch 62/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 63/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 64/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 65/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0049\n",
            "Epoch 66/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0043\n",
            "Epoch 67/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0043\n",
            "Epoch 68/75\n",
            "72/72 [==============================] - 6s 90ms/step - loss: 0.0044\n",
            "Epoch 69/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0043\n",
            "Epoch 70/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0072\n",
            "Epoch 71/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0055\n",
            "Epoch 72/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 73/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0047\n",
            "Epoch 74/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 75/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0045\n",
            "48/48 [==============================] - 3s 18ms/step\n",
            "Epoch 1/75\n",
            "72/72 [==============================] - 22s 83ms/step - loss: 0.0162\n",
            "Epoch 2/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0078\n",
            "Epoch 3/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0068\n",
            "Epoch 4/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0065\n",
            "Epoch 5/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0062\n",
            "Epoch 6/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0060\n",
            "Epoch 7/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0059\n",
            "Epoch 8/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0058\n",
            "Epoch 9/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0057\n",
            "Epoch 10/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0055\n",
            "Epoch 11/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0057\n",
            "Epoch 12/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0055\n",
            "Epoch 13/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0053\n",
            "Epoch 14/75\n",
            "72/72 [==============================] - 6s 87ms/step - loss: 0.0054\n",
            "Epoch 15/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 16/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0055\n",
            "Epoch 17/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0054\n",
            "Epoch 18/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0053\n",
            "Epoch 19/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0052\n",
            "Epoch 20/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0049\n",
            "Epoch 21/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 22/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0050\n",
            "Epoch 23/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 24/75\n",
            "72/72 [==============================] - 6s 87ms/step - loss: 0.0051\n",
            "Epoch 25/75\n",
            "72/72 [==============================] - 7s 91ms/step - loss: 0.0050\n",
            "Epoch 26/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0049\n",
            "Epoch 27/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 28/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 29/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 30/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 31/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 32/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 33/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 34/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 35/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 36/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 37/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 38/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 39/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 40/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 41/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 42/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0161\n",
            "Epoch 43/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0080\n",
            "Epoch 44/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0057\n",
            "Epoch 45/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0056\n",
            "Epoch 46/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0052\n",
            "Epoch 47/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0054\n",
            "Epoch 48/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 49/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0051\n",
            "Epoch 50/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0049\n",
            "Epoch 51/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 52/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0049\n",
            "Epoch 53/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 54/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 55/75\n",
            "72/72 [==============================] - 7s 95ms/step - loss: 0.0048\n",
            "Epoch 56/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0049\n",
            "Epoch 57/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 58/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0046\n",
            "Epoch 59/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 60/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 61/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 62/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 63/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 64/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 65/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 66/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 67/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 68/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 69/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 70/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 71/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 72/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 73/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 74/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0043\n",
            "Epoch 75/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "48/48 [==============================] - 3s 17ms/step\n",
            "Epoch 1/75\n",
            "72/72 [==============================] - 21s 84ms/step - loss: 0.0151\n",
            "Epoch 2/75\n",
            "72/72 [==============================] - 6s 87ms/step - loss: 0.0073\n",
            "Epoch 3/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0066\n",
            "Epoch 4/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0068\n",
            "Epoch 5/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0062\n",
            "Epoch 6/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0061\n",
            "Epoch 7/75\n",
            "72/72 [==============================] - 7s 95ms/step - loss: 0.0059\n",
            "Epoch 8/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0058\n",
            "Epoch 9/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0057\n",
            "Epoch 10/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0056\n",
            "Epoch 11/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0057\n",
            "Epoch 12/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0054\n",
            "Epoch 13/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0055\n",
            "Epoch 14/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0052\n",
            "Epoch 15/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0053\n",
            "Epoch 16/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0051\n",
            "Epoch 17/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0054\n",
            "Epoch 18/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0051\n",
            "Epoch 19/75\n",
            "72/72 [==============================] - 6s 89ms/step - loss: 0.0050\n",
            "Epoch 20/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0052\n",
            "Epoch 21/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0050\n",
            "Epoch 22/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0053\n",
            "Epoch 23/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 24/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0050\n",
            "Epoch 25/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 26/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 27/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0048\n",
            "Epoch 28/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0051\n",
            "Epoch 29/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 30/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 31/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0047\n",
            "Epoch 32/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 33/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 34/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 35/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 36/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0049\n",
            "Epoch 37/75\n",
            "72/72 [==============================] - 7s 92ms/step - loss: 0.0048\n",
            "Epoch 38/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0048\n",
            "Epoch 39/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 40/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 41/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 42/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 43/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 44/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 45/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0048\n",
            "Epoch 46/75\n",
            "72/72 [==============================] - 6s 87ms/step - loss: 0.0047\n",
            "Epoch 47/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 48/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0047\n",
            "Epoch 49/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 50/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 51/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 52/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 53/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 54/75\n",
            "72/72 [==============================] - 6s 90ms/step - loss: 0.0045\n",
            "Epoch 55/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 56/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 57/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 58/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 59/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 60/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 61/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 62/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 63/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 64/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 65/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0043\n",
            "Epoch 66/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0044\n",
            "Epoch 67/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0043\n",
            "Epoch 68/75\n",
            "72/72 [==============================] - 7s 95ms/step - loss: 0.0044\n",
            "Epoch 69/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 70/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 71/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0043\n",
            "Epoch 72/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 73/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0042\n",
            "Epoch 74/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 75/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0044\n",
            "48/48 [==============================] - 3s 18ms/step\n",
            "Epoch 1/75\n",
            "72/72 [==============================] - 20s 82ms/step - loss: 0.0163\n",
            "Epoch 2/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0080\n",
            "Epoch 3/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0070\n",
            "Epoch 4/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0066\n",
            "Epoch 5/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0062\n",
            "Epoch 6/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0061\n",
            "Epoch 7/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0058\n",
            "Epoch 8/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0058\n",
            "Epoch 9/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0057\n",
            "Epoch 10/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0055\n",
            "Epoch 11/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0065\n",
            "Epoch 12/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0057\n",
            "Epoch 13/75\n",
            "72/72 [==============================] - 7s 93ms/step - loss: 0.0056\n",
            "Epoch 14/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0052\n",
            "Epoch 15/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0052\n",
            "Epoch 16/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0051\n",
            "Epoch 17/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0050\n",
            "Epoch 18/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0050\n",
            "Epoch 19/75\n",
            "72/72 [==============================] - 6s 82ms/step - loss: 0.0050\n",
            "Epoch 20/75\n",
            "72/72 [==============================] - 6s 81ms/step - loss: 0.0050\n",
            "Epoch 21/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0051\n",
            "Epoch 22/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 23/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 24/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 25/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0049\n",
            "Epoch 26/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0050\n",
            "Epoch 27/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 28/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0049\n",
            "Epoch 29/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 30/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0048\n",
            "Epoch 31/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0047\n",
            "Epoch 32/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0049\n",
            "Epoch 33/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 34/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 35/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 36/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 37/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 38/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 39/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 40/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 41/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 42/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 43/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 44/75\n",
            "72/72 [==============================] - 7s 96ms/step - loss: 0.0045\n",
            "Epoch 45/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 46/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 47/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 48/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 49/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0047\n",
            "Epoch 50/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 51/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 52/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 53/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 54/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 55/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 56/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 57/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 58/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 59/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 60/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 61/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 62/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 63/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 64/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 65/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 66/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0042\n",
            "Epoch 67/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 68/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 69/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 70/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0043\n",
            "Epoch 71/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 72/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0043\n",
            "Epoch 73/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 74/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0043\n",
            "Epoch 75/75\n",
            "72/72 [==============================] - 7s 96ms/step - loss: 0.0044\n",
            "48/48 [==============================] - 3s 18ms/step\n",
            "Epoch 1/75\n",
            "72/72 [==============================] - 22s 87ms/step - loss: 0.0182\n",
            "Epoch 2/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0078\n",
            "Epoch 3/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0070\n",
            "Epoch 4/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0065\n",
            "Epoch 5/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0060\n",
            "Epoch 6/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0064\n",
            "Epoch 7/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0060\n",
            "Epoch 8/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0058\n",
            "Epoch 9/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0054\n",
            "Epoch 10/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0058\n",
            "Epoch 11/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0055\n",
            "Epoch 12/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0052\n",
            "Epoch 13/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0054\n",
            "Epoch 14/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0052\n",
            "Epoch 15/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 16/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0054\n",
            "Epoch 17/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0052\n",
            "Epoch 18/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0053\n",
            "Epoch 19/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 20/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0051\n",
            "Epoch 21/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0050\n",
            "Epoch 22/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0052\n",
            "Epoch 23/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 24/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0050\n",
            "Epoch 25/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0049\n",
            "Epoch 26/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 27/75\n",
            "72/72 [==============================] - 7s 95ms/step - loss: 0.0051\n",
            "Epoch 28/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0057\n",
            "Epoch 29/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0050\n",
            "Epoch 30/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 31/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0048\n",
            "Epoch 32/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 33/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0048\n",
            "Epoch 34/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 35/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 36/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0047\n",
            "Epoch 37/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 38/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 39/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0046\n",
            "Epoch 40/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 41/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 42/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0046\n",
            "Epoch 43/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0047\n",
            "Epoch 44/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 45/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 46/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "Epoch 47/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 48/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 49/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0048\n",
            "Epoch 50/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 51/75\n",
            "72/72 [==============================] - 6s 87ms/step - loss: 0.0045\n",
            "Epoch 52/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0050\n",
            "Epoch 53/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 54/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 55/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 56/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 57/75\n",
            "72/72 [==============================] - 6s 86ms/step - loss: 0.0044\n",
            "Epoch 58/75\n",
            "72/72 [==============================] - 7s 95ms/step - loss: 0.0044\n",
            "Epoch 59/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0046\n",
            "Epoch 60/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 61/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 62/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0047\n",
            "Epoch 63/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 64/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0046\n",
            "Epoch 65/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0044\n",
            "Epoch 66/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 67/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 68/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 69/75\n",
            "72/72 [==============================] - 6s 85ms/step - loss: 0.0043\n",
            "Epoch 70/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0043\n",
            "Epoch 71/75\n",
            "72/72 [==============================] - 6s 83ms/step - loss: 0.0045\n",
            "Epoch 72/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0043\n",
            "Epoch 73/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0043\n",
            "Epoch 74/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0045\n",
            "Epoch 75/75\n",
            "72/72 [==============================] - 6s 84ms/step - loss: 0.0047\n",
            "48/48 [==============================] - 3s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45612808-4af7-4670-ac1b-1acec7ce4ab6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08377927841317555, 0.09009077832209982, 0.08461076623210706, 0.08624892703134622, 0.08574718745357517]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ykd7kawkvX",
        "outputId": "dcea189c-7805-4840-af83-9fd49c06ad9b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08377927841317555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "7707bb26-68c8-42ec-b95c-67a498d01217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8ef9a08460>]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}