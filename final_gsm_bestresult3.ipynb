{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/final_gsm_bestresult3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "62a45691-169d-4edc-8d13-74ac2d90d4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.8/dist-packages (0.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_02.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "3b4838d7-c532-4ab2-ca8d-0ef1e2e8d4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t31\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F034A678D40),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F034A678D40, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "768c46f5-9474-4c26-d430-176b380a7f60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 15) dtype=float32 (created by layer 'input_3')>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='relu',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='relu'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='relu',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='relu'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.002), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=75, batch_size=64)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(np.sqrt(mean_squared_error(y,y_test)))\n",
        "  best_weights= model.get_weights()\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "05968f98-d45d-4535-fd55-e578e5eb9da0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "66/66 [==============================] - 23s 93ms/step - loss: 0.0156\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0066\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0060\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0059\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0058\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0051\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0052\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0049\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0052\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0049\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0049\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0048\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0047\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0048\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 6s 85ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0044\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0046\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0047\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0044\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0045\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0045\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0043\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0042\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0044\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 85ms/step - loss: 0.0047\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0045\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0043\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0042\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0042\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 7s 110ms/step - loss: 0.0042\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0040\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0042\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0041\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0041\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0040\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0042\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0040\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0042\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0039\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0041\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0042\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0041\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0039\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0040\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0040\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0038\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0039\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0038\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0038\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0040\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 7s 113ms/step - loss: 0.0040\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0040\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0040\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0038\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0039\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 85ms/step - loss: 0.0038\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0038\n",
            "65/65 [==============================] - 3s 18ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 23s 93ms/step - loss: 0.0130\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0066\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0058\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0056\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0053\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0051\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0051\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0050\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0050\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0047\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0048\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0050\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0046\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0044\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 7s 106ms/step - loss: 0.0045\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0044\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0043\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0043\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0045\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0044\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0043\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0044\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0045\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0045\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0042\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 85ms/step - loss: 0.0041\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0041\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0044\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0042\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0043\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0041\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 7s 109ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 8s 123ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0040\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0039\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0042\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0041\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0041\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0041\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0039\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0038\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0042\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0039\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 7s 110ms/step - loss: 0.0040\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0039\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0040\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0041\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0040\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0038\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0040\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0038\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0041\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0039\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 99ms/step - loss: 0.0038\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0040\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0038\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0040\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0038\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0038\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0037\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0037\n",
            "65/65 [==============================] - 4s 19ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 34s 97ms/step - loss: 0.0143\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0067\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0060\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0060\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0056\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 8s 117ms/step - loss: 0.0052\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0050\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0051\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0050\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0049\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0046\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0048\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0047\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0046\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0046\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0046\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0046\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0044\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0045\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0045\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0045\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0043\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0044\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0043\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0043\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0042\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0042\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0041\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0043\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0043\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0042\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0040\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0041\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 7s 112ms/step - loss: 0.0040\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0041\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0041\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0040\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0042\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0041\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0039\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0041\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0039\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0041\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0038\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0038\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0039\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0038\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0038\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0039\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0040\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0040\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0037\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0038\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0039\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0039\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 8s 119ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0037\n",
            "65/65 [==============================] - 4s 23ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 24s 100ms/step - loss: 0.0148\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0069\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 7s 113ms/step - loss: 0.0059\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 7s 108ms/step - loss: 0.0058\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0054\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0052\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0053\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0050\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0050\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0049\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 7s 111ms/step - loss: 0.0047\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0050\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 8s 114ms/step - loss: 0.0048\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 8s 120ms/step - loss: 0.0048\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0046\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0045\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 7s 106ms/step - loss: 0.0044\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0045\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0043\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 8s 120ms/step - loss: 0.0046\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0044\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0044\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0043\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0042\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0046\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0041\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0042\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0040\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0042\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0042\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0044\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0041\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 6s 86ms/step - loss: 0.0040\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0042\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0039\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 7s 110ms/step - loss: 0.0040\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 7s 105ms/step - loss: 0.0039\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0040\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 8s 122ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0040\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0038\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0038\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0039\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0039\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0038\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0039\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0038\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 87ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0037\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0038\n",
            "65/65 [==============================] - 3s 19ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 23s 101ms/step - loss: 0.0139\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0070\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0064\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0054\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0056\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0055\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0051\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0052\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0049\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 8s 119ms/step - loss: 0.0048\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0047\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0047\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0049\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0047\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0044\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0046\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0044\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0044\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0046\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0044\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0043\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0046\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0044\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0041\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0043\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0043\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0045\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0041\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0041\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0041\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0042\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0046\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0042\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0042\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0041\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0041\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0040\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0041\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0040\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0041\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0041\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0040\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0040\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0039\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 6s 88ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0039\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0038\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0039\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0040\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0039\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0037\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0039\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0039\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0039\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0039\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 8s 121ms/step - loss: 0.0039\n",
            "65/65 [==============================] - 4s 24ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 23s 97ms/step - loss: 0.0139\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0068\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0063\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0058\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0056\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0057\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0052\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0049\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0050\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0050\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0049\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0047\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0045\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0047\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0047\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0045\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0044\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0045\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0045\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0045\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0047\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0044\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0044\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 8s 122ms/step - loss: 0.0044\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0042\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0042\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0043\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0042\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0042\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0041\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0042\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0044\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0040\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0040\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0043\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0041\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0039\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0040\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0038\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 7s 105ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 7s 107ms/step - loss: 0.0040\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 7s 111ms/step - loss: 0.0038\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 7s 106ms/step - loss: 0.0041\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0038\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0039\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0039\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0040\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0039\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0038\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0037\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0039\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0037\n",
            "65/65 [==============================] - 3s 20ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 25s 105ms/step - loss: 0.0136\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0075\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0058\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0056\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0055\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0053\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 8s 125ms/step - loss: 0.0051\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0053\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0051\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0049\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0048\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0047\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0046\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 6s 89ms/step - loss: 0.0046\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0047\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0045\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0045\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0044\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0044\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0045\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0046\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0043\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0043\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0044\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0043\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0045\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0044\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0043\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0040\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0040\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0041\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0042\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0042\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 8s 121ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0042\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0040\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0041\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0042\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0042\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0040\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0041\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0040\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0040\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0039\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0041\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0042\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 6s 90ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0039\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0042\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0039\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0039\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 7s 105ms/step - loss: 0.0038\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0038\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0039\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0037\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 8s 119ms/step - loss: 0.0040\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0037\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0038\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0040\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0039\n",
            "65/65 [==============================] - 3s 21ms/step\n",
            "Epoch 1/75\n",
            "66/66 [==============================] - 24s 103ms/step - loss: 0.0138\n",
            "Epoch 2/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0064\n",
            "Epoch 3/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0064\n",
            "Epoch 4/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0056\n",
            "Epoch 5/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0055\n",
            "Epoch 6/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0053\n",
            "Epoch 7/75\n",
            "66/66 [==============================] - 7s 108ms/step - loss: 0.0053\n",
            "Epoch 8/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0050\n",
            "Epoch 9/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0050\n",
            "Epoch 10/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0051\n",
            "Epoch 11/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0048\n",
            "Epoch 12/75\n",
            "66/66 [==============================] - 7s 98ms/step - loss: 0.0047\n",
            "Epoch 13/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0045\n",
            "Epoch 14/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0046\n",
            "Epoch 15/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "66/66 [==============================] - 7s 107ms/step - loss: 0.0045\n",
            "Epoch 17/75\n",
            "66/66 [==============================] - 7s 98ms/step - loss: 0.0043\n",
            "Epoch 18/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0046\n",
            "Epoch 19/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0044\n",
            "Epoch 20/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0044\n",
            "Epoch 21/75\n",
            "66/66 [==============================] - 8s 121ms/step - loss: 0.0043\n",
            "Epoch 22/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0045\n",
            "Epoch 23/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0046\n",
            "Epoch 24/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0042\n",
            "Epoch 25/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0044\n",
            "Epoch 26/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 27/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 28/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0042\n",
            "Epoch 29/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0042\n",
            "Epoch 30/75\n",
            "66/66 [==============================] - 7s 106ms/step - loss: 0.0043\n",
            "Epoch 31/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0041\n",
            "Epoch 32/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0042\n",
            "Epoch 33/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0041\n",
            "Epoch 34/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0043\n",
            "Epoch 35/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0041\n",
            "Epoch 36/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 37/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0043\n",
            "Epoch 39/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0042\n",
            "Epoch 40/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0042\n",
            "Epoch 42/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0041\n",
            "Epoch 43/75\n",
            "66/66 [==============================] - 7s 100ms/step - loss: 0.0042\n",
            "Epoch 44/75\n",
            "66/66 [==============================] - 7s 106ms/step - loss: 0.0041\n",
            "Epoch 45/75\n",
            "66/66 [==============================] - 6s 97ms/step - loss: 0.0041\n",
            "Epoch 46/75\n",
            "66/66 [==============================] - 7s 107ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "66/66 [==============================] - 6s 98ms/step - loss: 0.0041\n",
            "Epoch 49/75\n",
            "66/66 [==============================] - 7s 104ms/step - loss: 0.0040\n",
            "Epoch 50/75\n",
            "66/66 [==============================] - 6s 93ms/step - loss: 0.0039\n",
            "Epoch 51/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0040\n",
            "Epoch 52/75\n",
            "66/66 [==============================] - 8s 120ms/step - loss: 0.0039\n",
            "Epoch 53/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0039\n",
            "Epoch 54/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0038\n",
            "Epoch 55/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "66/66 [==============================] - 6s 91ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0039\n",
            "Epoch 58/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0038\n",
            "Epoch 61/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0040\n",
            "Epoch 62/75\n",
            "66/66 [==============================] - 7s 106ms/step - loss: 0.0038\n",
            "Epoch 63/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0039\n",
            "Epoch 64/75\n",
            "66/66 [==============================] - 6s 94ms/step - loss: 0.0039\n",
            "Epoch 65/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0038\n",
            "Epoch 66/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0039\n",
            "Epoch 67/75\n",
            "66/66 [==============================] - 6s 92ms/step - loss: 0.0040\n",
            "Epoch 68/75\n",
            "66/66 [==============================] - 6s 96ms/step - loss: 0.0039\n",
            "Epoch 69/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0038\n",
            "Epoch 70/75\n",
            "66/66 [==============================] - 7s 102ms/step - loss: 0.0039\n",
            "Epoch 71/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0039\n",
            "Epoch 72/75\n",
            "66/66 [==============================] - 6s 95ms/step - loss: 0.0038\n",
            "Epoch 73/75\n",
            "66/66 [==============================] - 7s 99ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "66/66 [==============================] - 7s 103ms/step - loss: 0.0037\n",
            "Epoch 75/75\n",
            "66/66 [==============================] - 7s 101ms/step - loss: 0.0038\n",
            "65/65 [==============================] - 3s 20ms/step\n",
            "Epoch 1/75\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m       outputs = reduce_per_replica(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2888\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1029\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \"\"\"\n\u001b[0;32m--> 537\u001b[0;31m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[1;32m    538\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36m_WhileGrad\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0mnum_intermediates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhile_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnum_original_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m   grads = [\n\u001b[0m\u001b[1;32m    362\u001b[0m       \u001b[0m_preprocess_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=g-complex-comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    361\u001b[0m   grads = [\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0m_preprocess_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=g-complex-comprehension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m       for grad, body_out, while_in, while_out in zip(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36m_preprocess_grad\u001b[0;34m(grad, body_graph_output, while_op_input, while_op_output)\u001b[0m\n\u001b[1;32m    579\u001b[0m       default_gradient.supports_default_grad(while_op_input) and grad is None):\n\u001b[0;32m--> 580\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_zeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhile_op_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhile_op_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/while_v2.py\u001b[0m in \u001b[0;36m_zeros_like\u001b[0;34m(op_input, op_output)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;31m# restored saved model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m     return array_ops.zeros(\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2972\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2973\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   3032\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure it's a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3033\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3034\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(dims, value, name)\u001b[0m\n\u001b[1;32m    245\u001b[0m   \"\"\"\n\u001b[0;32m--> 246\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(dims, value, name)\u001b[0m\n\u001b[1;32m   3508\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3509\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   3510\u001b[0m         \"Fill\", dims=dims, value=value, name=name)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    798\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3753\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3754\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3755\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2130\u001b[0m                                 control_input_ops, op_def)\n\u001b[0;32m-> 2131\u001b[0;31m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_str\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2647\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'Adam/gradients/ones' has no attr named '_read_only_resource_inputs'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-e17d81e7f30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-813c58c6f0a2>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHalvAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m-> 2452\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         if x is not None)\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    462\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m       \u001b[0;31m# and last_write_to_resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;34m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m   \u001b[0mreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_read_write_resource_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m   \u001b[0msaturated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaturated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mread_only_input_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2646\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a3d1a2-c7c7-4eed-fbef-fef0a6e36f25"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06418048769412463, 0.06437364825832646, 0.06141868107486259, 0.06130336597239615, 0.0681851409736459, 0.062307581557692955, 0.0633189796502963, 0.06178009429413827]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ykd7kawkvX",
        "outputId": "474dbbdb-6611-4162-ce2c-93dfb95de499"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06130336597239615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "ddf75305-391a-4653-d30f-53ca0955d0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0359a97310>]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwc9Znv+8+jfbHWlrxK3lq2iTdsx6twgonBgZscSCYhAyEJQ1gCwSzDzHCYc+7JvZMz957LeU3sLIeTBAIJicMWsjnBEwcwwcl4wbLaNtgGL8JuyassdcuSZVnbc//okiNEy2pLLVUvz/v18kvd1VWtp2Spv1W/X/1+JaqKMcaY5JPidgHGGGPcYQFgjDFJygLAGGOSlAWAMcYkKQsAY4xJUmluF3A5SkpKdPLkyW6XYYwxcWXnzp1nVLW07/K4CoDJkydTVVXldhnGGBNXRORouOXWBGSMMUnKAsAYY5KUBYAxxiQpCwBjjElSFgDGGJOkLACMMSZJWQAYY0ySsgAwJka98d5paupb3C7DJDALAGNiUHtnN/f+bCeP/+Fdt0sxCcwCwJgYtP/EWS50drOtppGubrtpkxkeEQWAiFwvIu+JyCEReSzM65ki8qLz+nYRmdzrtbkislVE9orI2yKS5Sy/1Xm+R0T+ICIl0dopY+Kdzx8AoOl8B/tPnHW5GpOoBgwAEUkFngBuAGYCt4rIzD6r3QkEVLUCWAs87mybBqwD7lXVWcAKoMNZ/h3gGlWdC+wBVkdlj4xJAL7aIHlZoam6thw+43I1JlFFcgawGDikqjWq2g68ANzUZ52bgGedxy8DK0VEgFXAHlXdDaCqDaraBYjzL9dZLx84PuS9MSZB+PxBlleU4C3NZcvhBrfLMQkqkgCYANT2el7nLAu7jqp2Ak2AB5gOqIhsFJFqEXnUWacDuA94m9AH/0zg6SHshzEJ40zLBfyNrSyYWESlt4S33m+ko6vb7bJMAhruTuA0YDlwm/P1syKyUkTSCQXAfGA8oSagfw73BiJyj4hUiUhVfX39MJdrjPt8/iAA8ycWUun10NrexZ66JperMokokgA4BpT3el7mLAu7jtO+XwA0EDpb2KyqZ1S1FdgALADmAajqYVVV4CWgMtw3V9UnVXWhqi4sLf3Q/QyMSTg+f4C0FGH2hAKWTvUAsNX6AcwwiCQAdgDTRGSKiGQAtwDr+6yzHrjdefx5YJPzwb4RmCMiOU4wXA3sIxQYM0Wk5xP9OmD/0HbFmMRQ7Q8wc3w+WempFOVmMHNcvvUDmGExYAA4bfqrCX2Y7wdeUtW9IvJNEbnRWe1pwCMih4BHgMecbQPAGkIhsguoVtVXVPU48C/AZhHZQ+iM4P+N7q4ZE386u7rZU9fEgolFF5dVej1UHQ3Q1tHlYmUmEUV0S0hV3UCo+ab3sm/0etwG3NzPtusIXQrad/kPgB9cTrHGJLoDp1pobe9i/sTCi8sqKzz86C/vU+0PUOm14TImemwksDExpNoZADa//K9nAIsmF5OaImy1ZiATZRYAxsQQnz9IyagMyouzLy7Ly0pnblmB9QOYqLMAMCaG+GoDzCsvIjQ+8q8qvR521wZpudDpUmUmEVkAGBMjgq3t1NSf+0D7f49lU0vo7FZ2HGl0oTKTqCwAjIkRvtrQALDeVwD1+OikIjJSU6wfwESVBYAxMcLnD5IiMLes4EOvZWekMn9ioU0MZ6LKAsCYGOHzB5gxNp/czPBXZ1d6S9h7/CzB1vYRrswkKgsAY2JAd7eyyx9kQZj2/x6VFR5UYVuN9QOY6LAAMCYGHK5voflCJ/PDtP/3uLKskOz0VJsXyESNBYAxMaD3DKD9yUhLYdGUYhsPYKLGAsCYGFDtD1CQnc4UT+4l16v0ejh4uoXTzW0jVJlJZBYAxsQAnz/I/ImFpKTIJder9Iamh7Z+ABMNFgDGuKy5rYMDp5s/MP9Pf2aNLyAvK836AUxUWAAY47LdtU2oXrr9v0dqirB0qsf6AUxUWAAY4zKfP4AIzIsgACDUDHS0oZW6QOswV2YSnQWAMS7z1QapKB1FflZ6ROv33BPApoUwQ2UBYIyLVBWfPxBR80+P6WNG4cnNsAAwQ2YBYIyLjjS0EmjtCDsBXH9EhKXeUD9A6NbbxgyOBYAxLvL13AHsMgIAQv0AJ8+28f6Zc8NRlkkSFgDGuKjaH2BUZhoVo0dd1nY9/QB2NZAZiogCQESuF5H3ROSQiDwW5vVMEXnReX27iEzu9dpcEdkqIntF5G0RyRKRPBHZ1evfGRH5dvR2y5j44PMHmVdeSOoAA8D6muzJYVxBlvUDmCEZMABEJBV4ArgBmAncKiIz+6x2JxBQ1QpgLfC4s20asA64V1VnASuADlVtVtV5Pf+Ao8CvorRPxsSF1vZO3j3ZfFkdwD1EhGVeD1trGujutn4AMziRnAEsBg6pao2qtgMvADf1Wecm4Fnn8cvASgnd1HQVsEdVdwOoaoOqdvXeUESmA6OBPw9+N4yJP3vqmujq1kEFAISagRrPtfPeqeYoV2aSRSQBMAGo7fW8zlkWdh1V7QSaAA8wHVAR2Sgi1SLyaJj3vwV4Ufu5nEFE7hGRKhGpqq+vj6BcY+JDzwyg8yKYAiKcZc68QNYPYAZruDuB04DlwG3O18+KyMo+69wCPN/fG6jqk6q6UFUXlpaWDl+lxowwnz/AlJJcinMzBrX9hMJsJntybF4gM2iRBMAxoLzX8zJnWdh1nHb/AqCB0NnCZlU9o6qtwAZgQc9GInIlkKaqOwe9B8bEIVWl2h9kfvngmn96LPOWsL2mkc6u7ihVZpJJJAGwA5gmIlNEJIPQEfv6PuusB253Hn8e2OQ06WwE5ohIjhMMVwP7em13K5c4+jcmUdUFznOm5cKg2/97VHo9NF/oZO/xs1GqzCSTAQPAadNfTejDfD/wkqruFZFvisiNzmpPAx4ROQQ8AjzmbBsA1hAKkV1Ataq+0uvtv4AFgElCvtqeO4ANrv2/x9Kp1g9gBi8tkpVUdQOh5pvey77R63EbcHM/264jdClouNemRlypMQnE5w+QlZ7CFWPzhvQ+pXmZzBiTx5bDZ7hvhTdK1ZlkYSOBjXFBtT/I3LJC0lKH/ie4zOthx5FG2jutH8BcHgsAY0ZYW0cX+443XdYEcJdS6fXQ1tHNLqdZyZhIWQAYM8L2Hj9LR9fgB4D1tWSKBxHYYpeDmstkAWDMCPvrDKDRCYCCnHRmjy+wjmBz2SwAjBlhPn+QsqJsRudlRe09K70efP4A59u7Bl7ZGIcFgDEjLHQHsOi0//dY5vXQ0aVUHW2M6vuaxGYBYMwIOtnUxvGmtiGPAO5r0eRi0lLEmoHMZbEAMGYERbv9v0duZhrzygstAMxlsQAwZgT5aoNkpKUwa3xB1N+70uvh7bogZ9s6ov7eJjFZABgzgqqPBpg9Pp+MtOj/6S3zltCt8FaN9QOYyFgAGDNC2ju7eftYU9Q7gHvMn1hIZlqKNQOZiFkAGDNC3j15lgud3VEbAdxXVnoqCycX2YAwEzELAGNGSPXR4ekA7q3SW8K7J5tpPNc+bN/DJA4LAGNGiK82yJj8TMYVRG8AWF89t4ncVmPNQGZgFgDGjBCfP8iCiUWIyLB9j7kTChiVmWbNQCYiFgDGjIAzLRfwN7YOa/MPQFpqCounFFtHsImIBYAxI8Dnj84dwCKxbKqHmvpznGxqG/bvZeKbBYAxI8DnD5CWIsyZEP0BYH319ANsrbFmIHNpFgDGjACfP8jM8flkpacO+/eaOS6fgux0thyyZiBzaRYAxgyzzq5udtcFoz4BXH9SUoRlUz1sOdyAqo7I9zTxKaIAEJHrReQ9ETkkIo+FeT1TRF50Xt8uIpN7vTZXRLaKyF4ReVtEspzlGSLypIgcEJF3ReRz0dopY2LJgVMttLZ3jUj7f4/KCg/HguepbTw/Yt/TxJ8BA0BEUoEngBuAmcCtIjKzz2p3AgFVrQDWAo8726YB64B7VXUWsALomanqvwKnVXW6875vDnlvjIlBvtrQALDhGgEcTqXTD2CXg5pLieQMYDFwSFVrVLUdeAG4qc86NwHPOo9fBlZK6GLnVcAeVd0NoKoNqtpzy6KvAv/DWd6tqvabahJS9dEgntwMyouzR+x7ektHUZqXaZeDmkuKJAAmALW9ntc5y8Kuo6qdQBPgAaYDKiIbRaRaRB4FEJGextD/7iz/hYiMCffNReQeEakSkar6+vqId8yYWOGrDTB/YuGwDgDrS0So9Fo/gLm04e4ETgOWA7c5Xz8rIiud5WXAFlVdAGwF/i3cG6jqk6q6UFUXlpaWDnO5xkRXsLWdmvpzI9r+36PS6+FMywUOnW4Z8e9t4kMkAXAMKO/1vMxZFnYdp92/AGggdLawWVXPqGorsAFY4LzWCvzK2f4XznJjEoqvtmcA2MhcAdRbpbcEwJqBTL8iCYAdwDQRmSIiGcAtwPo+66wHbncefx7YpKHzzo3AHBHJcYLhamCf89rvCHUKA6wE9g1pT4yJQT5/kBSBK8tGPgDKi3MoK8q2jmDTr7SBVlDVThFZTejDPBV4RlX3isg3gSpVXQ88DfxMRA4BjYRCAlUNiMgaQiGiwAZVfcV56//sbPNtoB64I8r7ZozrfP4AM8bmk5s54J/asKj0eti49xTd3UpKysj1QZj4ENFvpapuINR803vZN3o9bgNu7mfbdYQuBe27/Cjw8csp1ph40t2t7KoN8p+uHO9aDZXeEl6qqmPfibPMHoFpKEx8sZHAxgyTw/UtNLd1jtgI4HAuzgtk/QAmDAsAY4ZJzwygCyaN/BVAPcbkZzG1NNf6AUxYFgDGDJNqf4CC7HSmeHJdraPS6+Gt9xvp6Op2tQ4TeywAjBkmPn+QeeWFrne+VnpLONfexZ66JlfrMLHHAsCYYdDc1sGB080jOv9Pf5ZO7ekHsGYg80EWAMYMgz11Tai6MwCsr+LcDD4yLt8GhJkPsQAwZhhUHw3NAHqli1cA9Vbp9VB1NEBbR9fAK5ukYQFgzDDw1QapGD2Kgux0t0sBQgHQ3tlNtT/gdikmhlgAGBNlqorPH2BBDDT/9Fg8pZjUFLHxAOYDLACMibIjDa0EWjtcmQG0P3lZ6cyZUGD9AOYDLACMiTKf08wSCx3AvVV6PeyuDdJyodPtUkyMsAAwJsp8/iCjMtOYNjrP7VI+oNJbQme3suNIo9ulmBhhAWBMlFX7A1xZXkBqjM2++dFJRWSkplg/gLnIAsCYKGpt7+Tdk83ML4+d9v8e2RmpzJ9YaAFgLrIAMCaK3q5roqtbWTApttr/e1R6S3jneBNNrR1ul2JigAWAMVFU7cwAOi8GzwAgND20Kmx7384CjAWAMVHl8weY7MmhODfD7VLCmldeSFa69QOYEAsAY6JEVfHVBmNiArj+ZKSlsGhysd0fwAAWAMZETV3gPPXNF2Lu+v++Kr0lHDjVQn3zBbdLMS6zADAmSny1ofb/WBoBHE5lz20ia6wZKNlFFAAicr2IvCcih0TksTCvZ4rIi87r20Vkcq/X5orIVhHZKyJvi0iWs/xPznvucv6NjtZOGeMGnz9AVnoKM8bG1gCwvmaNzycvK83uD2BIG2gFEUkFngCuA+qAHSKyXlX39VrtTiCgqhUicgvwOPC3IpIGrAO+rKq7RcQD9L7+7DZVrYrWzhjjpmp/kLllhaSnxvaJdVpqCkumeGxeIBPRGcBi4JCq1qhqO/ACcFOfdW4CnnUevwysFBEBVgF7VHU3gKo2qKpNSG4STltHF/uON8V8+3+PSq+How2t1AVa3S7FuCiSAJgA1PZ6XucsC7uOqnYCTYAHmA6oiGwUkWoRebTPdj92mn/+mxMYHyIi94hIlYhU1dfXR1CuMSNv7/GzdHRpTI4ADqeyouc2kXYWkMyG+1w1DVgO3OZ8/ayIrHReu01V5wAfc/59OdwbqOqTqrpQVReWlpYOc7nGDE7PDKCxdA+AS5k+Og9PboYFQJKLJACOAeW9npc5y8Ku47T7FwANhM4WNqvqGVVtBTYACwBU9ZjztRl4jlBTkzFxyecPMqEwm9H5WW6XEpGUFGGpN9QPoKpul2NcEkkA7ACmicgUEckAbgHW91lnPXC78/jzwCYN/VZtBOaISI4TDFcD+0QkTURKAEQkHfg08M7Qd8cYd/j8gbhp/+9R6fVw8mwb758553YpxiUDBoDTpr+a0If5fuAlVd0rIt8UkRud1Z4GPCJyCHgEeMzZNgCsIRQiu4BqVX0FyAQ2isgeZ/kx4Kmo7pkxI+RkUxvHm9piegRwOJXeEgC7GiiJDXgZKICqbiDUfNN72Td6PW4Dbu5n23WELgXtvewc8NHLLdaYWBSrdwAbyGRPDuMKstha08CXlk5yuxzjgti+YNmYOOCrDZKRmsLM8flul3JZRIRlUz1sO9xAd7f1AyQjCwBjhsjnDzB7Qj6Zaalul3LZlnk9NJxr58DpZrdLMS6wADBmCNo7u9lT1xTz8//0Z5kzL9CWQ9YPkIwsAIwZgndPnuVCZ3fctf/3KCvKYZInxzqCk5QFgDFD4PPHxwygl1Lp9bC9poHOrm63SzEjzALAmCGo9gcYk5/J+IL4GAAWzjJvCc0XOtl7/KzbpZgRZgFgzBD4/EHmlxfRz1RWcWHZVKcfwJqBko4FgDGDdKblAv7G1rht/+9RmpfJ9DGj7DaRScgCwJhB6mn/XzApftv/e1R6S9hxpJH2TusHSCYWAMYMks8fIC1FmD2+wO1ShmyZ10NbRze7nNtamuRgARCDbHbG+ODzB/nIuHyyM+JvAFhfS6d4EMGagZJMRHMBmehq6+iiLnCeukArtYHz1DW2Uhc4T22gldrGVjq7lN89sJzJJblul2r60dnVze66IDd/tMztUqKiICed2eML2HK4gYevdbsaM1IsAIZBR1c3J4JtFz/Qe3+41wXOc7r5wgfWz0hNYUJRNmVF2cyaPY5f++r4zusHWfu381zaAzOQA6daaG3viuvr//uq9Hp45j/e53x7V0Kc1ZiBWQAMQle3cupsW+iDvbGV2kDrxcd1gfOcaDpP77m1UlOEcQVZlBflcPX0UsqLcygvzqasKIfyohxG52WSkvLXywjzs9N4cnMNX1/hZdqYPBf20AzEVxufM4BeyjKvhx9urqHqaCMfm2Z330sGFgBhqCpnWto/cNReF2iltjH09VjwPB1df/2EF4ExeVmUFWWzeEox5UWhD/ey4mzKi0JT7qalRt7d8rWPe1m39Sjffu0gT9y2YDh20QyRzx/Ek5vBxOIct0uJmkWTi0lLEbYebrAASBJJGQCqStP5josf6LW9PtxrnQ/7to4PXg7nyc2grDiH2RMKuH72OMqdD/eyomwmFGVHdSbI4twMvrp8Ct/bdIj7j5+Nu2mGk0G1cweweB4A1lduZhpXlhfagLAkkhQB8Pxbfg6earl4RH8scJ7mC50fWCc/K43y4hy8pbmscJppyoqyL37NyRjZH9Vdy6fyky1HWPvaAZ76ysIR/d7m0oKt7dTUn+NzCxKjA7i3Sq+HJ944xNm2DvKz0t0uxwyzpAiAn249ypEz5y4etS+ZUux8sP+1Lb4gO7Z+2Qty0rn7Y1NZ8+oB9tQFmVuWOG3N8a7nWvn55Yn3f7LM6+F7mw6x4/1GVn5kjNvlmGGWFAHwi3uXkZuRGnen63dcNZln/uN91rx6gJ/csdjtcoyj2h8kRWBuAgbAgolFZKSlsOVwgwVAEoioZ1JErheR90TkkIg8Fub1TBF50Xl9u4hM7vXaXBHZKiJ7ReRtEcnqs+16EXlnqDtyKaMy0+Luwx8gLyudr33cy5/eq2fn0Ua3yzEOnz/A9DF5jMpMvOOnrPRUFk4qsn6AJDFgAIhIKvAEcAMwE7hVRGb2We1OIKCqFcBa4HFn2zRCN4S/V1VnASuAjl7v/TdAy9B3I3HdXjmJklEZfOuPB9wuxQDd3cqu2mBCXf/fV6XXw/4TZ2k81+52KWaYRXIGsBg4pKo1qtoOvADc1Gedm4BnnccvAysldMi9CtijqrsBVLVBVbsARGQU8Ajwr0PfjcSVk5HGfSsq2HK4ga12VOa6w/UtNLd1siCBrv/va5m3BIBtNfb7lugiCYAJQG2v53XOsrDrqGon0AR4gOmAishGEakWkUd7bfPfgW8BrZf65iJyj4hUiUhVfX19BOUmntuWTGRMfiZrXn3P5glyWSLcAWwgc8sKyM1ItXmBksBwTwaXBiwHbnO+flZEVorIPMCrqr8e6A1U9UlVXaiqC0tLk3NwSlZ6KquvqWDHkQB/Pmh/lG7y1QbIz0pjagLP05SemsLiKcXWD5AEIgmAY0B5r+dlzrKw6zjt/gVAA6Gzhc2qekZVW4ENwAJgGbBQRI4AfwGmi8ifBr8bie8Li8qZUJjNt149YGcBLqo+Gmr/7z11RyKq9JZQU3+Ok01tbpdihlEkAbADmCYiU0QkA7gFWN9nnfXA7c7jzwObNPQptRGYIyI5TjBcDexT1e+r6nhVnUzozOCAqq4Y+u4krsy0VB74RAW7a4Nseve02+Ukpea2Dg6cbk6o+X/6s8wbuk3k1ho740xkAwaA06a/mtCH+X7gJVXdKyLfFJEbndWeBjwicohQx+5jzrYBYA2hENkFVKvqK9HfjeTwuY+WMbE4hzWvHqC7284CRtqeuiZUE7v9v8fMcfkUZKez5ZA1AyWyiC5kVtUNhJpvei/7Rq/HbcDN/Wy7jtCloP299xFgdiR1JLv01BQeWjmNf/jFbjbuPckNc8a5XVJSqT4amgF0XgIOAOsrJUVYNtXDlsMNqGpcjqMxA7M7gsWZz8yfgLc0l7WvHaDLzgJGlK82SMXoUTE3bchwWeb1cCx4ntrG826XYoaJBUCcSU0RHr52OgdOtfD7PcfdLidpqCo+fyAh5//pT6X1AyQ8C4A49Kk547hibB7fee0gnV3dA29ghuxoQyuB1o6kaP/vUTF6FCWjMu1y0ARmARCHUpyzgJoz5/jNLjsLGAnV/lD7/4JJyXMGICJUev/aD2ASjwVAnPrkrDHMnpDPd18/SIedBQw7nz9IbkYq00Yn1y06K70e6psvcLjepuxKRBYAcUpEeOS66fgbW3l5Z53b5SQ8X22AK8sLSU3wAWB9VTrzAlkzUGKyAIhj18wYzbzyQr73+kEudHa5XU7Cam3vZP+JZhYkUft/j/LibCYUZtt4gARlARDHRIR/XDWD401tvPBW7cAbmEF5u66Jrm5NihHAffX0A2ytabDBhwnIAiDOXVXhYfGUYp544xBtHXYWMBx8zi0gk2EAWDiVFR6aznew78RZt0sxUWYBEOdEhH+4bjqnmy+wbttRt8tJSNVHA0z25OAZlel2Ka5YNjXUD2D3o0g8FgAJYMlUD8srSvj+nw5z7kKn2+UkFFXFl+B3ABvI2IIsppbm2v0BEpAFQIJ4ZNV0Gs618+zWI26XklCOBc9T33whKdv/e6v0enjr/Ua75DjBWAAkiAUTi7hmRik/fLOGs20dA29gIlLt3AEsGa8A6q3SW8K59i721DW5XYqJIguABPLIdTNoOt/BM3953+1SEobPHyArPYUZY5NrAFhfS6c68wIlQDPQ6eY2/vlXe/j3t0+4XYrrLAASyJyyAlbNHMPTf36fYGu72+UkBJ8/yNwJhaSnJvefSnFuBleMzYvrAWGqym93HWPV2s08/1Yt9z9Xze92J/dUKsn9W52AHlk1nZb2Tp76c43bpcS9to4u9h5vSvr2/x6V3hKqjgbi8nLj081tfO1nO3nohV1M9uTyu9XLWTipmIdf3MWGJD4TsABIMFeMzedTc8bx4/84QkPLBbfLiWt7j5+lo0uT+gqg3iq9Hto7u/E5/SLxoPdR/58O1PPYDVfwy/sqmVNWwDN3LGJ+eSEPPu/jD++cdLtUV1gAJKCHr51OW0cXP9xsZwFD4XNmALUzgJDFU4tJkfjpB6hvvsC960JH/ZM8uWx4cDn3Xu29OJ/TqMw0fnzHIuaUFbD6uWpe3XfK5YpHngVAAqoYPYrPzJvAT7ce4XRzm9vlxC2fP8iEwmzG5Ge5XUpMyM9KZ05ZYcz3A6gq63cfZ9XaN3nj3Xr+8/VX8Mt7l1ERZibXvKx0nv3qYmaNz+frP9/JG++edqFi91gAJKgHV06jo0v5328cdruUuOXzB+zov49Kr4ddtcGYHXB4puUC962r5sHnfUz05PLKg8u5b4WXtEt04udnpfPTO5cwY2weX1u3kzcP1I9gxe6KKABE5HoReU9EDonIY2FezxSRF53Xt4vI5F6vzRWRrSKyV0TeFpEsZ/kfRGS3s/wHIpIarZ0yMLkkl88vKOO57X6OB+2erpfrZFMbx5varP2/j0qvh85uZceRRrdL+ZDf7znOdWveZNO7p3n0+hn88t5lTBsT2eW7BdnprLtzCRWlo7jnp1X85WB8NHMN1YAB4HwwPwHcAMwEbhWRmX1WuxMIqGoFsBZ43Nk2DVgH3Kuqs4AVQM8opS+o6pXAbKAUuHnIe2M+4IGVFSjK/3rjkNulxJ1dtdb+H87CScWkp0pMzQt0puUCX//5TlY/56O8OIffP7icr6+ouORRfziFORmsu2sJU0pyueunO5Ji6otIfkKLgUOqWqOq7cALwE191rkJeNZ5/DKwUkQEWAXsUdXdAKraoKpdzuOeqQXTgAzA5pqNsrKiHG5ZNJGXdtRS29jqdjlxpdofJCM1hVnj890uJaZkZ6Qyf2JRzPQDvLLnBKvWbua1faf5p0/O4Ff3VTI9wqP+cIpzM/j5XUuYWJzDnT+pYntNbOzncIkkACYAvSebr3OWhV1HVTuBJsADTAdURDaKSLWIPNp7IxHZCJwGmgkFx4eIyD0iUiUiVfX1ydM2Fy33X1NBSorw3dcPul1KXPH5A8yakE9mmrVM9lXp9fDO8SaaWt2bcqSh5QL3/7ya+5+rZkJhNr97YDn3X3P5R/3heEZl8vO7ljK+MIs7frKDqhhs7oqW4e4ETgOWA7c5Xz8rIit7XlTVTwLjgEzgE+HeQFWfVNWFqrqwtLR0mMtNPGMLsvjSkkn8yneM98+cc7ucuNDR1cZIMtAAABPGSURBVM2euibml1v7fziV3hJUYdv77hwdb3g7dNT/x30n+adPzuDXX6+M+lQdpXmZPH/3UsbmZ3H7M29R7VwSnGgiCYBjQHmv52XOsrDrOO3+BUADobOFzap6RlVbgQ3Agt4bqmob8Fs+3KxkouS+FV4yUlP4zmsH3C4lLuw/cZYLnd0smGTt/+HMKy8kKz1lxPsBGloucP9z1Xz959WML8zm9w98LGpH/eGMzs/iubuXUpKXye1Pv8Xu2vgZABepSH5yO4BpIjJFRDKAW4D1fdZZD9zuPP48sElVFdgIzBGRHCcYrgb2icgoERkHFwPjU8C7Q98dE05pXiZfqZzEb3cf5+CpZrfLiXk9I13tCqDwMtJSWDS5eEQ7Sf+956h/70n+cdV0fjUMR/3hjC3I4vm7l1KYm86Xn97O2wk2G+qAAeC06a8m9GG+H3hJVfeKyDdF5EZntacBj4gcAh4BHnO2DQBrCIXILqBaVV8BcoH1IrLHWX4a+EFU98x8wNc+7iUnPZW1dhYwIJ8/wOi8TMYX2ACw/lR6SzhwqoX65uGdbqTxXDurn6vmvp9XM64wi989sJzVn5g2opPzjS/M5vm7l5KXlc6Xnt7O3uOJEwJpkaykqhsINd/0XvaNXo/b6OcyTlVdR+hS0N7LTgGLLrdYM3jFuRl8dfkUvrfpEHuPNzFrfIHbJcWsan+QBROLCF3IZsKp9DrTQ9c0cOOV44fle/zhnRP8n795h6bzHfzDddO5d4XXtVlZy4pyeOGepfztD7fypR9t57m7l/KRcfF/hZiNBE4idy2fSl5WGmtftSuC+nOm5QL+xla7/n8As8bnk5eZNizzAgXOtfPA8z7uXVfNmPws1q9ezgMrR/aoP5zy4hyev2cpmWmp3Paj7RxIgOZUC4AkUpCTzj0fm8pr+08lZIdWNOyy9v+IpKWmsGRqcdTHA/zhnZNct/ZN/v3tE/z9tdP5zf1XxdSR9iRPLs/dvYS0FOGLT23j0On4DgELgCRzx/IpFOWks+ZV6wsIp9ofIC1FmDPBmsgGssxbwtGGVo5FYaqRwLl2HnrBx73rdjI6L3TU/9C17h/1hzO1dBTP3b0UEG59ajuH61vcLmnQYu+na4bVqMw0vna1lzcP1LPzaOIOcBksnz/IR8blk51hA8AGcrEfYIhnAX/ce5Lr1m7mlT0nePjaafx29VXMjPER2BWjR/H83Uvo7la++NQ2jsTpGBsLgCT0lWWTKBmVwbf+aGcBvXV1K7vrgtb+H6EZY/Iozs0Y9OWgwdZ2Hn7Bxz0/28novEzWr17Ow9dOj8mj/nCmjcnjubuX0tGl3PrUNvwN8TfdSnz8pE1U5WSkcd+KCrYcbkiKCa8i9d7JZlrbuywAIpSSIiyb6mHr4QZCw34i98e9J7l2zWZ+v+cED62cxm/uj/2j/nBmjM1j3Z1LON/Rxa1PbYu7ObcsAJLUbUsmMiY/kzV/PHDZf7yJyufMALrAOoAjtszr4URTG0ciPPoNtrbz9y/u4p6f7aRkVAa/XX0Vf3/ddDLS4vejaOb4fNbduYTmtg5ufWpbVPpERkr8/tTNkGSlp7L6mgqqjgbYnCRznw/E5w9SnJvBxOIct0uJGz39AJGcSb627xTXrd3M73Yf58GV01i/ennCjEeZPaGAdXctoel8B198ahsnmuIjBCwAktgXFpUzoTCbNX98z84CCF0BNL+80AaAXYYpJbmMzc+65OWgTa0dPPLiLu76aRWe3Ax+c/9VPBLnR/3hzC0r5KdfXUxDSztffGo7p87G/u1YE+t/wFyWzLRUHlxZwe66Jl7fn1z3Qu0r2NpOTf05Fkyy5p/LISJUej1sO9xAd/eHDyJe33+K69a+yW93H+fBT1SwfvVyZifwJbbzJxbx7FcXcfpsG7c+tS3m78ltAZDk/mZBGZM8Oax59UDYP+BkscsZGDe/3DqAL9cyr4eGc+0c6DUoqqm1g0de2sWdz1ZRnJvBb++/ikdWzUi4o/5wPjqpmB/fsZgTwTa++NR2zrQM73xJQ5H4/xvmktJTU3ho5TT2nTjLxr0n3S7HNT5/kBSBuRYAl21ZTz/AoVAz0KZ3T7Hq22/y213HeSAJjvrDWTylmGf+bhF1gVZue2o7DTEaAhYAhpvmTcBbmsuaVw/QlaRnAdX+ANPH5DEqM6L5EU0vZUU5TPLk8Nr+U/zjL3bz1Z9UUZidwW++fhX/kCRH/eEs83p4+vZFHGk4x20/2k7gXLvbJX1Icv7PmA9ITREevnY6B0+38Ps9x90uZ8R1dyu7aoM2/88QVHo9bDncwK99x1h9TQXrH7iKOWXJddQfzlUVJTz1lYXUnDnHl57e7uptNMOxADAAfGrOOK4Ym8e3XztIZ1e32+WMqJozLTS3ddoAsCG4ZdFEPnHFaH799Ur+8ZMz7F7KvXx8eik//PJHOXiqhS8/s52m87ETAhYABgiN6nz42um8f+Ycv/b1veNnYqs+GuoAXmABMGhXlhfyzN8tYm6Z/QzDuWbGaL7/pQXsP3GWrzzzFmfbYiMELADMRZ+cNYbZE/L57qaDdCTRWYCvNkB+VhpTS0a5XYpJYCs/Mob/9cUF7D3WxN898xYtFzrdLskCwPyViPAP182gtvE8v6iqc7ucEePzB5k3sYiUFBsAZobXJ2eN5Xu3zmd3XRN3/PgtzrkcAhYA5gNWzChl/sRCvrfpIG0dXW6XM+ya2zp471SzXf9vRswNc8bxnVvmsfNogK/+ZAet7e6FgAWA+YCes4ATTW288Jbf7XKG3Z66JlSxEcBmRH167njW/u08dhxp5K5nqzjf7s7BVkQBICLXi8h7InJIRB4L83qmiLzovL5dRCb3em2uiGwVkb0i8raIZIlIjoi8IiLvOsv/v+jtkhmqqyo8LJ5SzBN/OuzaL+ZI8flDM4DOs85LM8JumjeBf7v5SrbWNHDPz6pcOeMeMABEJBV4ArgBmAncKiIz+6x2JxBQ1QpgLfC4s20asA64V1VnASuAnu7vf1PVK4D5wFUicsPQd8dEQ+gsYDr1zRdYt+2o2+UMq2p/EG9pLgU56W6XYpLQ3ywo4/HPzeXPB8/wtZ/t5ELnyIZAJGcAi4FDqlqjqu3AC8BNfda5CXjWefwysFJCUyquAvao6m4AVW1Q1S5VbVXVN5xl7UA1UDb03THRsmSqh+UVJXz/zcOud1QNF1XF5w/Y/P/GVV9YWM7/+Js5vHmgnvvWVY9oCEQSABOA2l7P65xlYddR1U6gCfAA0wEVkY0iUi0ij/Z9cxEpBP4T8Hq4by4i94hIlYhU1dfXR1CuiZZHVk2n8Vw7P9lyxO1ShsXRhlYCrR02Ati47tbFE/nXz8xm07unWf2cb8Quwx7uTuA0YDlwm/P1syKysudFp4noeeC7qloT7g1U9UlVXaiqC0tLS4e5XNPbgolFfOKK0Ty5uSZmBq5EU7XT/m8jgE0s+NLSSfzLjbN4dd8pHnx+ZEIgkgA4BpT3el7mLAu7jvOhXgA0EDpb2KyqZ1S1FdgALOi13ZPAQVX99uDKN8Ptkeum03S+g2f+8r7bpUSdzx8kNyOV6WPy3C7FGABur5zMf/v0TP79nZM8/OKuYZ+WJZIA2AFME5EpIpIB3AKs77POeuB25/HngU0ausXURmCOc9VPGnA1sA9ARP6VUFA8PPTdMMNl9oQCPjlrDE//+X2CrbE3m+FQ+GoDXFleSKoNADMx5M7lU/gv/8cVvLLnBI+8tHtYZ+gdMACcNv3VhD7M9wMvqepeEfmmiNzorPY04BGRQ8AjwGPOtgFgDaEQ2QVUq+orIlIG/FdCVxVVi8guEbkryvtmouTvr5tOS3snT24O20oXl863d7H/RLM1/5iYdM/HvTx6/QzW7z7OP/1i+EIgosnPVXUDoeab3su+0etxG3BzP9uuI3QpaO9ldYAddsWJK8bm86k54/jJliPcuXwKnlGZbpc0ZHvqgnR1K/PLrQPYxKavr6igq0v51qsHSEkR/ufn5kZ9uhIbCWwi8vC102nr6OIHbx52u5So8PXcAtLOAEwMe2DlNB5aOY0/vVfPyWG4ybwFgIlIxehRfGbeBH669Sinh+EXcaT5/AEmeXIS4mzGJLaHr53Gxoc/xvjC7Ki/twWAidhD106js1v533+K77MAVaXaH7QJ4ExcEJFhO1CxADARm+TJ5eaPlvHcdj/Hg+fdLmfQjgXPU998wSaAM0nPAsBcltWfqEBRvrfpkNulDJrP77T/WwewSXIWAOaylBXlcMuiifyiqhZ/Q6vb5QxKtT9AVnoKV4yzAWAmuVkAmMt2/zUVpKQI39100O1SBsXnDzJ3QiHpqfbrb5Kb/QWYyza2IIsvLZnEr6rrqKlvcbucy3Khs4t9x8/a5Z/GYAFgBum+FV4y01L5zuvxdRbwzrGztHd1WwAYgwWAGaTSvExur5zM+t3H2Xk0EDf3D/ZdnAHUOoCNiWgqCGPC+drHp7Ju21E+9/0tAORkpFKUk0FRbnroa04GxbkZFOakO18zKM756/OinAyyM1JHtGZfbZAJhdmMyc8a0e9rTCyyADCDVpSbwW/ur2T7+40EzrUTaO1wvrbT2NqBv7GVxnPtNLf1f0exrPSUi2HROziKcjMo6ic4cjJSCd1w7vL5jgaYb9f/GwNYAJghqhidR8XoS19O2dHVTbC1g2BrO409QdEaCoq+wXE8eJZAaztN5zvQfiZAzEhN6RMWHw6OIucMozgng8LcdPIy0zh19gLHm9r4qo0ANgawADAjID01hdK8TErzIh/O3tWtNJ3voPFc+8XgCLZ20BgmON472UzACZj+Zs1NSxFynOYmGwFsTIgFgIlJqSlCcW6oDyFS3d3K2bYOAq3hgyPY2k5mWipzJxQMY+XGxA8LAJMwUlKEwpxQn8GUkly3yzEm5tlloMYYk6QsAIwxJklZABhjTJKKKABE5HoReU9EDonIY2FezxSRF53Xt4vI5F6vzRWRrSKyV0TeFpEsZ/n/IyK1IhJfk8kYY0yCGDAARCQVeAK4AZgJ3CoiM/usdicQUNUKYC3wuLNtGqEbwt+rqrOAFUCHs83vgMVR2AdjjDGDEMkZwGLgkKrWqGo78AJwU591bgKedR6/DKyU0FDNVcAeVd0NoKoNqtrlPN6mqieisRPGGGMuXyQBMAGo7fW8zlkWdh1V7QSaAA8wHVAR2Sgi1SLy6NBLNsYYEw3DPQ4gDVgOLAJagddFZKeqvh7pG4jIPcA9ABMnThyWIo0xJhlFEgDHgPJez8ucZeHWqXPa/QuABkJnC5tV9QyAiGwAFgARB4CqPgk86WxfLyJHI922jxLgzCC3HWnxVCvEV73xVCvEV73xVCvEV71DrXVSuIWRBMAOYJqITCH0QX8L8MU+66wHbge2Ap8HNqmqishG4FERyQHagasJdRIPiqqWDnZbEalS1YWD3X4kxVOtEF/1xlOtEF/1xlOtEF/1DletA/YBOG36q4GNwH7gJVXdKyLfFJEbndWeBjwicgh4BHjM2TYArCEUIruAalV9xdmh/ykidUCOiNSJyP8d3V0zxhhzKRH1AajqBmBDn2Xf6PW4Dbi5n23XEboUtO/yRwHrFDbGGJck00jgJ90u4DLEU60QX/XGU60QX/XGU60QX/UOS62i/d11wxhjTEJLpjMAY4wxvVgAGGNMkkr4ABhoIrtYIiLPiMhpEXnH7VoGIiLlIvKGiOxzJvp7yO2aLkVEskTkLRHZ7dT7L27XNBARSRURn4j83u1aBiIiR5zJHneJSJXb9VyKiBSKyMsi8q6I7BeRZW7X1B8RmeH8THv+nRWRh6P2/oncB+BMZHcAuI7QoLQdwK2qus/VwvohIh8HWoCfqupst+u5FBEZB4xT1WoRyQN2Ap+J4Z+tALmq2iIi6cBfgIdUdZvLpfVLRB4BFgL5qvppt+u5FBE5AizsGfQZy0TkWeDPqvojEckAclQ16HZdA3E+z44BS1R1sANiPyDRzwAimcguZqjqZqDR7ToioaonVLXaedxMaIxI3zmiYoaG9Ew9nu78i9mjHxEpAz4F/MjtWhKJiBQAHyc0dglVbY+HD3/HSuBwtD78IfEDIJKJ7MwQOfd/mA9sd7eSS3OaVHYBp4FXVTWW6/02oXEy3W4XEiEF/igiO535u2LVFKAe+LHTvPYjEYmXG0jfAjwfzTdM9AAww0xERgG/BB5W1bNu13MpqtqlqvMIzWe1WERisplNRD4NnFbVnW7XchmWq+oCQvcNud9pzoxFaYTmI/u+qs4HzuHMXBDLnKaqG4FfRPN9Ez0AIpnIzgyS05b+S+Dnqvort+uJlHPK/wZwvdu19OMq4EanXf0F4BMi8qHR9LFEVY85X08DvyZ2b/ZUB9T1Ovt7mVAgxLobCE2lcyqab5roAXBxIjsnQW8hNHGdGSKnU/VpYL+qrnG7noGISKmIFDqPswldGPCuu1WFp6r/rKplqjqZ0O/sJlX9kstl9UtEcp0LAXCaU1YBMXklm6qeBGpFZIazaCUQkxcu9HErUW7+geG/H4CrVLVTRHomsksFnlHVvS6X1S8ReZ7QbTNLnIny/i9Vfdrdqvp1FfBl4G2nXR3gvzjzRsWiccCzzpUUKYQmNYz5yyvjxBjg16FjAtKA51T1D+6WdEkPAD93DgprgDtcrueSnFC9Dvha1N87kS8DNcYY079EbwIyxhjTDwsAY4xJUhYAxhiTpCwAjDEmSVkAGGNMkrIAMMaYJGUBYIwxSer/B4WEVzfnnhnbAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}