{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult10mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "15641868-9cca-4029-e0e3-bfae2a7c9e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.9/dist-packages (0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_10.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "13893596-806b-49bb-8f47-a74d4d0906cb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t16\n",
            "Tentative: \t10\n",
            "Rejected: \t23\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t9\n",
            "Rejected: \t23\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t9\n",
            "Rejected: \t23\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t17\n",
            "Tentative: \t9\n",
            "Rejected: \t23\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t18\n",
            "Tentative: \t8\n",
            "Rejected: \t23\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t18\n",
            "Tentative: \t8\n",
            "Rejected: \t23\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t6\n",
            "Rejected: \t23\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t6\n",
            "Rejected: \t23\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t6\n",
            "Rejected: \t23\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t20\n",
            "Tentative: \t5\n",
            "Rejected: \t24\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t21\n",
            "Tentative: \t4\n",
            "Rejected: \t24\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t22\n",
            "Tentative: \t3\n",
            "Rejected: \t24\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=101,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA538976040),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FA538976040, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=101,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA538976040),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FA538976040, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=101,\n",
              "                      random_state=RandomState(MT19937) at 0x7FA538976040)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=101,\n",
              "                      random_state=RandomState(MT19937) at 0x7FA538976040)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=101,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FA538976040),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7FA538976040, verbose=2)"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "541c26eb-2e66-4672-d082-cde205929b5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 22) dtype=float32 (created by layer 'input_4')>"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuddmGCf_1dR",
        "outputId": "c88db59a-d43a-42cb-abbc-a29b922dadd5"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "36/36 [==============================] - 23s 156ms/step - loss: 0.0126\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0043\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0039\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0038\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0032\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0033\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0031\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0031\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0031\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0032\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0029\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0033\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0029\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0027\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0027\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0026\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0028\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0028\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0026\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 5s 128ms/step - loss: 0.0029\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0029\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0025\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0028\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0027\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 5s 153ms/step - loss: 0.0025\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0025\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0029\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0025\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0025\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0027\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0024\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0027\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0025\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0026\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0023\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 127ms/step - loss: 0.0025\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 151ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0024\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0025\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0025\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0024\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0023\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 129ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 5s 130ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0026\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0022\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0024\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 5s 131ms/step - loss: 0.0023\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0022\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0024\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0023\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0021\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0021\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0022\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0022\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0022\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0021\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0022\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0025\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0024\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 5s 133ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 132ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "48/48 [==============================] - 3s 20ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 35s 160ms/step - loss: 0.0121\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0040\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0037\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0034\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0032\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 8s 211ms/step - loss: 0.0035\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0031\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0032\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0029\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0031\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0028\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0030\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0032\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0028\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 7s 209ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0027\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0027\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0026\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0026\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0029\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0028\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0027\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0027\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0025\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0025\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0025\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0025\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0024\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0029\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0027\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0025\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0025\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0024\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 0.0024\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0025\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0025\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0026\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0026\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0026\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0026\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0024\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 7s 209ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0023\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0025\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0025\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0023\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 7s 178ms/step - loss: 0.0023\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 7s 209ms/step - loss: 0.0023\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 7s 188ms/step - loss: 0.0023\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0023\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0024\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0022\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0024\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 7s 187ms/step - loss: 0.0023\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0024\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0023\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0022\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0023\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 7s 177ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0023\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 7s 191ms/step - loss: 0.0023\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0023\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0022\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0022\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0022\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0021\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0023\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 7s 189ms/step - loss: 0.0022\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0023\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0021\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0021\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0021\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0022\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0022\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 7s 203ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0021\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0020\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "48/48 [==============================] - 4s 36ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 26s 210ms/step - loss: 0.0124\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0045\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0036\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0036\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0032\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0034\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0031\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0030\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0028\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0030\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0033\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0029\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0029\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0028\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0027\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 7s 179ms/step - loss: 0.0029\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0027\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0026\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0025\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0026\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0026\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0027\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0026\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0027\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0026\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0025\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0026\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 7s 209ms/step - loss: 0.0026\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0025\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 7s 179ms/step - loss: 0.0025\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0024\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 0.0026\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0025\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0024\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0026\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 7s 208ms/step - loss: 0.0027\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0024\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0025\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 7s 187ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0025\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0026\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0024\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0023\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0024\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 7s 195ms/step - loss: 0.0023\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0023\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0024\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0023\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 7s 188ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0024\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0024\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0022\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0027\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0024\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0023\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0024\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0023\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0024\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0023\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0023\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 7s 189ms/step - loss: 0.0024\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0023\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 7s 188ms/step - loss: 0.0023\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0023\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 8s 212ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0023\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0023\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0023\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 7s 186ms/step - loss: 0.0023\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 7s 187ms/step - loss: 0.0021\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 7s 187ms/step - loss: 0.0022\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 8s 214ms/step - loss: 0.0021\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0021\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 7s 187ms/step - loss: 0.0022\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0020\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0021\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0021\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 7s 178ms/step - loss: 0.0023\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0022\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0022\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 7s 189ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 7s 184ms/step - loss: 0.0021\n",
            "48/48 [==============================] - 3s 28ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 25s 181ms/step - loss: 0.0133\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0041\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0035\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0034\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0031\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0031\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0029\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0030\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0030\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0028\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 7s 181ms/step - loss: 0.0028\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0028\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0034\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0028\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 7s 183ms/step - loss: 0.0030\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0028\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 7s 206ms/step - loss: 0.0027\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0029\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0028\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0027\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0028\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0026\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0026\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0026\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0026\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0025\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0025\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0025\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 7s 185ms/step - loss: 0.0025\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0026\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 7s 206ms/step - loss: 0.0025\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0025\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0025\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0024\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 7s 182ms/step - loss: 0.0026\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0024\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0025\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0024\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0023\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0025\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0026\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0025\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0024\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0023\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 7s 202ms/step - loss: 0.0024\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0023\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0025\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0024\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0024\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0023\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0024\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0023\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0023\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 0.0023\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0023\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 7s 200ms/step - loss: 0.0022\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0024\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0022\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0022\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 6s 175ms/step - loss: 0.0024\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0022\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 0.0023\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 6s 172ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0023\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0022\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0021\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0022\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0021\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0022\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 7s 180ms/step - loss: 0.0021\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0021\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 178ms/step - loss: 0.0022\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0021\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 180ms/step - loss: 0.0023\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0023\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 181ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 6s 156ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 6s 179ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0021\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 7s 205ms/step - loss: 0.0022\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0023\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0022\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 6s 168ms/step - loss: 0.0022\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0021\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 6s 171ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 152ms/step - loss: 0.0022\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 174ms/step - loss: 0.0022\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0021\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0021\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 7s 206ms/step - loss: 0.0020\n",
            "48/48 [==============================] - 3s 26ms/step\n",
            "Epoch 1/120\n",
            "36/36 [==============================] - 23s 139ms/step - loss: 0.0125\n",
            "Epoch 2/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0052\n",
            "Epoch 3/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0037\n",
            "Epoch 4/120\n",
            "36/36 [==============================] - 6s 177ms/step - loss: 0.0035\n",
            "Epoch 5/120\n",
            "36/36 [==============================] - 6s 155ms/step - loss: 0.0033\n",
            "Epoch 6/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0033\n",
            "Epoch 7/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0033\n",
            "Epoch 8/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0031\n",
            "Epoch 9/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0030\n",
            "Epoch 10/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0033\n",
            "Epoch 11/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0030\n",
            "Epoch 12/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0030\n",
            "Epoch 13/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0030\n",
            "Epoch 14/120\n",
            "36/36 [==============================] - 5s 150ms/step - loss: 0.0030\n",
            "Epoch 15/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0029\n",
            "Epoch 16/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0027\n",
            "Epoch 17/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0028\n",
            "Epoch 18/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0029\n",
            "Epoch 19/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0029\n",
            "Epoch 20/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0028\n",
            "Epoch 21/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0028\n",
            "Epoch 22/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0027\n",
            "Epoch 23/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0027\n",
            "Epoch 24/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0027\n",
            "Epoch 25/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0027\n",
            "Epoch 26/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0027\n",
            "Epoch 27/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0027\n",
            "Epoch 28/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0026\n",
            "Epoch 29/120\n",
            "36/36 [==============================] - 7s 190ms/step - loss: 0.0026\n",
            "Epoch 30/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0026\n",
            "Epoch 31/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0028\n",
            "Epoch 33/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0027\n",
            "Epoch 34/120\n",
            "36/36 [==============================] - 6s 157ms/step - loss: 0.0026\n",
            "Epoch 35/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0026\n",
            "Epoch 36/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0025\n",
            "Epoch 37/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 38/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0026\n",
            "Epoch 39/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 40/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 41/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0026\n",
            "Epoch 42/120\n",
            "36/36 [==============================] - 6s 170ms/step - loss: 0.0025\n",
            "Epoch 43/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0024\n",
            "Epoch 44/120\n",
            "36/36 [==============================] - 5s 134ms/step - loss: 0.0026\n",
            "Epoch 45/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0026\n",
            "Epoch 46/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 47/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0025\n",
            "Epoch 48/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 49/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 50/120\n",
            "36/36 [==============================] - 5s 148ms/step - loss: 0.0025\n",
            "Epoch 51/120\n",
            "36/36 [==============================] - 5s 151ms/step - loss: 0.0025\n",
            "Epoch 52/120\n",
            "36/36 [==============================] - 5s 147ms/step - loss: 0.0024\n",
            "Epoch 53/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0025\n",
            "Epoch 54/120\n",
            "36/36 [==============================] - 7s 192ms/step - loss: 0.0025\n",
            "Epoch 55/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "Epoch 56/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0024\n",
            "Epoch 57/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 58/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0025\n",
            "Epoch 59/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0025\n",
            "Epoch 60/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0023\n",
            "Epoch 61/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0024\n",
            "Epoch 62/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 63/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0025\n",
            "Epoch 64/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0024\n",
            "Epoch 65/120\n",
            "36/36 [==============================] - 6s 159ms/step - loss: 0.0024\n",
            "Epoch 66/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0024\n",
            "Epoch 67/120\n",
            "36/36 [==============================] - 7s 191ms/step - loss: 0.0023\n",
            "Epoch 68/120\n",
            "36/36 [==============================] - 5s 141ms/step - loss: 0.0024\n",
            "Epoch 69/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0022\n",
            "Epoch 70/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 71/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 72/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0025\n",
            "Epoch 73/120\n",
            "36/36 [==============================] - 5s 145ms/step - loss: 0.0023\n",
            "Epoch 74/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 75/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0024\n",
            "Epoch 76/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0022\n",
            "Epoch 77/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0024\n",
            "Epoch 78/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0023\n",
            "Epoch 79/120\n",
            "36/36 [==============================] - 6s 165ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0023\n",
            "Epoch 81/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 82/120\n",
            "36/36 [==============================] - 6s 162ms/step - loss: 0.0022\n",
            "Epoch 83/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0022\n",
            "Epoch 84/120\n",
            "36/36 [==============================] - 5s 144ms/step - loss: 0.0022\n",
            "Epoch 85/120\n",
            "36/36 [==============================] - 6s 152ms/step - loss: 0.0023\n",
            "Epoch 86/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0022\n",
            "Epoch 88/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0022\n",
            "Epoch 89/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0022\n",
            "Epoch 90/120\n",
            "36/36 [==============================] - 5s 137ms/step - loss: 0.0021\n",
            "Epoch 91/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0022\n",
            "Epoch 92/120\n",
            "36/36 [==============================] - 5s 149ms/step - loss: 0.0023\n",
            "Epoch 93/120\n",
            "36/36 [==============================] - 6s 173ms/step - loss: 0.0022\n",
            "Epoch 94/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0022\n",
            "Epoch 96/120\n",
            "36/36 [==============================] - 6s 160ms/step - loss: 0.0022\n",
            "Epoch 97/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0023\n",
            "Epoch 98/120\n",
            "36/36 [==============================] - 6s 163ms/step - loss: 0.0023\n",
            "Epoch 99/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "36/36 [==============================] - 6s 161ms/step - loss: 0.0021\n",
            "Epoch 101/120\n",
            "36/36 [==============================] - 5s 136ms/step - loss: 0.0022\n",
            "Epoch 102/120\n",
            "36/36 [==============================] - 6s 153ms/step - loss: 0.0021\n",
            "Epoch 103/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0021\n",
            "Epoch 104/120\n",
            "36/36 [==============================] - 6s 169ms/step - loss: 0.0022\n",
            "Epoch 105/120\n",
            "36/36 [==============================] - 6s 150ms/step - loss: 0.0021\n",
            "Epoch 106/120\n",
            "36/36 [==============================] - 5s 135ms/step - loss: 0.0021\n",
            "Epoch 107/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0022\n",
            "Epoch 108/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0022\n",
            "Epoch 109/120\n",
            "36/36 [==============================] - 6s 166ms/step - loss: 0.0020\n",
            "Epoch 110/120\n",
            "36/36 [==============================] - 5s 138ms/step - loss: 0.0021\n",
            "Epoch 111/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0020\n",
            "Epoch 112/120\n",
            "36/36 [==============================] - 5s 139ms/step - loss: 0.0021\n",
            "Epoch 113/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0021\n",
            "Epoch 114/120\n",
            "36/36 [==============================] - 5s 146ms/step - loss: 0.0021\n",
            "Epoch 115/120\n",
            "36/36 [==============================] - 5s 142ms/step - loss: 0.0020\n",
            "Epoch 116/120\n",
            "36/36 [==============================] - 6s 176ms/step - loss: 0.0021\n",
            "Epoch 117/120\n",
            "36/36 [==============================] - 6s 154ms/step - loss: 0.0022\n",
            "Epoch 118/120\n",
            "36/36 [==============================] - 6s 167ms/step - loss: 0.0021\n",
            "Epoch 119/120\n",
            "36/36 [==============================] - 5s 140ms/step - loss: 0.0020\n",
            "Epoch 120/120\n",
            "36/36 [==============================] - 6s 164ms/step - loss: 0.0020\n",
            "48/48 [==============================] - 4s 38ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ef5057-1321-4dd1-a85c-711fb60db4dd"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.03250018975237776, 0.031574389538425236, 0.033533223785669204, 0.033417643499372866, 0.03248248668864926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "79635331-bbaa-43eb-e3de-d14b6f167a52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.031574389538425236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "b039b029-e438-4a90-e21e-c95758d9c801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa553868fa0>]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}