{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult02mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "8907c14e-7620-45a3-f52f-e8d687667e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_02.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "6dc852ca-b2eb-49f9-acb0-c9c41325d0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t31\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F38DCF64440),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F38DCF64440, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F38DCF64440),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7F38DCF64440, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F38DCF64440),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7F38DCF64440, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                      random_state=RandomState(MT19937) at 0x7F38DCF64440)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                      random_state=RandomState(MT19937) at 0x7F38DCF64440)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "2a1930b5-ad0b-43a0-c48c-7061d857867b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 15) dtype=float32 (created by layer 'input_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>9:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "86af74a4-5832-4de6-ad43-24338c62e0a1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "37/37 [==============================] - 26s 154ms/step - loss: 0.0141\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0063\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0057\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0055\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0052\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0052\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0051\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0051\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0050\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0045\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0045\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0045\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0047\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0047\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0046\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0047\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0045\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0043\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0043\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0043\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0043\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0042\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0045\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0042\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0041\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0041\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0043\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0041\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0041\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0042\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0044\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0042\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0042\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0041\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0045\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 0.0041\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0042\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0040\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0042\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0041\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0040\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0041\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0042\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0042\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0041\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0040\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0041\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0040\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0040\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0040\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0040\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0039\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0038\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0038\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0039\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0040\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0039\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0039\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0039\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0041\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0038\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 148ms/step - loss: 0.0039\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0040\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0039\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0039\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0037\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0038\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0040\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0041\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0039\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0038\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0038\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0036\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0037\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0038\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0038\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0036\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0039\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0038\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0037\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0038\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0037\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0036\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0038\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0044\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0041\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0039\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0038\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0040\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0037\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0038\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0037\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0038\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0039\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0037\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0038\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0038\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0037\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0037\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0037\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0037\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0037\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0037\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0036\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0035\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0035\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0037\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0036\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0038\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0036\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0035\n",
            "50/50 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 24s 152ms/step - loss: 0.0164\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0065\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0057\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0056\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0057\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0054\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 4s 117ms/step - loss: 0.0049\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0051\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0049\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 4s 117ms/step - loss: 0.0047\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0045\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0047\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0045\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0046\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0046\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0048\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0045\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0043\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0045\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0046\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0044\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0042\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0044\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0043\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0043\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0042\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0043\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0044\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0042\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0040\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0041\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0042\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0041\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0040\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0041\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0040\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0040\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0040\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0040\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0041\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0041\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0042\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0040\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0040\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0041\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0041\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0040\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0039\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0039\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0041\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0040\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0040\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0040\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0038\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0039\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0038\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0041\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0039\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0038\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0039\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0038\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0039\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0038\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0039\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0040\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0039\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0037\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0039\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0038\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0039\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0037\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0038\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0037\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0040\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0038\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0039\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0037\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0038\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0037\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0037\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0037\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0037\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0038\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0038\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0036\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0037\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0038\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0037\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0037\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0037\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 7s 188ms/step - loss: 0.0036\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0038\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0038\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0037\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0036\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0036\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0037\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0037\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0036\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0036\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0035\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0036\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0038\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0036\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0036\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0035\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0036\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0037\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0036\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0036\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0036\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0036\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0035\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0037\n",
            "50/50 [==============================] - 3s 20ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 25s 142ms/step - loss: 0.0167\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0062\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0059\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0056\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0052\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0054\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0049\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0057\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0051\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0048\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0048\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0046\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0046\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0044\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0045\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0045\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0046\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0044\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0042\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 4s 117ms/step - loss: 0.0043\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0048\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0043\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0041\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0042\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0044\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0041\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0044\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 4s 122ms/step - loss: 0.0042\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0043\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 4s 117ms/step - loss: 0.0042\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0041\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0042\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0040\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0041\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0040\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0041\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0039\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0043\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0041\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0040\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0039\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0041\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0041\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0040\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0041\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0039\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0040\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0040\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0047\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0044\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0042\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0042\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0041\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0040\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0040\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 4s 120ms/step - loss: 0.0040\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0040\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 4s 122ms/step - loss: 0.0040\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0039\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0039\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0040\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0038\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0039\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0040\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0039\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0040\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0038\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0039\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0038\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0039\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0039\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0038\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0038\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0037\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0039\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0038\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0037\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0039\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0039\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0037\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0037\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0038\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0038\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0038\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0038\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0039\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0040\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0037\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0038\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0038\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0037\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0039\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0038\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0038\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0039\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0038\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0037\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0036\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0038\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0036\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0036\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0036\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0036\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0036\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0036\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0037\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0036\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0037\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0039\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0037\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0037\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0036\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0036\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0036\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0035\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0038\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0043\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0037\n",
            "50/50 [==============================] - 3s 19ms/step\n",
            "Epoch 1/120\n",
            "31/37 [========================>.....] - ETA: 0s - loss: 0.0170"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-24-e17d81e7f30f>\", line 1, in <module>\n",
            "    GlobalMinimaSearch(mdl.get_weights())\n",
            "  File \"<ipython-input-23-6f7e5cf3d4de>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-23-6f7e5cf3d4de>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-23-6f7e5cf3d4de>\", line 22, in GlobalMinimaSearch\n",
            "    GlobalMinimaSearch(params_2)\n",
            "  File \"<ipython-input-23-6f7e5cf3d4de>\", line 9, in GlobalMinimaSearch\n",
            "    model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1650, in fit\n",
            "    tmp_logs = self.train_function(iterator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 880, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 912, in _call\n",
            "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 134, in __call__\n",
            "    return concrete_function._call_flat(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1745, in _call_flat\n",
            "    return self._build_call_outputs(self._inference_function.call(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 378, in call\n",
            "    outputs = execute.execute(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1505, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 185, in findsource\n",
            "    lines = linecache.getlines(file, globals_dict)\n",
            "  File \"/usr/lib/python3.9/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.9/linecache.py\", line 137, in updatecache\n",
            "    lines = fp.readlines()\n",
            "  File \"/usr/lib/python3.9/codecs.py\", line 319, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b866fe6-e545-4ca6-d769-ab8b6e4d0c18"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.029487375518843072, 0.03362953331751392, 0.031843745824821794]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "15b6816c-c184-4635-8fef-71e5832a9afe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.029487375518843072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "e6a21e41-1456-4380-c648-4c518209db02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f38648075e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}