{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/bestresult05mae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "d1aa9bce-70f7-4b55-82c4-e0cd65b89192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.9/dist-packages (0.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.2.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from Boruta) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_05.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "5c16e7a7-784f-47d5-9cdf-b4b3456729bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t4\n",
            "Rejected: \t34\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t2\n",
            "Rejected: \t36\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t1\n",
            "Rejected: \t36\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t0\n",
            "Rejected: \t36\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t0\n",
            "Rejected: \t36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=72,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAAF35D4040),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7FAAF35D4040, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=72,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAAF35D4040),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FAAF35D4040, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=72,\n",
              "                                         random_state=RandomState(MT19937) at 0x7FAAF35D4040),\n",
              "         n_estimators=&#x27;auto&#x27;,\n",
              "         random_state=RandomState(MT19937) at 0x7FAAF35D4040, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=72,\n",
              "                      random_state=RandomState(MT19937) at 0x7FAAF35D4040)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=7, n_estimators=72,\n",
              "                      random_state=RandomState(MT19937) at 0x7FAAF35D4040)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "eb708352-d0d2-4c48-fb4c-2347fd39c5a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 13) dtype=float32 (created by layer 'input_3')>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='LeakyReLU',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='LeakyReLU'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='LeakyReLU',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='LeakyReLU',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='LeakyReLU'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>4:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.003), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=120, batch_size=128)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(mean_absolute_error(y,y_test))\n",
        "  best_weights= model.get_weights()\n",
        "  \n",
        "\n",
        "     \n",
        "\n",
        "  params_1 =[final_weight + (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  #GlobalMinimaSearch(params_1)\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XuddmGCf_1dR",
        "outputId": "7551535a-253c-4c3b-deef-8fc82773cccc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "37/37 [==============================] - 31s 170ms/step - loss: 0.0112\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0046\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0038\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0036\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0036\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 8s 209ms/step - loss: 0.0037\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0033\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0034\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0032\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0035\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0033\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0032\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0032\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0033\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0034\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0031\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 7s 201ms/step - loss: 0.0030\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0030\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0030\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0029\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0029\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0029\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0029\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0029\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0031\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0032\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0029\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0028\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0029\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0028\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0029\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0028\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0029\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0028\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0029\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0028\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0029\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0029\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0027\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0027\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0028\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0028\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0029\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0028\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0027\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0027\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0027\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0028\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0027\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0029\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0027\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0027\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0028\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0027\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0027\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0027\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0027\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 7s 189ms/step - loss: 0.0026\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0027\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0027\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0028\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0026\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0026\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0026\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0026\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0026\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0026\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0025\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0025\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0025\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0026\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0025\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0025\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0027\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0027\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0026\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0027\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0025\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0025\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0026\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 7s 191ms/step - loss: 0.0025\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0026\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0026\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0025\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0025\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0024\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 7s 194ms/step - loss: 0.0024\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 7s 193ms/step - loss: 0.0025\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0025\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0024\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0025\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0024\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0024\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0025\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0026\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0024\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0024\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0023\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0024\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0024\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0024\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0025\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0023\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 7s 187ms/step - loss: 0.0025\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0024\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0023\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0023\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0024\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0024\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0023\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0023\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0024\n",
            "50/50 [==============================] - 3s 22ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 27s 168ms/step - loss: 0.0110\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0044\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0041\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0038\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0038\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0035\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0036\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 7s 203ms/step - loss: 0.0033\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0033\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 7s 182ms/step - loss: 0.0035\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 143ms/step - loss: 0.0032\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0032\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0032\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0031\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 6s 175ms/step - loss: 0.0032\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0030\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 8s 217ms/step - loss: 0.0031\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0030\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 7s 178ms/step - loss: 0.0034\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0033\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0029\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0030\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0029\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 7s 175ms/step - loss: 0.0030\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0029\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 7s 186ms/step - loss: 0.0030\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 7s 198ms/step - loss: 0.0029\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0029\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0028\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 7s 192ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0031\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 7s 184ms/step - loss: 0.0028\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0028\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0029\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0028\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.0028\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0027\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0028\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0027\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 7s 181ms/step - loss: 0.0028\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0027\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0027\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0027\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 7s 177ms/step - loss: 0.0027\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 0.0028\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 7s 190ms/step - loss: 0.0028\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0027\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 6s 177ms/step - loss: 0.0027\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0027\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0027\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0027\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0027\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0027\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0026\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0026\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0026\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0027\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 8s 210ms/step - loss: 0.0027\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0026\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0027\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0027\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0026\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0026\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0025\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0025\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0027\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0027\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 173ms/step - loss: 0.0026\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0027\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0026\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0025\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0027\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0025\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0025\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0025\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0026\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 176ms/step - loss: 0.0026\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0025\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0026\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0026\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0026\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0025\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0025\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0024\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 0.0025\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0025\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0026\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0025\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0024\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0025\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0023\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0026\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0025\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0024\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0023\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0024\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0023\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0024\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 140ms/step - loss: 0.0024\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0024\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0024\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0023\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0023\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 7s 193ms/step - loss: 0.0024\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 145ms/step - loss: 0.0024\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0023\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0024\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0023\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0023\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0023\n",
            "50/50 [==============================] - 3s 18ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 24s 150ms/step - loss: 0.0122\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0055\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0041\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0039\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 170ms/step - loss: 0.0039\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 4s 118ms/step - loss: 0.0040\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0038\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0033\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0034\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0033\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0031\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0032\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0032\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 7s 185ms/step - loss: 0.0030\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0032\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0030\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0032\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0030\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0029\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0030\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0029\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0029\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0030\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0029\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0028\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 7s 199ms/step - loss: 0.0028\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0029\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0029\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0030\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0028\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0027\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0028\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0029\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0028\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 6s 147ms/step - loss: 0.0028\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0028\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 7s 193ms/step - loss: 0.0027\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0027\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0029\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0028\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0027\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0028\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0029\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0028\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0028\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0031\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 138ms/step - loss: 0.0028\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0027\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0027\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0027\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0027\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0026\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 147ms/step - loss: 0.0027\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0027\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 4s 119ms/step - loss: 0.0027\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0027\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0027\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0028\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0027\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0026\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0028\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0026\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 7s 179ms/step - loss: 0.0026\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 5s 144ms/step - loss: 0.0026\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0027\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0027\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0026\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0030\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 7s 183ms/step - loss: 0.0027\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0026\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 159ms/step - loss: 0.0026\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0025\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0025\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0025\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0025\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0026\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0024\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0025\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0025\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 6s 151ms/step - loss: 0.0026\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0025\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0025\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0026\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0026\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0025\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0025\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0026\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0026\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0024\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0024\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0024\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0024\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0024\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0025\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 4s 121ms/step - loss: 0.0024\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0024\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0024\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 5s 146ms/step - loss: 0.0024\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0024\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 7s 196ms/step - loss: 0.0024\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 5s 122ms/step - loss: 0.0025\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 6s 166ms/step - loss: 0.0024\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0024\n",
            "Epoch 113/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0024\n",
            "Epoch 114/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 115/120\n",
            "37/37 [==============================] - 5s 123ms/step - loss: 0.0023\n",
            "Epoch 116/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0024\n",
            "Epoch 117/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0023\n",
            "Epoch 118/120\n",
            "37/37 [==============================] - 6s 152ms/step - loss: 0.0023\n",
            "Epoch 119/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0023\n",
            "Epoch 120/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0024\n",
            "50/50 [==============================] - 4s 38ms/step\n",
            "Epoch 1/120\n",
            "37/37 [==============================] - 30s 135ms/step - loss: 0.0109\n",
            "Epoch 2/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0045\n",
            "Epoch 3/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0040\n",
            "Epoch 4/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0042\n",
            "Epoch 5/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0035\n",
            "Epoch 6/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0036\n",
            "Epoch 7/120\n",
            "37/37 [==============================] - 8s 205ms/step - loss: 0.0035\n",
            "Epoch 8/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0034\n",
            "Epoch 9/120\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 0.0036\n",
            "Epoch 10/120\n",
            "37/37 [==============================] - 6s 154ms/step - loss: 0.0034\n",
            "Epoch 11/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0031\n",
            "Epoch 12/120\n",
            "37/37 [==============================] - 6s 171ms/step - loss: 0.0032\n",
            "Epoch 13/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0031\n",
            "Epoch 14/120\n",
            "37/37 [==============================] - 6s 172ms/step - loss: 0.0032\n",
            "Epoch 15/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0030\n",
            "Epoch 16/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0030\n",
            "Epoch 17/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0031\n",
            "Epoch 18/120\n",
            "37/37 [==============================] - 6s 174ms/step - loss: 0.0030\n",
            "Epoch 19/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0029\n",
            "Epoch 20/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0030\n",
            "Epoch 21/120\n",
            "37/37 [==============================] - 6s 157ms/step - loss: 0.0030\n",
            "Epoch 22/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0030\n",
            "Epoch 23/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0029\n",
            "Epoch 24/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0029\n",
            "Epoch 25/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0029\n",
            "Epoch 26/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0029\n",
            "Epoch 27/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0030\n",
            "Epoch 28/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0029\n",
            "Epoch 29/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0029\n",
            "Epoch 30/120\n",
            "37/37 [==============================] - 6s 163ms/step - loss: 0.0031\n",
            "Epoch 31/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0028\n",
            "Epoch 32/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0028\n",
            "Epoch 33/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0027\n",
            "Epoch 34/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0028\n",
            "Epoch 35/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0028\n",
            "Epoch 36/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0031\n",
            "Epoch 37/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0029\n",
            "Epoch 38/120\n",
            "37/37 [==============================] - 6s 156ms/step - loss: 0.0028\n",
            "Epoch 39/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0027\n",
            "Epoch 40/120\n",
            "37/37 [==============================] - 7s 194ms/step - loss: 0.0027\n",
            "Epoch 41/120\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 0.0028\n",
            "Epoch 42/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0028\n",
            "Epoch 43/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0030\n",
            "Epoch 44/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0027\n",
            "Epoch 45/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0029\n",
            "Epoch 46/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0030\n",
            "Epoch 47/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0028\n",
            "Epoch 48/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0027\n",
            "Epoch 49/120\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 0.0028\n",
            "Epoch 50/120\n",
            "37/37 [==============================] - 6s 155ms/step - loss: 0.0026\n",
            "Epoch 51/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0027\n",
            "Epoch 52/120\n",
            "37/37 [==============================] - 7s 201ms/step - loss: 0.0028\n",
            "Epoch 53/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0027\n",
            "Epoch 54/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0027\n",
            "Epoch 55/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0027\n",
            "Epoch 56/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0027\n",
            "Epoch 57/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0027\n",
            "Epoch 58/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0027\n",
            "Epoch 59/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0026\n",
            "Epoch 60/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0027\n",
            "Epoch 61/120\n",
            "37/37 [==============================] - 5s 124ms/step - loss: 0.0025\n",
            "Epoch 62/120\n",
            "37/37 [==============================] - 6s 177ms/step - loss: 0.0026\n",
            "Epoch 63/120\n",
            "37/37 [==============================] - 6s 169ms/step - loss: 0.0026\n",
            "Epoch 64/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0026\n",
            "Epoch 65/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0027\n",
            "Epoch 66/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0026\n",
            "Epoch 67/120\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 0.0025\n",
            "Epoch 68/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0026\n",
            "Epoch 69/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0027\n",
            "Epoch 70/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0028\n",
            "Epoch 71/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 72/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 73/120\n",
            "37/37 [==============================] - 5s 141ms/step - loss: 0.0026\n",
            "Epoch 74/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0027\n",
            "Epoch 75/120\n",
            "37/37 [==============================] - 6s 168ms/step - loss: 0.0027\n",
            "Epoch 76/120\n",
            "37/37 [==============================] - 6s 150ms/step - loss: 0.0025\n",
            "Epoch 77/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 78/120\n",
            "37/37 [==============================] - 7s 180ms/step - loss: 0.0026\n",
            "Epoch 79/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0027\n",
            "Epoch 80/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0026\n",
            "Epoch 81/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0025\n",
            "Epoch 82/120\n",
            "37/37 [==============================] - 5s 125ms/step - loss: 0.0025\n",
            "Epoch 83/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0025\n",
            "Epoch 84/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0025\n",
            "Epoch 85/120\n",
            "37/37 [==============================] - 6s 161ms/step - loss: 0.0025\n",
            "Epoch 86/120\n",
            "37/37 [==============================] - 5s 142ms/step - loss: 0.0025\n",
            "Epoch 87/120\n",
            "37/37 [==============================] - 7s 192ms/step - loss: 0.0025\n",
            "Epoch 88/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0025\n",
            "Epoch 89/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0025\n",
            "Epoch 90/120\n",
            "37/37 [==============================] - 6s 167ms/step - loss: 0.0024\n",
            "Epoch 91/120\n",
            "37/37 [==============================] - 5s 128ms/step - loss: 0.0025\n",
            "Epoch 92/120\n",
            "37/37 [==============================] - 6s 158ms/step - loss: 0.0025\n",
            "Epoch 93/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0025\n",
            "Epoch 94/120\n",
            "37/37 [==============================] - 5s 149ms/step - loss: 0.0025\n",
            "Epoch 95/120\n",
            "37/37 [==============================] - 5s 137ms/step - loss: 0.0024\n",
            "Epoch 96/120\n",
            "37/37 [==============================] - 5s 130ms/step - loss: 0.0025\n",
            "Epoch 97/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0026\n",
            "Epoch 98/120\n",
            "37/37 [==============================] - 6s 164ms/step - loss: 0.0024\n",
            "Epoch 99/120\n",
            "37/37 [==============================] - 6s 162ms/step - loss: 0.0025\n",
            "Epoch 100/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 101/120\n",
            "37/37 [==============================] - 6s 149ms/step - loss: 0.0024\n",
            "Epoch 102/120\n",
            "37/37 [==============================] - 5s 139ms/step - loss: 0.0026\n",
            "Epoch 103/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 104/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0023\n",
            "Epoch 105/120\n",
            "37/37 [==============================] - 5s 126ms/step - loss: 0.0025\n",
            "Epoch 106/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 107/120\n",
            "37/37 [==============================] - 5s 129ms/step - loss: 0.0024\n",
            "Epoch 108/120\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 0.0024\n",
            "Epoch 109/120\n",
            "37/37 [==============================] - 6s 165ms/step - loss: 0.0023\n",
            "Epoch 110/120\n",
            "37/37 [==============================] - 6s 153ms/step - loss: 0.0024\n",
            "Epoch 111/120\n",
            "37/37 [==============================] - 6s 160ms/step - loss: 0.0024\n",
            "Epoch 112/120\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 0.0023\n",
            "Epoch 113/120\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-e17d81e7f30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-437148bbfb2e>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-437148bbfb2e>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-437148bbfb2e>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mparams_2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mGlobalMinimaSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-437148bbfb2e>\u001b[0m in \u001b[0;36mGlobalMinimaSearch\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHalvAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1f47ea-8021-415e-f707-b95832d8149d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.029623319620663145, 0.03307015190971046, 0.030982311449644032]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "id": "56ykd7kawkvX",
        "outputId": "6c056353-7081-4631-9a9a-bf2d3f16b991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.029623319620663145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "a7431d0d-3a9f-4e3c-d638-286aa23fba80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faaf895db20>]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}