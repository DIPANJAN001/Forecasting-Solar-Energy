{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPANJAN001/Forecasting-Solar-Energy/blob/master/final_gsm_bestresult5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzs_vH9vlX74",
        "outputId": "79dabb0d-2b3a-40d0-a41a-04ed08234133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from Boruta) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.1->Boruta) (1.2.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Bidirectional\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from sklearn.decomposition import PCA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lDilv4v2lz-w"
      },
      "outputs": [],
      "source": [
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "    # Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "    # Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "        # if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "        # Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "        # Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "    # Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iQt_oZP7QczL"
      },
      "outputs": [],
      "source": [
        "df=pd.read_excel(\"/content/pv_02.xlsx\")\n",
        "weather_input1=df.drop('power_normed',axis=1)\n",
        "weather_input=weather_input1.drop('time_idx',axis=1)\n",
        "solpow=df['power_normed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoPnMw4oQQlc",
        "outputId": "15df04b9-8b26-412a-d90d-947aa6fca420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: \t1 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t2 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t3 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t4 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t5 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t6 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t7 / 100\n",
            "Confirmed: \t0\n",
            "Tentative: \t49\n",
            "Rejected: \t0\n",
            "Iteration: \t8 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t9 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t10 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t11 / 100\n",
            "Confirmed: \t6\n",
            "Tentative: \t15\n",
            "Rejected: \t28\n",
            "Iteration: \t12 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t13 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t14 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t15 / 100\n",
            "Confirmed: \t8\n",
            "Tentative: \t11\n",
            "Rejected: \t30\n",
            "Iteration: \t16 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t17 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t18 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t8\n",
            "Rejected: \t30\n",
            "Iteration: \t19 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t20 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t21 / 100\n",
            "Confirmed: \t11\n",
            "Tentative: \t7\n",
            "Rejected: \t31\n",
            "Iteration: \t22 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t23 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t24 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t25 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t26 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t27 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t28 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t29 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t30 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t31 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t32 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t33 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t34 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t35 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t36 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t37 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t38 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t39 / 100\n",
            "Confirmed: \t12\n",
            "Tentative: \t6\n",
            "Rejected: \t31\n",
            "Iteration: \t40 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t41 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t42 / 100\n",
            "Confirmed: \t13\n",
            "Tentative: \t5\n",
            "Rejected: \t31\n",
            "Iteration: \t43 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t44 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t45 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t46 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t47 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t48 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t49 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t50 / 100\n",
            "Confirmed: \t14\n",
            "Tentative: \t4\n",
            "Rejected: \t31\n",
            "Iteration: \t51 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t52 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t53 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t54 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t55 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t56 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t57 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t58 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t59 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t60 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t61 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t62 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t63 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t64 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t65 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t66 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t67 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t68 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t69 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t70 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t71 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t72 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t73 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t74 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t75 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t76 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t77 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t78 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t79 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t80 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t81 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t82 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t83 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t84 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t85 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t86 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t87 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t88 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t89 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t90 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t91 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t92 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t93 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t94 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t95 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t96 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t97 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t98 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "Iteration: \t99 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t3\n",
            "Rejected: \t31\n",
            "\n",
            "\n",
            "BorutaPy finished running.\n",
            "\n",
            "Iteration: \t100 / 100\n",
            "Confirmed: \t15\n",
            "Tentative: \t2\n",
            "Rejected: \t31\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BorutaPy(estimator=RandomForestRegressor(max_depth=7, n_estimators=85,\n",
              "                                         random_state=RandomState(MT19937) at 0x7F758463E840),\n",
              "         n_estimators='auto',\n",
              "         random_state=RandomState(MT19937) at 0x7F758463E840, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "rfc = RandomForestRegressor(random_state=1, n_estimators=1000, max_depth=7)\n",
        "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2, random_state=1)\n",
        "boruta_selector.fit(np.array(weather_input), np.array(solpow)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u2NoSDCGUFNU"
      },
      "outputs": [],
      "source": [
        "X_important_train = boruta_selector.transform(np.array(weather_input))\n",
        "num_steps = 3\n",
        "# training set\n",
        "(x_transformed_train,\n",
        " y_transformed_train) = lstm_data_transform(X_important_train,solpow , num_steps=num_steps)\n",
        "assert x_transformed_train.shape[0] == y_transformed_train.shape[0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_transformed_train,y_transformed_train,test_size=0.25, random_state=42,shuffle=False)\n",
        "#X_train_,X_val,y_train_,y_val=train_test_split(X_train,y_train,test_size=0.2, random_state=42,shuffle=False)\n",
        "inputs1 = Input(shape=(X_train.shape[1],X_train.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKjqiCK5m_T",
        "outputId": "fd5ef4e2-436d-409b-f9b2-b840594a3e2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 3, 15) dtype=float32 (created by layer 'input_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "inputs1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V27z-GjNapD4"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uxD0diT8a4c2"
      },
      "outputs": [],
      "source": [
        "opt=optimizers.Adam(learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YM0Epc0yvWnJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t0f48T0zsiAs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "class HalvAdam(Adam):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prev_gradients = None\n",
        "\n",
        "    @tf.function\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [math_ops.cast(x, \"float32\") for x in grads]\n",
        "\n",
        "        if self.prev_gradients is not None:\n",
        "            for i in range(len(grads)):\n",
        "                if (grads[i] * self.prev_gradients[i] < 0):\n",
        "                    self.updates[i] = self.updates[i] / 2\n",
        "\n",
        "        self.prev_gradients = grads\n",
        "        return self.updates"
      ],
      "metadata": {
        "id": "MpStRslgCRBO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "cSM9vzEq3G3U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "nq9ZwBIrI_qj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "18n5dRvpuI5T"
      },
      "outputs": [],
      "source": [
        "def define_model():\n",
        "\n",
        "\n",
        "  fe2_0 = Bidirectional(LSTM(256, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe2_1 = Dropout(0.6)(fe2_0)\n",
        "  fe2_2 = Bidirectional(LSTM(64, activation='relu',return_sequences = True))(fe2_1)\n",
        "  fe2_3= Dropout(0.5)(fe2_2)\n",
        "  fe2_4=Bidirectional(LSTM(4, activation='relu'))(fe2_3)\n",
        "  out2_1=Dense(1, activation='relu')(fe2_4)\n",
        "\n",
        "  fe3_0 =Bidirectional(LSTM(128, activation='relu',return_sequences = True))(inputs1)\n",
        "  fe3_1 = Dropout(0.6)(fe3_0)\n",
        "  fe3_2 = Bidirectional(LSTM(96, activation='relu',return_sequences = True))(fe3_1)\n",
        "  fe3_3= Dropout(0.5)(fe3_2)\n",
        "  fe3_4=Bidirectional(LSTM(8, activation='relu'))(fe3_3)#16\n",
        "  out3_1=Dense(1, activation='relu')(fe3_4)\n",
        " \n",
        " \n",
        "\n",
        "  output = layers.average([out2_1, out3_1])\n",
        "  #merged3 = concatenate([out2_1,out3_1], name='concat3')\n",
        "  #output = Dense(1, activation='relu')( merged3)\n",
        "  \n",
        "\n",
        "  model = Model(inputs=[inputs1], outputs=[output])\n",
        "  \n",
        " \n",
        "  return model\n",
        "mdl=define_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=[]"
      ],
      "metadata": {
        "id": "P5UqekV1_q7F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model"
      ],
      "metadata": {
        "id": "9zy5UX8p_zSl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dAICp2p5OCER"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "9iwWrmDs0z7O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GlobalMinimaSearch(weights):\n",
        "  if len(loss)>9:\n",
        "   return\n",
        "  \n",
        "  initial_weights =weights\n",
        "  model=clone_model(mdl)\n",
        "  model.set_weights(weights)\n",
        "  model.compile(optimizer=HalvAdam(learning_rate=0.002), loss='mean_squared_error')\n",
        "  model.fit(X_train, y_train, epochs=75, batch_size=64)\n",
        "  y= model.predict(X_test)\n",
        "  loss.append(np.sqrt(mean_squared_error(y,y_test)))\n",
        "  best_weights= model.get_weights()\n",
        "\n",
        "\n",
        "  params_2 =[final_weight - (final_weight - initial_weight) for initial_weight, final_weight in zip(initial_weights, best_weights)]\n",
        "  GlobalMinimaSearch(params_2)\n",
        "  \n",
        " "
      ],
      "metadata": {
        "id": "FxpviTJb_nUR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GlobalMinimaSearch(mdl.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuddmGCf_1dR",
        "outputId": "99d0e93e-6ca3-442d-820e-e79e95770f21"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 77ms/step - loss: 0.0120\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0062\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0060\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0055\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0050\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0052\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0051\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0049\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0049\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0046\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0046\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0047\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0046\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0048\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0046\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0045\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0045\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0045\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0042\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0044\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0043\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 7s 93ms/step - loss: 0.0044\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0043\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 7s 94ms/step - loss: 0.0041\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0038\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0040\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0037\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0037\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0039\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "50/50 [==============================] - 3s 16ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 77ms/step - loss: 0.0122\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0066\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0059\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0054\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0052\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0050\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0053\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0050\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0047\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0046\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 7s 92ms/step - loss: 0.0044\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0045\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0044\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0047\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0044\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0040\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0040\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 7s 95ms/step - loss: 0.0040\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "50/50 [==============================] - 3s 17ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 78ms/step - loss: 0.0124\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 7s 90ms/step - loss: 0.0059\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0057\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0052\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0053\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0052\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0051\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0051\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0048\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0049\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0050\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0046\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0046\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0045\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0040\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 87ms/step - loss: 0.0040\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0038\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0038\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 7s 94ms/step - loss: 0.0038\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0037\n",
            "50/50 [==============================] - 2s 16ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 19s 78ms/step - loss: 0.0112\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0062\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0056\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0055\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0051\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0049\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0049\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0048\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0051\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0045\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 0.0046\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0048\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0046\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0045\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0044\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0045\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0043\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0044\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.0043\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 86ms/step - loss: 0.0043\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0043\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0046\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0043\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0043\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0043\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0039\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0040\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0041\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 7s 95ms/step - loss: 0.0039\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0038\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "50/50 [==============================] - 2s 16ms/step\n",
            "Epoch 1/75\n",
            "74/74 [==============================] - 20s 78ms/step - loss: 0.0122\n",
            "Epoch 2/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0060\n",
            "Epoch 3/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0056\n",
            "Epoch 4/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0053\n",
            "Epoch 5/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0051\n",
            "Epoch 6/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0051\n",
            "Epoch 7/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0051\n",
            "Epoch 8/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0051\n",
            "Epoch 9/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0048\n",
            "Epoch 10/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0047\n",
            "Epoch 11/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0047\n",
            "Epoch 12/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0047\n",
            "Epoch 13/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0047\n",
            "Epoch 14/75\n",
            "74/74 [==============================] - 7s 88ms/step - loss: 0.0044\n",
            "Epoch 15/75\n",
            "74/74 [==============================] - 6s 84ms/step - loss: 0.0044\n",
            "Epoch 16/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0044\n",
            "Epoch 17/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0043\n",
            "Epoch 18/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 19/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0043\n",
            "Epoch 20/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0043\n",
            "Epoch 21/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 22/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0042\n",
            "Epoch 23/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0043\n",
            "Epoch 24/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 25/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 26/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0042\n",
            "Epoch 27/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0041\n",
            "Epoch 28/75\n",
            "74/74 [==============================] - 6s 77ms/step - loss: 0.0042\n",
            "Epoch 29/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0043\n",
            "Epoch 30/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 31/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0041\n",
            "Epoch 32/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0042\n",
            "Epoch 33/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 34/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0044\n",
            "Epoch 35/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 36/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 37/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0039\n",
            "Epoch 38/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 39/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0041\n",
            "Epoch 40/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0041\n",
            "Epoch 41/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 42/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 43/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0043\n",
            "Epoch 44/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 45/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 46/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 47/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 48/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0039\n",
            "Epoch 49/75\n",
            "74/74 [==============================] - 7s 95ms/step - loss: 0.0040\n",
            "Epoch 50/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 51/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 52/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 53/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 54/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 55/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0040\n",
            "Epoch 56/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0041\n",
            "Epoch 57/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0040\n",
            "Epoch 58/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 59/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 60/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 61/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "Epoch 62/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0038\n",
            "Epoch 63/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0038\n",
            "Epoch 64/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0038\n",
            "Epoch 65/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 66/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 67/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0038\n",
            "Epoch 68/75\n",
            "74/74 [==============================] - 6s 80ms/step - loss: 0.0038\n",
            "Epoch 69/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0039\n",
            "Epoch 70/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 71/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0037\n",
            "Epoch 72/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0037\n",
            "Epoch 73/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0037\n",
            "Epoch 74/75\n",
            "74/74 [==============================] - 6s 78ms/step - loss: 0.0038\n",
            "Epoch 75/75\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.0039\n",
            "50/50 [==============================] - 2s 16ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "MnuUdKWaqgaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf168230-074e-4122-8dc4-373af71f1dc5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.057699886474616345, 0.05999266453609161, 0.06033102487528098, 0.060625398619082574, 0.06040812340158018]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ykd7kawkvX",
        "outputId": "64de6568-165a-46ab-9e1b-4135419d17f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.057699886474616345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5KwbVjdXKn01"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)"
      ],
      "metadata": {
        "id": "O622nEj3Krt7",
        "outputId": "1578743d-acf0-4e5b-e330-04739bc05a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f757179bc40>]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hV9Z3v8fc3N+4JkIRAuOWKXLygRFBQVFI7tp2ROrVVe1FbW29FnafznBnnPOc5Z8bznOfUzpnpqbeqVWfU1mprZ1raaj0VEBUVCV5BFHbCLQTJlQQIIbfv+WMvYkgC7ECStZN8Xs+zH/Ze67dWvmuFnU9+v9/aK+buiIiIdJYQdgEiIhJ/FA4iItKNwkFERLpROIiISDcKBxER6SYp7AL6QkZGhufk5IRdhojIoLJx48Zqd8/sad2QCIecnBxKSkrCLkNEZFAxs53HW6dhJRER6UbhICIi3SgcRESkG4WDiIh0o3AQEZFuFA4iItKNwkFERLoZEp9zEJHBramljXd21fFBeT3Z40dxZnYqOeljSEiwsEsbthQOIjLg2tqdD/fUsy5SzRul1ZTsqONIa/sxbcakJDI3O5V52WnMzU7lzOw0CrPGkpyoAY+BoHAQkX7n7kQqD7IuUs260hreKqvhQFMrALMnj+Mbi2aypCCd82ZMoKL+MJsrGti8p57NFQ38qmQ3jc1tAKQkJnDG5HHMy06NPqamMWdyKqNSEsM8vCFJ4SAi/aK8rpE3Smt4I1LNG6U1VB44AsCMiaP5y7OnsDg/gwvz08kYO+KY7SaMSWFedhoUTQeivYwdNYeOCYyXNn/Ksxt2A5BgkJ85NgiMNOZNTWXelDTSRicP7AEPMQoHEekTNQeP8GZZDesiNbxRWs3OmkYAMsaOYHF+OksK0lmcn8H0iaN7td/EBCM/cyz5mWO58pxsINoTqahvYvOeejZVNPBRRT3rt9fy2/cqOrabPnEU86akMS87lTOnRv+dlDqy7w54iFM4iMgpOXSklbe313YMFW3Z2wDAuBFJLMqbyA0X5rCkIINZWWMx69uJZTNj6vhRTB0/is/Pm9yxvObgETZXNLCpItrD+KiigT9t/rRjfea4ER1DUmdmpzEvO43pE0f1eX1Dgbl72DWctqKiItddWUX615HWNt7dtb9jqOi93ftpbXdSkhIomjmBJQXRYaKzp6aRFEeTxgeaWtiy9wCbgiGpzRX1bKs8SFt79Gdf6sikjonvM6dG/83LGBNXx9BfzGyjuxf1tC6mnoOZXQH8BEgEHnP3H3ZZPwJ4ClgA1ADXuPuOYN3ZwCNAKtAOnO/uTWa2APh3YBTwAnCXu7uZ/SPwPaAq2P1/dfcXYj5aEekTbe3ORxUNrCutZl2kmg07amlqaSfB4Kxp47l5aR5LCjJYMHMCI5Pjd0J43MhkFuZOZGHuxI5lTS1tbN13gE17omGxuaKBn7+1s+OKqZHJCcyenHrMkNSsrHFxfZx97aQ9BzNLBLYClwPlwAbgOnf/qFOb24Gz3f1WM7sWuMrdrzGzJOAd4Fvu/r6ZpQP73b3NzN4G7gTWEw2H+9z9xSAcDrr7/4n1INRzEDl97k5Z9SHeiFSzLlLDm2U11B9uAaBw0liWFGSwOD+dRXnppI0aepO9rW3tlFUfOqaHsbmioeOqqqQEo2DS2GN6GHOzUxk7YvCOzp9uz2EhEHH3smBnzwLLgY86tVkO/GPw/HngAYsO4n0e+MDd3wdw95pgH1OAVHd/K3j9FPBl4MXeHZqInI699YejE8jBFUWfNjQBRMfy52Z1BMJwmMhNSkxgVtY4ZmWN46/Piy5zd3bXHg7mMKJhsXZrFb95p7xju9yMMR2fwzg6n5He5QqswSiWcJgK7O70uhxYdLw27t5qZvVAOjALcDN7CcgEnnX3HwXtyzttXx4sO2qFmV0PlAB/6+51XYsys5uBmwFmzJgRw2GIyP7GZt4srWFdaTVvRGooqz4EwMQxKVyYn86S/AyWFKQzY+JoTdISnfiekT6aGemj+eJZUzqWVzY0RQNjTwObKxp4f/d+/vjB3o71U9JGfnZpbfB5jOy0kYPqnPZ3fygJuAg4H2gEVpnZRqD+BNv8FPifgAf//gvwna6N3P1R4FGIDiv1bdkiQ0NjcysbdtRFh4pKq9lc0YB79NPHC3Mn8vVFM1icn8HsyeN0q4pemJQ6kmWpI1k2O6tj2f7GZj6qaOgYktpU0cDqjysJ5r2ZMDr5s89hBKGRG8e3CIklHPYA0zu9nhYs66lNeTDPkEZ0YroceNXdqwHM7AXgPODnwX667dPd9x1daGY/A/7Qi+MRGdZa2tp5b/f+4LYUNby7q46WNic50Th3xgT+pngWSwrSOWf6eN2Goo+NH53C4oIMFhdkdCxrbG5ly94DfBQMSW2qqOffXt9Bc1t04ntMSiJzpkQnvecGQ1KFk8aRkhT+9yaWcNgAFJpZLtEf4NcCX+/SZiVwA/AmcDWwOrjy6CXg78xsNNAMXAL82N33mlmDmV1AdEL6euB+iM5HuPvR/tlVwKbTOkKRIay93dnyaQNvRKJDRW9vr6WxuQ0zODM7je9clMuS/AyKciYwOmXwTpwOVqNTklgwcwILZk7oWNbc2s62ygMdn8PYtKe+2y1CZk0e2zGHMTc7jTlTxg349++kXy2YQ1gBvET0UtYn3H2zmd0DlLj7SuBx4GkziwC1RAMEd68zs38lGjAOvODufwx2fTufXcr6Ip9NRv/IzOYH7XcAt/TFgYoMBe7OzprGjjmDN8tqqD3UDEBe5hi+ct40lhSkc0FeOuNHp4RcrfQkJSkhGFZK61jW3u5sP3qLkGAuo+stQvIyx3Jm53mM7P69RYg+BCcS5yobmjrC4I3SGvbsPwzA5NSRLC6ITiIvLkhnStqokCuVvtT5FiGdL63dW9/U0WbahFH83RWzO24r0lun/SE4ERk49YdbeKusJphEriFSeRCA8aOTuTAvnVsvzWdxfjp5GWMG1dUv0jsnu0XI0TmMjDH900NUOIiErKmljZIddUHvoJoP99TT7jAqOZHzcyfy1QXTWFKQwdwpqXF7ZYsMnPSxI1g6K5OlszL79esoHEQGWGtbO++X13dcXvrOzv00t7WTlGDMnz6eFcsKWZKfzrkzJsTFVSsyPCkcRPqZu/PJvgMdn0Rev72Wg0eit2SYMyWV6y+cyZKCDM7PnTiob8UgQ4v+J4r0g7pDzbyytZI1H1fxRmk11QejVxTlpI/myvnZLAn+0M3EfhovFjldCgeRPuDulFYd5OUtlazaso+NO+to9+gfurko+GDUkoIMpo7XFUUyOCgcRE5RS1s7G7bXRgPh430df/ls7pRUVlxWQPGcLM6amqZJZBmUFA4ivbC/sZlXPqni5S37WLu1igNNraQkJbA4P53vXpxH8exJZKt3IEOAwkHkBKLDRYdYtWUfq7ZUUrKztmO46AtnTqZ4ThYXFWQwRhPJMsTof7RIFy1t7WzYUcuqYP5gRzBcNGdKKrdfWkDxnEmcM228hotkSFM4iAD1jS28srWSl7dUsvaTShqaWklJTODC/HRuuiiXZXOyNJksw4rCQYat0qqDrN5Syctb9lGys462didjbAp/MS86XHRxoYaLZPjS/3wZNlrb2tmwoy46f/BxJduDv4I2e/I4br0kj+I5WczXcJEIoHCQIe7ocNGqLZW80mm46IL8dG5cnEPxnElMmzA67DJF4o7CQYac7dXRq4te3rKPDTuiw0XpY1L4/LzJfG7OJC4qzNRtKkROQu8QGfRa29op2VnH6o+j8wdlVdHhojOyxnHL0mC4aPp4EjVcJBIzhYMMSvWHW1i7tYpVW/bxyidV1B9uITnRuCAvnesvmEnxnCymT9RwkcipUjjIoLGj+hAvBx9G27CjltZ2Z+KYFIrnTOJzwdVF40b2359NFBlOFA4St1rb2nln1/6O+YPSYLhoVtZYvrc0equKc2dM0HCRSD9QOEhcaWhqYe0nwXDR1ir2N0aHixblpvPNC2ZSPDuLGekaLhLpbwoHCd3OmkMdt7p+e3t0uGjC6GSWnTGJ4jlZLJ2l4SKRgaZwkAHX1u68s6uuY/4gUnkQgIJJY7np4lw+NyeL8zRcJBIqhYMMiANNLby6tZpVW/ax5pNK6hpbSEowFuVN5OsLZ1A8ZxIz08eEXaaIBBQO0m921TRGewcfR4eLWtqc8aOTueyMSRTPmcTSWZmkarhIJC7FFA5mdgXwEyAReMzdf9hl/QjgKWABUANc4+47gnVnA48AqUA7cL67N5nZAuDfgVHAC8Bd7u5mNhF4DsgBdgBfc/e60zpKGRBt7c67u+o65g+2BcNF+Zlj+M6SXIrnZHHejPEkJSaEXKmInMxJw8HMEoEHgcuBcmCDma109486NbsJqHP3AjO7FrgXuMbMkoCfA99y9/fNLB1oCbb5KfA9YD3RcLgCeBG4G1jl7j80s7uD13/fB8cq/eBAUwuvbavm5eDDaLWHmklKMM7Pmcg150/nc3OyyMnQcJHIYBNLz2EhEHH3MgAzexZYDnQOh+XAPwbPnwceMDMDPg984O7vA7h7TbCPKUCqu78VvH4K+DLRcFgOXBrs60ngFRQOcWV3bWPHnU3fKquhpc1JG5XMZWdkBlcXZZI2SsNFIoNZLOEwFdjd6XU5sOh4bdy91czqgXRgFuBm9hKQCTzr7j8K2pd32efU4HmWu+8Nnn8KZMV+ONKftu07wB2/fJePPz0AQF7mGL69JJfi2ZNYMHOChotEhpD+npBOAi4CzgcagVVmthGoj2XjYA7Ce1pnZjcDNwPMmDGjb6qVE/rfL35Mxf7D/LcvzaF4Tha5Gi4SGbJi+VVvDzC90+tpwbIe2wTzDGlEJ6bLgVfdvdrdG4nOLZwXtJ92nH3uC4adjg4/VfZUlLs/6u5F7l6UmZkZw2HI6di0p57VH1dy89I8vntxnoJBZIiLJRw2AIVmlmtmKcC1wMoubVYCNwTPrwZWu7sDLwFnmdnoIDQuAT4Kho0azOyCYG7ieuB3Pezrhk7LJUT3rdpG6sgkrl+cE3YpIjIATjqsFMwhrCD6gz4ReMLdN5vZPUCJu68EHgeeNrMIUEs0QHD3OjP7V6IB48AL7v7HYNe389mlrC8GD4AfAr8ys5uAncDX+uRI5ZR9VNHA//toH3cVF+pzCSLDhEV/wR/cioqKvKSkJOwyhqzbf7GRV7dWs+7vl5E2WuEgMlSY2UZ3L+ppnS4vkRPauu8AL276lBsX5ygYRIYRhYOc0AOrI4xKTuSmi3LDLkVEBpDCQY4rUnmQ339QwbcunMmEMSlhlyMiA0jhIMf10JoII5IS+N7FeWGXIiIDTOEgPdpRfYjfvV/BNxfNJGPsiLDLEZEBpnCQHj30SoSkBOPmpeo1iAxHCgfpZndtI//xzh6uWziDSakjwy5HREKgcJBuHnqllAQzbrlEvQaR4UrhIMeo2H+Y5zfu5mvnT2NK2qiwyxGRkCgc5BgPry3FHW69JD/sUkQkRAoH6fBpfRPPvr2bqxdMY9qE0WGXIyIhUjhIh0deLaXNndsvLQi7FBEJmcJBAKg80MQz63dx1blTmZGuXoPIcKdwEAAee207LW3tfP8y9RpEROEgQM3BIzz95k6uPCdbf+FNRACFgwCPv76dptY2VixTr0FEohQOw9z+xmaefGMHXzprCgWTxoVdjojECYXDMPfE69s51Kxeg4gcS+EwjNUfbuHf1u3ginmTmT05NexyRCSOKByGsSff2MGBI63cUaxeg4gcS+EwTB1oauHx17fzuTlZzMtOC7scEYkzCodh6qk3d1J/uIU71WsQkR4oHIahQ0daeey1Mi49I5Ozp40PuxwRiUMKh2HoF+t3UtfYwh3LCsMuRUTilMJhmDnc3Majr5ZxcWEGC2ZOCLscEYlTMYWDmV1hZp+YWcTM7u5h/Qgzey5Yv97McoLlOWZ22MzeCx4Pd9rmGjP7wMw2m9m9nZbfaGZVnbb57ukfphz1zNu7qD7YrF6DiJxQ0skamFki8CBwOVAObDCzle7+UadmNwF17l5gZtcC9wLXBOtK3X1+l32mA/8MLHD3KjN70syK3X1V0OQ5d19xeocmXTW1tPHI2lIuyJvIwtyJYZcjInEslp7DQiDi7mXu3gw8Cyzv0mY58GTw/Hmg2MzsBPvMA7a5e1Xw+mXgK7GXLafiVyW7qTxwhDuL1WsQkROLJRymArs7vS4PlvXYxt1bgXogPViXa2bvmtlaM7s4WBYBzgiGnZKALwPTO+3vK8GQ0/Nm1nl5BzO72cxKzKykqqqqpybSyZHWNn76SilFMydwYV76yTcQkWGtvyek9wIz3P1c4AfAM2aW6u51wG3Ac8BrwA6gLdjm90COu58N/JnPeiTHcPdH3b3I3YsyMzP7+TAGv+c3lrO3vok7iws5cadORCS2cNjDsb/VTwuW9dgm6AmkATXufsTdawDcfSNQCswKXv/e3Re5+4XAJ8DWYHmNux8J9vsYsOBUDkw+09LWzkNrSpk/fTwXF2aEXY6IDAKxhMMGoNDMcs0sBbgWWNmlzUrghuD51cBqd3czywwmtDGzPKAQKAteTwr+nQDcTjQIMLMpnfZ7JbDlVA5MPvOf7+xhz/7D3KVeg4jE6KRXK7l7q5mtAF4CEoEn3H2zmd0DlLj7SuBx4GkziwC1RAMEYClwj5m1AO3Are5eG6z7iZmdEzy/x923Bs/vNLMrgdZgXzee9lEOY61t7TywJsJZU9O49AwNv4lIbMzdw67htBUVFXlJSUnYZcSl32ws529//T6PfmsBn583OexyRCSOmNlGdy/qaZ0+IT2EtbU7D66JMGdKKpfPzQq7HBEZRBQOQ9gfPqigrPoQdy4r0FyDiPSKwmGIam937l8dYVbWWP5Cw0ki0ksKhyHqxU2fEqk8yIplhSQkqNcgIr2jcBiCor2GbeRljuFLZ005+QYiIl0oHIagP2/Zx8efHuCOZQUkqtcgIqdA4TDEuDv3rdrGzPTR/NXZ2WGXIyKDlMJhiFn9cSWbKxr4/mUFJCXq2ysip0Y/PYYQd+e+1RGmTRjFVed2vXGuiEjsFA5DyKvbqnl/936+f1kByeo1iMhp0E+QIcLd+cnLW8lOG8lXzpsWdjkiMsgpHIaIN0preGfXfm67NJ+UJH1bReT06KfIEHHfqm1kpY7gq0U9/uE8EZFeUTgMAW+V1bB+ey23LM1nZHJi2OWIyBCgcBgC7l+9jYyxI7hu4YywSxGRIULhMMht3FnLukgNtyzNY1SKeg0i0jcUDoPcfasiTByTwjcuUK9BRPqOwmEQe2/3ftZureK7F+cyOuWkf/FVRCRmCodB7P5V2xg/OpnrL8wJuxQRGWIUDoPUpj31rPq4kpuW5DJ2hHoNItK3FA6D1P2rtzFuZBI3LMkJuxQRGYIUDoPQlr0NvLR5H99ekkvqyOSwyxGRIUjhMAg9sDrC2BFJfEe9BhHpJwqHQWbbvgO8sGkvNyyeyfjRKWGXIyJDlMJhkHlgTYRRyYncdFFe2KWIyBAWUziY2RVm9omZRczs7h7WjzCz54L1680sJ1ieY2aHzey94PFwp22uMbMPzGyzmd17sn0JlFYd5PfvV/CtC2YycYx6DSLSf04aDmaWCDwIfAGYC1xnZnO7NLsJqHP3AuDHwL2d1pW6+/zgcWuwz3Tgn4Fid58HTDaz4hj2Naw9uCZCSlIC371YvQYR6V+x9BwWAhF3L3P3ZuBZYHmXNsuBJ4PnzwPFZmYn2GcesM3dq4LXLwNfOcV9DQs7aw7xu/cq+MaimWSOGxF2OSIyxMUSDlOB3Z1elwfLemzj7q1APZAerMs1s3fNbK2ZXRwsiwBnBMNOScCXgekx7KuDmd1sZiVmVlJVVdV19ZDz0JpSEhOMW5aq1yAi/a+/J6T3AjPc/VzgB8AzZpbq7nXAbcBzwGvADqCtNzt290fdvcjdizIzM/u47Piyu7aR37xTznXnT2dS6siwyxGRYSCWcNjDZ7/VA0wLlvXYJugJpAE17n7E3WsA3H0jUArMCl7/3t0XufuFwCfA1hPtq/eHNnT8dG0pCWbceml+2KWIyDARSzhsAArNLNfMUoBrgZVd2qwEbgieXw2sdnc3s8xgQhszywMKgbLg9aTg3wnA7cBjJ9rXqRzcUFCx/zC/LtnNV4umMSVtVNjliMgwcdI7trl7q5mtAF4CEoEn3H2zmd0DlLj7SuBx4GkziwC1RAMEYClwj5m1AO3Are5eG6z7iZmdEzy/x92P9hyOt69h6ZG1pbjDbeo1iMgAsqHwS3lRUZGXlJSEXUaf29fQxMU/WsNV86dy79Vnh12OiAwxZrbR3Yt6WqdPSMexR9aW0dbu3H6Zeg0iMrAUDnGq6sARnnl7J1+eP5WZ6WPCLkdEhhmFQ5x67LUymlvb+b56DSISAoVDHKo5eISn3tzJX52TTV7m2LDLEZFhSOEQhx5/fTtNrW2suKwg7FJEZJhSOMSZ/Y3NPPXmTr541hQKs8aFXY6IDFMKhzjzxLodHDzSyh3L1GsQkfAoHOJIQ1ML/7ZuO38xL4vZk1PDLkdEhjGFQxx5ct0ODjS1cseywrBLEZFhTuEQJw4eaeWx17fzuTmTOHNqWtjliMgwp3CIE0+9uYP6wy3qNYhIXFA4xIHG5lYee207l8zK5Jzp48MuR0RE4RAPfvHWLmoPNXNnsXoNIhIfFA4hO9zcxiOvlrGkIJ0FMyeEXY6ICKBwCN0v395F9cEj3Km5BhGJIwqHEDW1tPHw2lIW5U5kUV562OWIiHRQOITo1yW7qTxwhLs01yAicUbhEJIjrW089EopC2ZO4MJ89RpEJL4oHELym4172FvfxJ3FhZhZ2OWIiBxD4RCClrZ2HnolwjnTx7O0MCPsckREulE4hOA/391Ded1h7iouUK9BROKSwmGAtba18+CaCGdOTeWyMyaFXY6ISI8UDgNs5fsV7Kxp5I5lmmsQkfilcBhAbe3OA6sjzJ48jsvnZIVdjojIcSkcBtAfP9xLWfUh7iwuJCFBvQYRiV8xhYOZXWFmn5hZxMzu7mH9CDN7Lli/3sxyguU5ZnbYzN4LHg932uY6M/vQzD4wsz+ZWUaw/B/NbE+nbb7YN4carvZ25/5V2yicNJYr5k0OuxwRkRM6aTiYWSLwIPAFYC5wnZnN7dLsJqDO3QuAHwP3dlpX6u7zg8etwT6TgJ8Al7n72cAHwIpO2/y40zYvnOrBxZM/bf6UbZUHWbGsQL0GEYl7sfQcFgIRdy9z92bgWWB5lzbLgSeD588DxXbi2VYLHmOCdqlARa8qH0Ta2537Vm0jL2MMf3l2dtjliIicVCzhMBXY3el1ebCsxzbu3grUA0fvCZFrZu+a2Vozuzho0wLcBnxINBTmAo932t+KYLjpCTPr8T7WZnazmZWYWUlVVVUMhxGel7fs4+NPD7BiWQGJ6jWIyCDQ3xPSe4EZ7n4u8APgGTNLNbNkouFwLpBNdFjpH4JtfgrkA/OD7f+lpx27+6PuXuTuRZmZmf18GKfO3blv9TZmpo/mynPUaxCRwSGWcNgDTO/0elqwrMc2wXxCGlDj7kfcvQbA3TcCpcAsoj/4cfdSd3fgV8DiYNk+d29z93bgZ0SHtQatNZ9UsmlPA9+/tICkRF0cJiKDQyw/rTYAhWaWa2YpwLXAyi5tVgI3BM+vBla7u5tZZjChjZnlAYVAGdEwmWtmR3/lvxzYErSb0mm/VwGben9Y8cHduW9VhKnjR3HVeV1H4kRE4lfSyRq4e6uZrQBeAhKBJ9x9s5ndA5S4+0qi8wVPm1kEqCUaIABLgXvMrAVoB25191oAM/sn4NVg3U7gxmCbH5nZfMCBHcAtfXKkIXhtWzXv7d7P/7rqTJLVaxCRQcSiozqDW1FRkZeUlIRdxjHcnasffpOK/Yd55b9cyoikxLBLEhE5hpltdPeintbp19l+8mZpDRt31nHbpfkKBhEZdBQO/eS+1duYNG4EXyuafvLGIiJxRuHQD9aX1fBWWS23XJLPyGT1GkRk8FE49IP7V0fIGJvC1xfOCLsUEZFTonDoYxt31vF6pJqbl+YxKkW9BhEZnBQOfez+1duYOCaFbyyaGXYpIiKnTOHQh97fvZ9XPqnipotyGTPipB8hERGJWwqHPnT/6m2kjUrm+gvVaxCRwU3h0Ec27ann5S2V3HRRLuNGJoddjojIaVE49JEHVkcYNzKJGxbnhF2KiMhpUzj0gY8/beBPmz/l24tzSBulXoOIDH4Khz5w/+oIY1IS+c5FuWGXIiLSJxQOpylSeYAXPtzLDYtzGD86JexyRET6hMLhND2wOsKo5ES+e3Fe2KWIiPQZhcNpKKs6yMr3K/jmBTOZOEa9BhEZOhQOp+HBNaUkJybwPfUaRGSIUTicol01jfz2vT18Y9FMMseNCLscEZE+pXA4RQ+9EiExwbjlEvUaRGToUTicgvK6Rp7fWM61508nK3Vk2OWIiPQ5hcMp+OkrpZjBrZfkh12KiEi/UDj00t76w/y6pJyvFk0ne/yosMsREekXCodeemRtGe3u3KZeg4gMYQqHXqhsaOKZt3fx1+dNZfrE0WGXIyLSbxQOvfDIq2W0tTvfv6wg7FJERPpVTOFgZleY2SdmFjGzu3tYP8LMngvWrzeznGB5jpkdNrP3gsfDnba5zsw+NLMPzOxPZpYRLJ9oZn82s23BvxP65lBPT/XBI/xi/U6Wz89mZvqYsMsREelXJw0HM0sEHgS+AMwFrjOzuV2a3QTUuXsB8GPg3k7rSt19fvC4NdhnEvAT4DJ3Pxv4AFgRtL8bWOXuhcCq4HXofvZaGUda29VrEJFhIZaew0Ig4u5l7t4MPAss79JmOfBk8Px5oNjM7AT7tOAxJmiXClT0sK8ngS/HUGO/qj3UzNNv7uSvzs4mP3Ns2OWIiPS7WMJhKrC70+vyYFmPbdy9FagH0oN1uWb2rpmtNbOLgzYtwG3Ah0RDYS7weNA+y933Bs8/BbJ6dUT94PHXyzjc0saKZeo1iMjw0N8T0nuBGe5+LvAD4BkzSzWzZKLhcC6QTXRY6R+6buzuDnhPO8v8KgkAAAcdSURBVDazm82sxMxKqqqq+u0A6htbePKNnXzxzCnMyhrXb19HRCSexBIOe4DpnV5PC5b12CaYT0gDatz9iLvXALj7RqAUmAXMD5aVBgHwK2BxsK99ZjYl2NcUoLKnotz9UXcvcveizMzMGA7j1DyxbjsHj7Sq1yAiw0os4bABKDSzXDNLAa4FVnZpsxK4IXh+NbDa3d3MMoMJbcwsDygEyoiGyVwzO/pT/XJgSw/7ugH4Xe8Pq280NLXwxLrtfH5uFnOmpIZVhojIgEs6WQN3bzWzFcBLQCLwhLtvNrN7gBJ3X0l0vuBpM4sAtUQDBGApcI+ZtQDtwK3uXgtgZv8EvBqs2wncGGzzQ+BXZnZTsPxrfXOovffkuh0caGrlzuLCsEoQEQmFRUd1BreioiIvKSnp030ePNLKRfeuZsGMCTx+4/l9um8RkXhgZhvdvaindfqE9HE8/eZO9je2cId6DSIyDCkcetDY3MrPXitj6axM5k8fH3Y5IiIDTuHQg2fW76L2UDN3FesKJREZnhQOXTS1tPHw2jIW56ezYObEsMsREQmFwqGLX769i+qDR3SFkogMawqHTqK9hlIW5k7kgrz0k28gIjJEKRw6+fXGcvY1HOEu9RpEZJhTOASaW9v56ZoI580Yz+J89RpEZHhTOAR+8045FfVN3FlcyInvNi4iMvQpHICWtnYeXBPhnGlpXDKr/27iJyIyWCgcgN++u4fyusPqNYiIBIZ9OLQGvYZ52aksmz0p7HJEROLCsA+H339QwY6aRu5Ypl6DiMhRwzoc2tqd+1dHmD15HJ+fG/pfIxURiRvDOhxe+HAvZVWHuGNZIQkJ6jWIiBw1rMNhzIhELp+bxRfOnBx2KSIiceWkfwluKFs2O4tlszWcJCLS1bDuOYiISM8UDiIi0o3CQUREulE4iIhINwoHERHpRuEgIiLdKBxERKQbhYOIiHRj7h52DafNzKqAnae4eQZQ3Yfl9BXV1Tuqq/fitTbV1TunU9dMd+/xj9gMiXA4HWZW4u5FYdfRlerqHdXVe/Fam+rqnf6qS8NKIiLSjcJBRES6UTjAo2EXcByqq3dUV+/Fa22qq3f6pa5hP+cgIiLdqecgIiLdKBxERKSbYRMOZnaFmX1iZhEzu7uH9SPM7Llg/Xozy4mTum40syozey94fHeA6nrCzCrNbNNx1puZ3RfU/YGZnRcndV1qZvWdztd/H4CappvZGjP7yMw2m9ldPbQZ8PMVY11hnK+RZva2mb0f1PVPPbQZ8PdjjHWF8n4Mvnaimb1rZn/oYV3fny93H/IPIBEoBfKAFOB9YG6XNrcDDwfPrwWei5O6bgQeCOGcLQXOAzYdZ/0XgRcBAy4A1sdJXZcCfxjgczUFOC94Pg7Y2sP3ccDPV4x1hXG+DBgbPE8G1gMXdGkTxvsxlrpCeT8GX/sHwDM9fb/643wNl57DQiDi7mXu3gw8Cyzv0mY58GTw/Hmg2MwsDuoKhbu/CtSeoMly4CmPegsYb2ZT4qCuAefue939neD5AWALMLVLswE/XzHWNeCCc3AweJkcPLpeGTPg78cY6wqFmU0DvgQ8dpwmfX6+hks4TAV2d3pdTvc3SUcbd28F6oH0OKgL4CvBUMTzZja9n2uKVay1h+HCYGjgRTObN5BfOOjOn0v0t87OQj1fJ6gLQjhfwRDJe0Al8Gd3P+75GsD3Yyx1QTjvx/8L/B3Qfpz1fX6+hks4DGa/B3Lc/Wzgz3z224H07B2i94s5B7gf+O1AfWEzGwv8Bvgbd28YqK97MiepK5Tz5e5t7j4fmAYsNLMzB+LrnkwMdQ34+9HM/hKodPeN/f21Ohsu4bAH6Jzw04JlPbYxsyQgDagJuy53r3H3I8HLx4AF/VxTrGI5pwPO3RuODg24+wtAspll9PfXNbNkoj+Af+Hu/9FDk1DO18nqCut8dfr6+4E1wBVdVoXxfjxpXSG9H5cAV5rZDqJDz8vM7Odd2vT5+Rou4bABKDSzXDNLITphs7JLm5XADcHzq4HVHszuhFlXl3HpK4mOG8eDlcD1wVU4FwD17r437KLMbPLRsVYzW0j0/3i//lAJvt7jwBZ3/9fjNBvw8xVLXSGdr0wzGx88HwVcDnzcpdmAvx9jqSuM96O7/4O7T3P3HKI/I1a7+ze7NOvz85V0OhsPFu7eamYrgJeIXiH0hLtvNrN7gBJ3X0n0TfS0mUWITnheGyd13WlmVwKtQV039nddAGb2S6JXsmSYWTnwP4hO0OHuDwMvEL0CJwI0At+Ok7quBm4zs1bgMHDtAIT8EuBbwIfBeDXAfwVmdKorjPMVS11hnK8pwJNmlkg0jH7l7n8I+/0YY12hvB970t/nS7fPEBGRbobLsJKIiPSCwkFERLpROIiISDcKBxER6UbhICIi3SgcRESkG4WDiIh08/8B3hk34YbxAtYAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}